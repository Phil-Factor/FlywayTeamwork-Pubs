<#

# The Flyway Teamworks Scriptblocks.

These are a collection of script blocks that are designed to run with Flyway either in the community edition or with Teams. They allow you to maintain backups or scripts for each version and make a Flyway a full participant in any source control system in use, even to the point of providing an  object-level source to allow the evolution of individual tables to be tracked.

Some of these will work on the major relational database systems, whereas some are for SQL Server only 

A simple database that is really nothing more than a grown-up spreadsheet will  not need any of this fancy stuff. It becomes increasingly important as the database grows into a team effort. The facilities cover automatic code reviews, source control, and change tracking. It provides build scripts that are, I believe, essential to successful branching and merging, and for databases such as MySQL that do not roll-back DDL changes in failed transactions. Some of these utilities are there primarily to show how to run external programs, or to save the results of a SQL Call as a JSON file or other result. I have  examples of SQL Code executed from a variable containing a query,  and from  a file. 

They share a common data object (hashtable) as a parameter. This is automatically  generated by the preliminary.ps1 that also obligingly retrieves your password from your local secure store. This hashtable has all the parameters set by the config  files and environment variables. It parses the URL provided to Flyway to retrieve the name of the database server, port and the database. Each scriptblock checks to see whether you've provided what is needed... the parameter is actually a hashTable passed by reference. 
Few of these parameters  are needed for any one task, and some such as version and password are filled in as the hashtable is accessed.

These Scriptblocks can be batched up into an array and processed one by one. Some of them are utility chores such as extracting passwords from a relatively safe place, or fetching prepared parameters. There
is one that merely formats the data. They are able to write errors or status to the data object. The hashtable is passed by reference. One has to be cautious with this because alterations to the hashtable can result in powerShell holding a copy instead. 

As well as being run in a series, they can be used individually. If one of them needs maintenance, it is very easy to pull it apart and run it interactively.

The reason for using this design was to make it easy to choose what gets run and in what order. 

**$CheckCodeInDatabase** *(SQL Server only)*
This scriptblock checks the code in the database for any issues, using SQL Code Guard to do all the work. This runs SQL Codeguard  and saves the report in a subdirectory the version directory of your project artefacts. It also reports back in the **$dbDetails** Hashtable. It checks the current database, not the scripts

**$CheckCodeInMigrationFiles** *(SQL Server only)*
This scriptblock checks the code in the migration files for any issues, using SQL Code Guard to do all the work. This runs SQL Codeguard and saves the report in a subdirectory the version directory of your 
project artefacts. It also reports back in the **$dbDetails** Hashtable. It checks the scripts not the current database.

**$CreateScriptFoldersIfNecessary**: *(SQL Server, Oracle, SQLite, MySQL, MariaDB, PostgreSQL)*
this task checks to see if a Source folder already exists for this version of the database and, if not, it will create one and fill it with subdirectories for each type of object. A tables folder will, for example, have a file for every table each containing a build script to create that object. When this exists, it allows SQL Compare
to do comparisons and check that a version has not drifted. It saves the Source folder as a subfolder for the supplied version, so it needs **$GetCurrentVersion** to have been run beforehand in the chain of tasks.

**$CreateBuildScriptIfNecessary**: *(PostgreSQL, MySQL, MariaDB,SQL Server, SQLite)*
produces a build script from the database, using SQL Compare. It saves the build script in the Scripts folder, as a subfolder for the supplied version, so it needs $GetCurrentVersion to have been run beforehand in the chain of tasks.

**$ExecuteTableSmellReport** *(SQL Server only)*
This scriptblock executes SQL that produces a report in XML or JSON from the database that alerts you to tables that may have issues

**$ExecuteTableDocumentationReport** *(SQL Server,MySQL, MariaDB, PosgreSQL)*
 This places in a report a json report of the documentation of every table and its columns. If you add or change tables, this can be subsequently used to update the **AfterMigrate** callback script
for the documentation. 

**$FetchAnyRequiredPasswords** *(all RDBMSs)*
This checks the hash table to see if there is a username without a password. If so, the task asks for it in a query window and stores it, encrypted, in the user area it. If the user already has the password stored for this username and database, it uses it.

**$ExecuteTableSmellReport** *(SQL Server only)*
This scriptblock executes SQL that produces a report in XML or JSON from the database

**$GetCurrentVersion** *(PostgreSQL, Oracle, MySQL, MariaDB,SQL Server, SQLite)*
This contacts the database and determines its current version, and previous version by interrogating the flyway_schema_history data table in the database. If it is an empty database,or there is just no Flyway data, then it returns a version of 0.0.0.

**$GetCurrentServerVersion** *(PostgreSQL, Oracle, MySQL, MariaDB,SQL Server, SQLite)*
This scriptblock gets the current version of the RDBMS on the server and is used mainly to check that the migration doesn't use any functionality that can't be supported on that server version. It updates
the $dbDetails.ServerVersion 

**$IsDatabaseIdenticalToSource:** *(SQL Server only)*
This uses SQL Compare to check that a version of a database is correct and hasn't been changed. To do this, the $CreateScriptFoldersIfNecessary task must have been run first. It compares the database to the associated source folder, for that version, and returns, in the hash table, the comparison equal to true if it was the same, or false if there has been drift, with a list of objects that have changed. If the comparison returns $null, then it means there has been an error. To access the right source folder for this database version, it needs $GetCurrentVersion to have been run beforehand in the chain of tasks

**$SaveDatabaseModelIfNecessary** *(PostgreSQL,  Oracle, MySQL, MariaDB,SQL Server, SQLite)*
This writes a JSON model of the database to a file that can be used subsequently to check for database version-drift or to create a narrative of changes for the flyway project between versions.

**$BulkCopyIn** *(SQL Server only)*
This script performs a bulk copy operation to get data into a database. It can only do this if the data is in a suitable directory. At the moment it assumes that you are using a DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path Unlike many other tasks, you are unlikely to want to do this more than once for any database.If you did, you'd need to clear out the existing data first! It is intended
for static scripts AKA baseline migrations.

**$BulkCopyout** *(SQL Server only)*
This script performs a bulk copy operation to get data out of a database, and into a suitable directory. At the moment it assumes that you wish to use a DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path.

**$CreateUndoScriptIfNecessary** *(SQL Server only)*
this creates a first-cut UNDO script for the metadata (not the data) which can be adjusted and modified quickly to produce an UNDO Script. It does this by using SQL Compare to generate a  idepotentic script comparing the database with the  contents of the previous version.

**$GeneratePUMLforGanttChart** *(All RDBMSs)*
This script creates a PUML file for a Gantt chart at the current version of the database. This can be read into any editor that takes PlantUML files to give a Gantt chart

**$CreatePossibleMigrationScript** *(SQL Server only)*
This creates a forward migration that scripts out all the changes made to the database since the current migration

**$SaveFlywaySchemaHistoryIfNecessary** *(all RDBMSs)*
This reads the flyway history table, and uses the information to annotate the directories containing the various reports and scripts for that version

**$CreateVersionNarrativeIfNecessary** *(PostgreSQL, Oracle, MySQL, MariaDB, SQL Server, SQLite)*
This aims to tell you what has changed between each version of the database. 

**$WriteOutERDiagramCode** *(PostgreSQL,  Oracle, MySQL, MariaDB, SQL Server, SQLite)*
This creates a simple entity diagram for the current version. You only need two files to do this and you don't need to contact the database. The ER diagram has all objects that are either added, removed or changed colour-coded so you can see immediately what has changed. The idea of this is to be able to paste the resulting SVG file or other image file of the diagram, produced by PlantUMLc.exe.

**$CheckFluffInPendingFiles** *(PostgreSQL,  Oracle, MySQL, MariaDB, SQL Server, SQLite)*
This scriptblock checks the code in the pending files for any issues,using SQL Fluff to do all the work. It saves the report in a subdirectory 
of the version directory of your project artefacts. It also reports back in the $DatabaseDetails Hashtable. 

**$DoesTheFlywayTableExist** *(PostgreSQL,  Oracle, MySQL, MariaDB, SQL Server, SQLite)* 
This checks to see if there is a flyway schema table 
in the database. It sets a value in the dbDetails object (FlywayTableExists) that is true or false.

**$ExtractFromSQLServerIfNecessary** *(SQL Server only)*
This connects to the SQL Server database and will then, For the current version of the database extract either a 
 - *'DacPac'*     (output a .dacpac single file). 
 - *'Flat'*  (all files in a single folder),
 - *'SchemaObjectType'* (files in folders for each schema and object type), 
 - *'Schema'* (files in folders for each schema),
 - *'ObjectType'*  (files in folders for each object type), 'Flat' (all files in the same folder)
 - *'File'* (1 single file). 

**$CreatePossibleMigrationScriptFromDacpac**  *(SQL Server only)*
   By default, this will take the DACPAC for the current version of the database, and will 
   compare it with the previous version. it should, logically, work either way to do forward 
   or UNDO migrations. Sadly it doesn't always work. The file is processed to remove  any CREATE SCHEMA
   statements,SQLCMD statements and any SQLCMD parameter definitions

## examples of usage

Tasks can be executed one at a time or stacked up and executed one after another. 
### Simple Arms-length usage
In the framework, you'd have a $dbDetails object to store what can become a huge number of parameters, especially if you really like your placeholders. Lets start without doing that, just to demonstrate that it is possible to just run the tasks you want without getting in too deep.

```
Process-FlywayTasks @{
   'version'='1.1.5'; 'server'='MyServer'; 'reportLocation'=MyDirectory; 
   'database'=' MyDatabase '; 'pwd'='mySecretPassword'; 'uid'='MyUserID';
   'project'='Pubs'; 'projectDescription'='A simple Demonstration';
   'problems'=@{};'warnings'=@{};'feedback'=@{};'writeLocations'=@{}
} $ExtractFromSQLServerIfNecessary 

`
### Using the shared DbDetails object.
However if you are using the preliminary.ps1 to keep your stash of parameters in a shared $DBDetails object, it is all simpler and neater.

Here are several tasks being done together 

```
`$PostMigrationTasks = @(`
	`$GetCurrentVersion, #checks the database and gets the current version number`
	`#it does this by reading the Flyway schema history table.` 
    `$GetCurrentServerVersion, #get the current version of the database server`
	`$CreateBuildScriptIfNecessary, #Create a build script for the database in a` 
	`#subdirectory for this version.`
	`$SaveDatabaseModelIfNecessary, #Build a JSON model of the database that we can`
	`#later use for comparing versions to create a chronicle of changes.`
	`$CreateVersionNarrativeIfNecessary,`
    `#save the information from the history table about when all the changes were made and by whom
‹    `$SaveFlywaySchemaHistoryIfNecessary`
`)`
`Process-FlywayTasks $dbDetails $PostMigrationTasks`
‹	`}`
`}`
```

here is one scriptblock being done

``` 
Process-FlywayTasks $dbDetails $GetCurrentServerVersion 
```

Some scriptblocks have extra parameters that allow them to be used more freely. 

here is an ERD diagram being done for a different version of the current project

``` 
Process-FlywayTasks $dbDetails $WriteOutERDiagramCode @('1.1.6')
```

In this case, there are other parameters that can be changed but they are ignored if set to NULL

```
Process-FlywayTasks $dbDetails $WriteOutERDiagramCode @(
		'1.1.7', #version - the flyway version of the database. Leave null if using framework
		  $null, #Title - the flyway project. Leave null if using framework
          $null, #FileLocations - where to store all files
		  $null, #MetadatachangeFile - Specify if not using the default location
		  $null, #modelFile - Specify if not using the default location
		  $null  #MyPUMLFile - The path to the PUML file
)
```

Here we change the title and the location of the files

```
Process-FlywayTasks @{
problems=@{};warnings=@{};feedback=@{};writeLocations=@{}
} $WriteOutERDiagramCode @(
		'1.1.7', #version - the flyway version of the database. Leave null if using framework
		'MyTitle', #Title - the flyway project. Leave null if using framework
        'MyFileLocation', #FileLocations - where to store all files
		$null, #MetadatachangeFile - Specify if not using the default location
		$null, #modelFile - Specify if not using the default location
		$null  #MyPUMLFile - The path to the PUML file
);Process-FlywayTasks $dbDetails $WriteOutERDiagramCode @('1.1.6')
```

You might not want all the project array because you're just generating diagrams from the two model files. Why not? So you just do the bare minimum hashtable

```
`Process-FlywayTasks @{`
`problems=@{};warnings=@{};feedback=@{};writeLocations=@{}`
`} $WriteOutERDiagramCode @(`
		`'1.1.7', #version - the flyway version of the database. Leave null if using framework`
		`'MyTitle', #Title - the flyway project. Leave null if using framework`
        `'MyFileLocation', #FileLocations - where to store all files`
		`$null, #MetadatachangeFile - Specify if not using the default location`
		`$null, #modelFile - Specify if not using the default location`
		`$null  #MyPUMLFile - The path to the PUML file`
`);`
```

To set off any task, all you need is a PowerShell script that is created in such a way that it can be executed by Flyway when it finishes a migration run. Although you can choose any of the significant points in any Flyway action, there are only one or two of these callback points that are useful to us.  

This can be a problem if you have several chores that need to be done in the same callback or you have a stack of scripts all on the same callback, each having to gather up and process parameters, or pass parameters such as the current version from one to another. 

A callback script can’t be debugged as easily as an ordinary script. In this design, the actual callback just executes a list of tasks in order, and you simply add a task to the list after you’ve debugged and tested it & placed in the DatabaseBuildAndMigrateTasks.ps1 file.
with just one callback script

Each task is passed a standard ‘parameters’ object. This keeps the ‘complexity beast’ snarling in its lair.
The parameter object is passed by reference so each task can add value to the data in the object, such as passwords, version number, errors, warnings and log entries. 

All parameters are passed by Flyway. It does so by environment variables that are visible to the script.
You can access these directly, and this is probably best for tasks that require special informationpassed by custom placeholders, such as the version of the RDBMS, or the current variant of the version  you're building

### getting the dbDetails object and all its values

The ". '.\preliminary.ps1" line - that this callback startes with - creates a DBDetails array.
You can dump this array for debugging so that it is displayed by Flyway

   `$DBDetails|convertTo-json

these routines return the path they write to in the $DbDetails if you need it.
You will also need to set the paths to the various commandline utilities to the correct value. 
For SQLCMD, for example, this is set by a string that is read from MyToolLocations.ps1 in your Flyway Teamwork directory. 
it can be set as a default in the resources directory in the toolLocations.ps1 file in the RESOURCES  directory 

### Using a callback script
Here is a worked example, with the tasks you want to execute. Some, like the on getting credentials, are essential before you execute others.
In order to execute tasks, you just load them up in the order you want. It is like loading a revolver.

``` 
. '.\preliminary.ps1'

$PostMigrationTasks = @(
	$GetCurrentVersion, #checks the database and gets the current version number
    #it does this by reading the Flyway schema history table. 
	$CreateBuildScriptIfNecessary, #writes out a build script if there isn't one for this version. This
    #uses SQL Compare
	$CreateScriptFoldersIfNecessary, #writes out a source folder with an object level script if absent.
    #this uses SQL Compare
	$ExecuteTableSmellReport, #checks for table-smells
    #This is an example of generating a SQL-based report
	$ExecuteTableDocumentationReport, #publishes table docuentation as a json file that allows you to
    #fill in missing documentation. 
	$CheckCodeInDatabase, #does a code analysis of the code in the live database in its current version
    #This uses SQL Codeguard to do this
	$CheckCodeInMigrationFiles, #does a code analysis of the code in the migration script
    #This uses SQL Codeguard to do this
	$IsDatabaseIdenticalToSource, # uses SQL Compare to check that a version of a database is correct
    #this makes sure that the target is at the version you think it is.
    $SaveDatabaseModelIfNecessary #writes out the database model
    #This writes out a model of the version for purposes of comparison, narrative and checking. 
    $CreateUndoScriptIfNecessary # uses SQL Compare
    #Creates a first-cut UNDo script. This is an idempotentic script that undoes to the previous version 
    $GeneratePUMLforGanttChart
    # This script creates a PUML file for a Gantt chart at the current version of the 
    #database. This can be read into any editor that takes PlantUML files to give a Gantt
    #chart 
            )
Process-FlywayTasks $DBDetails $PostMigrationTasks
```
Yeah, a lot of work is being done without you getting overwhelmed by the details and complexity


#>

$GetdataFromSqlite = { <# a Scriptblock way of accessing SQLite via a CLI to get JSON-based  results 
without having to explicitly open a connection. it will take either SQL files or queries.  #>
	Param (
		$Theargs,
		#this is the same ubiquitous hashtable 
		$query,
		#a query. If a file, put the path in the $fileBasedQuery parameter
		$fileBasedQuery = $null,
		$simpleText = $false,
		$timing = $false,#do you return timing information
        $muted = $false #do you return the data
        ) # $GetdataFromSqlite: (Don't delete this)
	$problems = @()
	$command = $null;
	$command = get-command sqlite -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($sqliteAlias -ne $null)
		{ Set-Alias sqlite $sqliteAlias }
		else
		{ $problems += 'You must have provided a path to sqlite.exe in the ToolLocations.ps1 file in the resources folder' }
	}
	
	if ($TheArgs.Database -in @($null, '')) # do we have the necessary values
	{ $problems += "Can't do this: no value for the database" }
	
	if ($problems.Count -eq 0)
	{
		$TempInputFile = "$($env:Temp)\TempInput$(Get-Random -Minimum 1 -Maximum 900).sql"
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery }
		else
		{ [System.IO.File]::WriteAllLines($TempInputFile, $query); }
		if ($timing)
		{ $AreWeTiming = 'on'; }
		else
		{ $AreWeTiming = 'off'; }
		$params = @(
			'.bail on',
			".timer $AreWeTiming",
			'.mode json',
			'.headers off',
			#".output $($TempOutputFile -replace '\\','/')",
			".read $($TempInputFile -replace '\\', '/')",
			'.quit')
		try
		{
			$result = (sqlite "$($Theargs.database)" @Params) -join "`n`r"
		}
		catch
		{ $problems += "SQL called to SQLite  failed because $($_)" }
		
		if ($?)
		{
			# we had a good result so maybe cope with timings, otherwise just the result
			if ($timing)
			{
				$regex = 'Run Time: real (?<RealTime>[\d\.]{1,20})\s{1,10}user (?<UserTime>[\d\.]{1,20})\s{1,10}sys\s{1,10}(?<SystemTime>[\d\.]{1,20})'
				
				if ($result -imatch $regex)
				{
					$timingData = [pscustomobject]$matches | convertto-json
					if (!($muted)) {($result -replace $regex, '').Trim()};
					write-output "the transaction in '$Query' took $([decimal]([pscustomobject]$matches.RealTime)*1000) ms."
				}
				else
				{
					$timingData = '';
					if (!($muted)) {$result};
				}
			}
			else
			{ $result }
		}
		else { $problems += 'The SQL Call to SQLite failed' }
		if ([string]::IsNullOrEmpty($FileBasedQuery)) { Remove-Item $TempInputFile }
	}
	if ($problems.Count -gt 0) { $Param1.Problems.'GetdataFromSqlite' = $problems }
}

#This is a utility scriptblock used by the task scriptblocks.
#with SQL Server, you really want your data back as JSON, but SQLcmd can't do it.
$GetdataFromSQLCMD = {<# a Scriptblock way of accessing SQL Server via a CLI to get JSON results without having to 
explicitly open a connection. it will take SQL files and queries. It will also deal with simple SQL queries if you
set 'simpleText' to true 
#>
	Param ($Theargs,
		#this is the same ubiquitous hashtable 
		$query,
		#either a query that returns JSON, or a simple expression
		$fileBasedQuery = $null,
		#if you specify input from a file
		$simpleText = $false,
        $timing = $false,#do you return timing information
        $muted = $false #do you return the data
        ) # $GetdataFromSQLCMD: (Don't delete this)
	$problems = @();
    $SQLQuery=$query;
	if ([string]::IsNullOrEmpty($TheArgs.server) -or [string]::IsNullOrEmpty($TheArgs.database))
	{ $Problems += "Cannot continue because name of either server ('$($TheArgs.server)') or database ('$($TheArgs.database)') is not provided"; }
	else
	{
		#the alias must be set to the path of your installed version of SQL Cmd
        $command=$null;
        $command = get-command SQLCmd -ErrorAction Ignore 
		if ($command -eq $null)
		{
			if ($SQLCmdAlias -ne $null)
			{ Set-Alias SQLCmd   $SQLCmdAlias }
			else
			{ $problems += 'You must have provided a path to SQLcmd.exe in the ToolLocations.ps1 file in the resources folder' }
		}
		$TempOutputFile = "$($env:Temp)\TempOutput$(Get-Random -Minimum 1 -Maximum 900).json"
        if ($timing) {$profile='-p1'} else {$profile=''}  #add the timing switch
		if ($simpleText -or $timing)
		{
		$FullQuery = "$query"
		}
		else
		{
		$FullQuery = "Set nocount on;SET QUOTED_IDENTIFIER ON; Declare @Json nvarchar(max) 
        Select @Json=($query) Select @JSON"
        };
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery;
          $SQLQuery=  Split-Path $FileBasedQuery -Leaf}
		else
		{ $TempInputFile = "$($env:Temp)\TempInput.sql" }
		#If we can't pass a query string directly, or we have a file ...
		if ($FullQuery -like '*"*' -or (!([string]::IsNullOrEmpty($FileBasedQuery))))
		{
			#Then we can't pass a query as a string. It must be passed as a file
			#Deal with query strings 
			if ([string]::IsNullOrEmpty($FileBasedQuery)) { $FullQuery>$TempInputFile; }
			if (-not [string]::IsNullOrEmpty($TheArgs.uid)) #if we need to use credentials
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -i $TempInputFile -U $TheArgs.Uid -P $TheArgs.pwd `
					   -o "$TempOutputFile" -u -y0 -b $profile
			}
			else #we are using integrated security
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -i $TempInputFile -E -o "$TempOutputFile" -u -y0 -b $profile
			}
            $Succeeded=$?
			#if it is just for storing a query
			if ([string]::IsNullOrEmpty($FileBasedQuery)) { Remove-Item $TempInputFile }
		}
		
		else #we can pass a query as a string
		{
			if (-not [string]::IsNullOrEmpty($TheArgs.uid)) #if we need to use credentials
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -Q "`"$FullQuery`"" -U $TheArgs.uid -P $TheArgs.pwd `
					   -o `"$TempOutputFile`" -u -y0 -b $profile
			}
			else #we are using integrated security
			{
				sqlcmd -S $TheArgs.server -d $TheArgs.database `
					   -Q "`"$FullQuery`"" -o `"$TempOutputFile`" -u -y0 -b $profile
			}
            $Succeeded=$?
		}
        If (Test-Path -Path $TempOutputFile)
		{
			#make it easier for the caller to read the error
			$response = [IO.File]::ReadAllText($TempOutputFile);
            Remove-Item $TempOutputFile

        if ($timing)            # if there was timing data appended
	        {
            $regex = '(?<PacketSize>\d{1,6}):(?<NoTransactions>\d{1,6}):(?<TotalTime>[.\d]{1,20}):(?<AverageTime>[.\d]{1,20}):(?<AverageTPS>[.\d]+)'
	
	            if ($response -imatch $regex)
	            {
		            $timingData = [pscustomobject]$matches | convertto-json
		            $response= ($response -replace $regex, '').Trim()
	            }
	            else
	            {
		            $timingData = ''
	            }
	            if ($timingData -ne '')
	            {
		            $TimingHashTable = $TimingData | convertfrom-JSON
		            if ($TimingHashTable.NoTransactions -gt 1)
		            {
			            write-output "the $($TimingHashTable.NoTransactions) transactions in  '$SQLQuery' took a total of  $($TimingHashTable.TotalTime) ms."
		            }
		            elseif ($TimingHashTable.NoTransactions -eq 1)
		            {
			            write-output "the transaction in '$SQLQuery' took  $($TimingHashTable.TotalTime) ms."
		            }
		            else
		            {
			            Write-Output "there were no transactions to time in $SQLQuery"
		            }
	            }
            }

    		if ($response -like 'Msg*' -or !($succeeded))
			{ $Problems += " When connecting to $($TheArgs.server); $($TheArgs.database) as $($TheArgs.uid) had, in response to $FullQuery the error $Response" }
			elseif ($response -like 'SqlCmd*')
			{ $problems += "SQLCMD says $Response" }
			
			if ($problems.count -gt 0)
			{ @{ Error = $problems } | convertTo-json }
			elseif ($response -like 'NULL*')
			{ '' }
			else
			{ if (!($muted)) {$response} }
		}
	}
}


$GetdataFromMySQL = {<# a Scriptblock way of accessing MySQL via a CLI to get JSON-based  results without having to 
explicitly open a connection. it will take either SQL files or queries.  #>
	Param (
		$Theargs,
		#this is the same ubiquitous hashtable 

		$query,
		#a query. If a file, put the path in the $fileBasedQuery parameter

		$fileBasedQuery = $null,
		$simpleText = $false,
		$timing = $false,
		#do you return timing information
		$muted = $false #do you return the data
	) # $GetdataFromMySQL: (Don't delete this)
	$problems = @()
	
	$command = $null;
	$command = get-command Mysql -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($MySQLAlias -ne $null)
		{ Set-Alias MySQL $MySQLAlias }
		else
		{ $problems += 'You must have provided a path to MySQL for $MySQLAlias in the ToolLocations.ps1 file in the resources folder' }
	}
	@('server', 'database', 'port', 'user', 'pwd') |
	foreach{ if ($TheArgs.$_ -in @($null, '')) { $problems += "Can't do this: no value for '$($_)'" } }
	
	if ($problems.Count -eq 0)
	{
		$TempInputFile = "$($env:Temp)\TempInput.sql"
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery }
		else
		{ [System.IO.File]::WriteAllLines($TempInputFile, $query); }
		if ($timing) { $timingParameter = '-vvv' }
		else { $timingParameter = '' }
		Try
		{
			
			$HTML = ([IO.File]::ReadAllText("$TempInputFile") | mysql "$($TheArgs.database)" "--host=$($TheArgs.server)" "--port=$($TheArgs.Port -replace '[^\d]', '')" "--show-warnings" "--password=$($TheArgs.pwd)"  "--user=$($TheArgs.uid)" $timingParameter '--comments'  '--html')
			if ($? -eq 0)
			{
				$problems += "The MySQL CLI returned an error $($error[0])"
			}
			$TheColumns = @();
			$Rows = Select-String '<TR>(.*?)</TR>' -input $html -AllMatches | foreach{ $_.matches } | Foreach{
				$Col = 0;
				$lineValue = $_.Value
				
				if ($Thecolumns.count -eq 0)
				{
					$TheColumns = Select-String '<TH>(.*?)</TH>' -input $lineValue -AllMatches |
					foreach{ $_.matches.groups } | where { $_.Name -eq 1 } | foreach{ $_.value }
				}
				else
				{
					$Row = [ordered]@{ };
					Select-String '<TD>(.*?)</TD>' -input $LineValue -AllMatches |
					foreach{ $_.matches.groups } | where { $_.Name -eq 1 } | foreach{
                        $currentValue=$_.value;
                        if ($currentValue -eq 'null'){$currentValue = $null}
						if ($TheColumns -is [string])
						{ $Row += @{ $TheColumns = $currentValue } }
						else
						{ $Row += @{ $TheColumns[$col++] = $currentValue } }
						
					}
					$Row
				}
				
			} | Where { $_.Count -gt 0 };
		}
		catch
		{ $problems += "$MySQL query failed because $($_)" }
		if ($simpleText)
		{ $Result = $Rows|foreach{[pscustomobject]$_}|Format-Table }
		else
		{ $Result = $Rows | ConvertTo-Json }
		if ($timing)
		{# we are getting timing data from the CLI tool
			$TimingRegex = '</TABLE>(?<rows>\d{1,20})\s{1,5}rows in set \((?<RealTime>[\d\.]{1,20})'
			if ($HTML -join '' -match $TimingRegex)
			{# if we found the timing information
				$timingData = [pscustomobject]$matches | convertto-json
				write-output "the transaction in '$Query' took $([decimal]([pscustomobject]$matches.RealTime)*1000) ms."
			}
			else
			{
				$timingData = '';
				
			}
			if (!($muted)) { $result };
		}
		else
		{
			$Result;
		}
		if ([string]::IsNullOrEmpty($FileBasedQuery)) { Remove-Item $TempInputFile };
	}
	if ($problems.Count -gt 0) { $Theargs.Problems.'GetdataFromMySQL' += $problems }
	if ($timing) { $Theargs.feedback.'FetchOrSaveDetailsOfParameterSet' += $timingData }
}

$GetdataFromPsql = {<# a Scriptblock way of accessing PosgreSQL via a CLI to get JSON-based  results without having to 
explicitly open a connection. it will take either SQL files or queries.
$query='SELECT  * FROM dbo.authors WHERE city=''Tacoma'';'
#>
	Param (
		$Theargs,
		#this is the same ubiquitous hashtable 
		$query,
		#a query. If a file, put the path in the $fileBasedQuery parameter
		$fileBasedQuery = $null,
		$simpleText = $false,
		$timing = $false,
		#do you return timing information
		$muted = $false #do you return the data       
	) # $GetdataFromPsql: (Don't delete this)
	
	$problems = @()
	$command = $null;
	$command = get-command psql -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($psqlAlias -ne $null)
		{ Set-Alias psql $psqlAlias }
		else
		{ $problems += 'You must have provided a path to psql in the ToolLocations.ps1 file in the resources folder' }
	}
	@('server', 'database', 'port', 'user', 'pwd') |
	foreach{ if ($TheArgs.$_ -in @($null, '')) { $problems += "Can't do this: no value for '$($_)'" } }
	
	if ($problems.Count -eq 0)
	{
		$TempOutputFile = "$($env:Temp)\TempOutput$(Get-Random -Minimum 1 -Maximum 900).csv"
		$TempInputFile = "$($env:Temp)\TempInput.sql"
		if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $TempInputFile = $FileBasedQuery }
		else
		{ [System.IO.File]::WriteAllLines($TempInputFile, $query); }
		Try
		{
			$Params = @(
				"--command=\timing $(if ($timing) { 'on' } else { 'off' })",
				"--dbname=$($TheArgs.database)",
				"--host=$($TheArgs.server)",
				"--username=$($TheArgs.user)",
				"--password=$($TheArgs.pwd)",
				"--port=$($TheArgs.Port -replace '[^\d]', '')",
				"--file=$TempInputFile",
				'--tuples-only',
				'-Pformat=unaligned',
				"--no-password")
			$env:PGPASSWORD = "$($TheArgs.pwd)"
			$result = (psql @params) -join "`r`n"
        }
		catch
		{ $problems += "$psql query failed because $($_)" }
		if ($?)
		{
		$result = ($result -replace 'Timing is (on|off)\.', '').trim();

			if ($timing)
			{
				# we are getting timing data from the CLI tool
				$TimingRegex = 'Time:\s{1,5}(?<RealTime>[\d\.]{1,20})\s{1,5}ms'
				$result = $result -join "`r`n"; #in case it comes back as an array
				if ($result -match $TimingRegex)
				{
					# if we found the timing information
					$timingData = [pscustomobject]$matches | convertto-json
					write-output "the transaction in '$Query' took $([pscustomobject]$matches.RealTime) ms."
					$result = $result -replace $TimingRegex, '';
				}
				else
				{
					$timingData = '';
					
				}
				if (!($muted)) { $result };
			}
			else
			{
				$Result;
			}
		}
		else { $problems += "The PSql CLI returned an error $($error[1])" }
	}
	if ($problems.Count -gt 0) { $Theargs.Problems.'GetdataFromPsql' += $problems }
}

$GetdataFromOracle = {<# a Scriptblock way of accessing oracle via Oracle SQLcl to get JSON results without having to 
explicitly open a connection. it will take SQL files and queries. 
$TheArgs=$DBDetails

#>
	Param ($Theargs,
		#this is the same ubiquitous hashtable 

		$query,
		#either a query that returns JSON, or a simple expression

		$fileBasedQuery = $null,
		#if you specify input from a file

		$simpleText = $false,
		#do you return timing information

		$timing = $false,
		#do we just want the timing information?

		$muted = $false #do you return the data
	) # $GetdataFromOracle: (Don't delete this)
	$problems = @();
    $response='';
	#check to see that we have the requisites
	$TheWallet = $TheArgs.ZippedWalletLocation;
	$Theservice = $TheArgs.service
	$TheUID = $TheArgs.uid
	$ThePassword = $TheArgs.pwd
	if ([string]::IsNullOrEmpty($TheArgs.ZippedWalletLocation) -or [string]::IsNullOrEmpty($TheArgs.service))
	{
		$Problems += "Cannot continue because name of either service ('$TheService'
    )  User ('$TheUID'), Password ('$ThePassword') or wallet ('$TheWallet') is not provided";
	}
	else
	{
		#the alias must be set to the path of your installed version of SQL Cmd
	    if ($OracleCmdAlias -eq $null -or (!(Test-Path -Path $OracleCmdAlias)) )
	    {
		    $problems += "'$OracleCmdAlias' is not a valid path. You must have provided a path to Oracle''s sqlcl.exe as OracleCmdAlias in the ToolLocations.ps1 file in the resources folder"
	    }
	}
	if ($problems.count -eq 0)
	{
		$TempSpoolOutputFile = "TempOutput$(Get-Random -Minimum 1 -Maximum 900).json"
		$TempQueryFile = "TempInput$(Get-Random -Minimum 1 -Maximum 900).SQL"
		$MaybeTiming = if ($timing) { "SET TIMING ON`n" }
		else { '' };
		$MaybeJSON = if ($simpleText) { '' }
		else { "set sqlformat json`n" };
		#do a command to set the output to JSON
		<# the CLI picks up its commands from a local file called login.sql rather
        than doing it at the command-line., and it reads its query from file too
        we now create the login file with the line to execute the command
        Then when we execute the CLI it just run the file.
        #>
        [System.IO.File]::WriteAllLines("$pwd\login.sql",@"
$($MaybeJSON)$($MaybeTimingSET)SET TERMOUT OFF 
SET VERIFY OFF
SET FEEDBACK OFF
set linesize 4000
set long 4000
set longchunksize 4000
spool $TempSpoolOutputFile
@$TempQueryFile
spool off
exit
"@)
        if (!([string]::IsNullOrEmpty($FileBasedQuery))) #if we've been passed a file ....
		{ $query = [System.IO.File]::ReadAllLines($FileBasedQuery) }
		[System.IO.File]::WriteAllLines("$pwd\$TempQueryFile", $Query)
		$env:SQLPATH = "$pwd"
        cmd.exe /c @"
`"$OracleCmdAlias`" -noupdates -S -L  -cloudconfig $TheWallet   $TheUID/$ThePassword@$Theservice  @$pwd/$TempQueryFile 
"@
		#we can't pass a query string directly so we have a file ...
		If (Test-Path -Path "$pwd/$TempQueryFile")
		{ Remove-Item "$pwd/$TempQueryFile" }
		
		#we have a file for the output too which we read in ...
		If (Test-Path -Path "$pwd/$TempSpoolOutputFile")
		{
			#make it easier for the caller to read the error
			$response = [IO.File]::ReadAllText("$pwd\$TempSpoolOutputFile") -ireplace '\d+? rows selected\.', '';
			Remove-Item "$pwd\$TempSpoolOutputFile"
		}
	    If (Test-Path -Path "$pwd\login.sql")
            {Remove-Item "$pwd\login.sql"}
		if ($response -like '*Error at*' -or $response -like '*Error report*')
		{ $Problems += " When connecting to oracle, we  had error $Response" }
		if ($problems.count -gt 0)
		{ @{ Error = $problems } | convertTo-json }
		elseif ($response -like 'NULL*')
		{ '' }
		else
		{ if (!($muted)) { ($response|convertfrom-json).results.items|convertTo-json } }
	If (Test-Path -Path "$pwd\login.sql")
        {Remove-Item "$pwd\login.sql"}
	}
}

$ExecutePLSQLScript = {<# a Scriptblock way of accessing oracle via Oracle SQLcl to get a set of
 JSON results without having to explicitly open a connection every time. it will take SQL files and queries 
 and also take an array of queries and filenames for the results. 
 $TheArgs=$DBDetails
#>
	Param ($Theargs,
		#this is the same ubiquitous hashtable 

		$Script = $null,
		#a valid SqlCl script or a valid file path 

		$Multiple = @()
		
	) # $ExecutePLSQLScript: (Don't delete this)
	$problems = @();
    $FilesToCleanUp = @()
	#check to see that we have the requisites
	$TheWallet = $TheArgs.ZippedWalletLocation;
	$Theservice = $TheArgs.service
	$TheUID = $TheArgs.uid
	$ThePassword = $TheArgs.pwd
	if ([string]::IsNullOrEmpty($TheArgs.ZippedWalletLocation) -or [string]::IsNullOrEmpty($TheArgs.service))
	{
		$Problems += "Cannot continue because name of either service ('$TheService'
    )  User ('$TheUID'), Password ('$ThePassword') or wallet ('$TheWallet') is not provided";
	}
	else
	{
		if ([string]::IsNullOrEmpty($OracleCmdAlias))
		{
			$OracleSqlPath = Get-Command 'sql.exe' -ErrorAction Ignore
			if ($OracleSqlPath -eq $null) #have you set the path
			{
				$problems += " please either provide a path for SqlCl or provide a OracleCmdAlias in the ToolLocations.ps1 file in the resources folder"
			}
			else
			{
				$OracleCmdAlias = $OracleSqlPath.Source #use the path 
			}
		}
	}
	if ($Multiple.count -gt 0)
	{
		$plSQLScript = $multiple | foreach -begin {
			$FilesToCleanUp = @(); $Workfile =@"
SET SQLFORMAT json
SET PAGESIZE 0
SET ECHO OFF
SET TERMOUT OFF 
SET VERIFY OFF
SET FEEDBACK OFF
set linesize 4000
set long 4000
set longchunksize 4000

"@
		} {
			$eachQueryFile = "$env:Tmp\TempInput$(Get-Random -Minimum 1 -Maximum 900).SQL"
			[System.IO.File]::writeAllLines($eachQueryFile, $_.SQL)
			$FilesToCleanUp += $eachQueryFile
			$Workfile = $Workfile + "spool $($_.ResultFile)`n@$eachQueryFile`nspool off`n"
		} -End { $Workfile + "`nexit`n" }
		
		$TempQueryFile = "TempMultipleInput$(Get-Random -Minimum 1 -Maximum 900).SQL"
        # write-warning " writing $plSQLScript to  $pwd\$TempQueryFile"
		[System.IO.File]::writeAllLines("$pwd\$TempQueryFile", $plSQLScript)
        $FinalscriptFile = "$pwd\$TempQueryFile"
        $FilesToCleanUp+= "$pwd\$TempQueryFile"
	}
	
	else #it is a simple string
	{
		if ([string]::IsNullOrEmpty($script))
		{
			$problems += "We need a script, please"
		}
		
		else
		{
			if (!(Test-Path -Path $script -PathType Leaf -ErrorAction Ignore))
			#if we've been not been passed a file ....
			{
				#we can't pass a query string directly so we have a file ...
				$TempQueryFile = "TempInput$(Get-Random -Minimum 1 -Maximum 900).SQL"
				[System.IO.File]::writeAllLines($TempQueryFile, $script)
				$FinalscriptFile = $TempQueryFile;
                $FilesToCleanUp+=$TempQueryFile
			}
			else { $FinalscriptFile = $script }
			
		}
	}

	if ($problems.count -eq 0)
	{
		$env:SQLPATH = "$pwd" #Use any local configuration you need
        $cmd = @"
`"$OracleCmdAlias`"  -noupdates -S -L    -cloudconfig $TheWallet   $TheUID/$ThePassword@$Theservice   @$FinalscriptFile
"@
	    cmd.exe /c $cmd
        
 	}
    $FilesToCleanUp|foreach{
        If (Test-Path -Path "$_")
	        {
                  Remove-Item "$_"
            }
        }
}
<# 
Note: now deprecated!
This scriptblock allows you to save and load the shared parameters for all these 
scriptblocks under a name you give it. you'd choose a different name for each database
within a project, but make them unique across projects 
If the parameters have the name defined but vital stuff missing, it fills 
in from the last remembered version. If it has the name and the vital stuff then it assumes
you want to save it. If no name then it ignores. 
#>
$FetchOrSaveDetailsOfParameterSet = {
	Param ($param1) # $FetchOrSaveDetailsOfParameterSet: (Don't delete this)
	$Param1.'Problems' = @{ };
	$Param1.'Warnings' = @{ };
	$Param1.'WriteLocations' = @{ };
	$problems = @();
	if ($Param1.name -ne $null -and $Param1.project -ne $null)
	{
		# define where we store our secret project details
		$StoredParameters = "$($env:USERPROFILE)\Documents\Deploy\$(($param1.Project).Split([IO.Path]::GetInvalidFileNameChars()) -join '_')";
		$ParametersFilename = 'AllParameters.json';
		$TheLocation = "$StoredParameters\$($Param1.Name)-$ParametersFilename"
	}
	$VariablesWeWant = @(
			'server', 'uid', 'port', 'project', 'database', 'projectFolder', 'projectDescription'
		);	
	if ($Param1.name -ne $null -and $Param1.project -ne $null -and $Param1.Server -eq $null -and $Param1.Database -eq $null)
	{
		# we don't want to keep passwords unencrypted and we don't want any of the transitory
		# variables that provide state (warnings, error, version number and so on)
		# If the directory doesn't exist then create it
		if (!(test-path -Path $StoredParameters -PathType Container))
		{ $null = New-Item -Path $StoredParameters -ItemType "directory" -Force }
		#if the file already exists then read it in.
		
		if (test-path $TheLocation -PathType leaf)
		{
			# so we read in all the details
			try
			{
				$JSONcontents = Get-Content -Path $TheLocation -Raw
				$TheActualParameters = $JSONcontents | ConvertFrom-json
			}
			catch
			{
				$Problems += "Cannot read file $TheLocation because it has unescaped content"
			}
			if ($problems.count -eq 0)
			{
				$TheActualParameters.psobject.properties |
				Foreach { if ($_.Name -in $VariablesWeWant) { $param1[$_.Name] = $_.Value } }
				write-verbose "fetched details from $TheLocation"
				$VariablesWeWant | where { $_ -notin @('port', 'ProjectFolder', 'projectDescription') } |
				foreach {
					if ($param1.$_ -eq $null) { $Problems += "missing a value for $($_)" }
				}
			}
		}
		else
		{
			$Problems += "Could not find project file $TheLocation"
		}
	}
	#if the user wants to save or resave they'll the name AND include both the the server and database
	elseif ($Param1.Name -ne $null -and $Param1.Server -ne $null -and $Param1.Database -ne $null)
	{
		$RememberedHashTable = @{ }
		$VariablesWeWant | foreach{ $RememberedHashTable.$_ = $param1.$_ }
		$RememberedHashTable | ConvertTo-json |
		out-file -FilePath $TheLocation -Force
		"Saved details to $TheLocation"
	}
	if ($Param1.'Checked' -eq $null) { $Param1.'Checked' = $false; }
	# has it been checked against a source directory?
	@('server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
	if ($TheLocation -ne $null)
	{ $Param1.WriteLocations.'FetchOrSaveDetailsOfParameterSet' = $TheLocation; }
	if ($problems.Count -gt 0)
	{ $Param1.Problems.'FetchOrSaveDetailsOfParameterSet' += $problems; }
    $Param1.feedback.'FetchOrSaveDetailsOfParameterSet' += 'This FetchOrSaveDetailsOfParameterSet scriptblock is deprecated!';
}

<# now we format the Flyway parameters #>
<# Sometimes we need to run Flyway, and the easiest approach is to create a hashtable
of the information Flyway needs. It is different to the one we use for each of these
scriptblocks  #>

$FormatTheBasicFlywayParameters = {
	Param ($param1) # $FormatTheBasicFlywayParameters (Don't delete this)
	$problems = @();
	@('server', 'database', 'projectFolder') |
	   foreach{ if ($param1.$_ -in @($null,'')) { $problems += "no value for '$($_)'" } }
	$migrationsPath= if ([string]::IsNullOrEmpty($param1.migrationsPath)) {'\scripts'} else {"\$($param1.migrationsPath)"}
    if ([string]::IsNullOrEmpty($param1.Reportdirectory)){$migrationsPath=''};# compatibility problem
	$MaybePort = "$(if ([string]::IsNullOrEmpty($param1.port)) { '' }
		else { ":$($param1.port)" })"
	if (!([string]::IsNullOrEmpty($param1.uid)))
	{
		if ($param1.pwd -eq $null) { $problems += "no password provided for $($($param1.uid))" }
		$FlyWayArgs =
		@("-url=jdbc:sqlserver://$($param1.Server)$maybePort;databaseName=$($param1.Database)",
			"-WriteLocations=filesystem:$($param1.ProjectFolder)$migrationsPath", <# the migration folder #>
			"-user=$($param1.uid)",
			"-password=$($param1.pwd)")
	}
	else <# we need to use an integrated security flag in the connection string #>
	{
		$FlyWayArgs =
		@("-url=jdbc:sqlserver://$($param1.Server)$maybePort;databaseName=$(
				$param1.Database
			);integratedSecurity=true",
			"-WriteLocations=filesystem:$($param1.ProjectFolder)")<# the migration folder #>
	}
	
	$FlyWayArgs += <# the project variables that we reference with placeholders #>
	@("-placeholders.projectDescription=$($param1.ProjectDescription)",
		"-placeholders.projectName=$($param1.Project)",
		"-community") <# Change this if using teams!! #>
	$Param1.FlywayArgs = $FlyWayArgs
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'FormatTheBasicFlywayParameters' += $problems;
	}
$Param1.feedback.'FormatTheBasicFlywayParameters' += 'This FormatTheBasicFlywayParameters scriptblock is deprecated!';
}



<# This scriptblock looks to see if we have the passwords stored for this userid, Database and RDBMS
if not we ask for it and store it encrypted in the user area
 #>
$FetchAnyRequiredPasswords = {
	Param ($param1) # $FetchAnyRequiredPasswords (Don't delete this)
	$problems = @()
	try
	{
       @('server') |
		foreach{ if ($param1.$_ -in @($null, '')) { $problems += "no value for '$($_)'" } }
		# some values, especially server names, have to be escaped when used in file paths.
		if ($problems.Count -eq 0)
		{
			$escapedServer = ($Param1.server.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
			# now we get the password if necessary
			
			if (!([string]::IsNullOrEmpty($param1.uid))) #then it is using SQL Server Credentials
			{
				# we see if we've got these stored already. If specifying RDBMS, then use that.
				if ([string]::IsNullOrEmpty($param1.RDBMS))
				{ $SqlEncryptedPasswordFile = "$env:USERPROFILE\$($param1.uid)-$($escapedServer).xml" }
				else
				{ $SqlEncryptedPasswordFile = "$env:USERPROFILE\$($param1.uid)-$($escapedServer)-$($param1.RDBMS).xml" }
				# test to see if we know about the password in a secure string stored in the user area
				if (Test-Path -path $SqlEncryptedPasswordFile -PathType leaf)
				{
					#has already got this set for this login so fetch it
					$SqlCredentials = Import-CliXml $SqlEncryptedPasswordFile
				}
				else #then we have to ask the user for it (once only)
				{
					# hasn't got this set for this login
					$SqlCredentials = get-credential -Credential $param1.uid
					# Save in the user area 
					$SqlCredentials | Export-CliXml -Path $SqlEncryptedPasswordFile
        <# Export-Clixml only exports encrypted credentials on Windows.
        otherwise it just offers some obfuscation but does not provide encryption. #>
				}
				
			$param1.Uid = $SqlCredentials.UserName;
			$param1.Pwd = $SqlCredentials.GetNetworkCredential().password
			}
        }
	}
	catch
	{
		$Param1.Problems.'FetchAnyRequiredPasswords' +=
             "$($PSItem.Exception.Message) at line $($_.InvocationInfo.ScriptLineNumber)"
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'FetchAnyRequiredPasswords' += $problems;
	}
    if (!([string]::IsNullOrEmpty($param1.uid)) -and [string]::IsNullOrEmpty($param1.Pwd))
         {Write-warning "returned no password"}
}

$DoesTheFlywayTableExist = { <#This checks to see if there is a flyway schema table 
in the database. It sets a value in the dbDetails object (FlywayTableExists) that is true or false #>
	Param ($param1) # $DoesTheFlywayTableExist parameter is a hashtable 
	$problems = @();
	$doit = $true;
	@('server', 'rdbms') | foreach{
		if ($param1.$_ -in @($null, ''))
		{
			$Problems += "no value for '$($_)'";
			$DoIt = $False;
		}
	}
	switch -Regex ($param1.RDBMS)
	{
		'sqlserver'   {
			$Exists = Execute-SQL $param1 @"
SELECT CASE WHEN (EXISTS
    (SELECT 1
        FROM INFORMATION_SCHEMA.TABLES
        WHERE  TABLE_NAME LIKE '$($param1.flywayTableName)'
        AND TABLE_SCHEMA = '$($param1.DefaultSchema)')) 
            THEN 1 ELSE 0 END AS ItExists
FOR JSON PATH
"@ | Convertfrom-json
		}
        'oracle'
        {
$Exists = Execute-SQL $param1 @"
select count(*) as ItExists
        FROM ALL_TABLES
        WHERE  TABLE_NAME LIKE '$($param1.flywayTableName)'
        AND OWNER = '$($param1.DefaultSchema)'; 
"@ | Convertfrom-json        }    


		'postgresql'
		{
			# Do it the PostgreSQL way
$Exists = Execute-SQL $param1 @"
SELECT CASE WHEN (EXISTS
    (SELECT 1
        FROM INFORMATION_SCHEMA.TABLES
        WHERE  TABLE_NAME LIKE '$($param1.flywayTableName)'
        AND TABLE_SCHEMA = '$($param1.DefaultSchema)')) 
            THEN 1 ELSE 0 END AS ItExists;
"@ | Convertfrom-json
		}
		'sqlite'
		{
			## OK, lets do it the SQLite way.
			$Version = Execute-SQL $param1 @"
SELECT count(*) as ItExists FROM sqlite_master 
WHERE type='table' AND name='$($param1.flywayTableName)
"@ | Convertfrom-json
			$Param1.ServerVersion = $Version.version
		}
		'mysql|mariadb'
		{
			#get MariaDB version
$Exists = Execute-SQL $param1 @"
SELECT count()
        FROM INFORMATION_SCHEMA.TABLES
        WHERE  TABLE_NAME LIKE '$($param1.flywayTableName)'
        AND TABLE_SCHEMA = '$($param1.DefaultSchema)'; 
"@ | Convertfrom-json
		}
		Default { $problems += "$($param1.RDBMS) is not supported yet. " }
	}
    $param1.'FlywayTableExists'=if ($Exists.ItExists -gt 0) {$true} else {$False};
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'$DoesTheFlywayTableExist' += $problems;
	}
	else
	{
		$Param1.feedback.'$DoesTheFlywayTableExist' = "The Flyway table $($param1.DefaultSchema).$($param1.flywayTableName) does$(if ($Exists.ItExists -gt 0) {' '} else {'not yet '})exist"
	}
}



<#This scriptblock checks the code in the database for any issues,
using SQL Code Guard to do all the work. This runs SQL Codeguard 
and saves the report in a subdirectory the version directory of your 
project artefacts. It also reports back in the $DatabaseDetails
Hashtable. It checks the current database, not the scripts
 #>
$CheckCodeInDatabase = {
	Param ($param1) # $CheckCodeInDatabase - (Don't delete this)
	#you must set this value correctly before starting.
	$Problems = @(); #our local problem counter  
    $Feedback=@(); 
    $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'; 
    $command=$null;
    $command = get-command Codeguard -ErrorAction Ignore 
	if ($command -eq $null)
	{    if ($CodeGuardAlias -ne $null)
        {Set-Alias CodeGuard   $CodeGuardAlias }
    else
        {$problems += 'You must have provided a path to CodeGuard.exe in the ToolLocations.ps1 file in the resources folder'}
        }
    try
    {
	#check that all the values we need are in the hashtable
	@('server', 'Database', 'version', 'Project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
	#now we create the parameters for CodeGuard.
    $escapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
	$MyDatabasePath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports"} 
        else {"$($param1.reportLocation)\$($param1.Version)\Reports"} #else the simple version
	if ($MyDatabasePath -like '*\\*'){ } { $Problems += "created an illegal path '$MyDatabasePath'" }
    $Arguments = @{
		server = $($param1.server) #The server name to connect
		Database = $($param1.database) #The database name to analyze
		outfile = "$MyDatabasePath\codeAnalysis.xml" <#
        The file name in which to store the analysis xml report#>
		#exclude='BP007;DEP004;ST001' 
		#A semicolon separated list of rule codes to exclude
		include = 'all' #A semicolon separated list of rule codes to include
	}

	#add the arguments for credentials where necessary
	if (!([string]::IsNullOrEmpty($param1.uid)))
	{
		$Arguments += @{
			User = $($param1.uid)
			Password = $($param1.pwd)
		}
	}
	# we need to make sure tha path is there
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $MyDatabasePath;
	}
	<# we only do the analysis if it hasn't already been done for this version,
    and we've hit no problems #>
    $AlreadyDone= Test-Path -PathType leaf  "$MyDatabasePath\codeAnalysis.xml"
    if (($problems.Count -eq 0) -and (-not $alreadyDone))
	{
		$result = codeguard @Arguments; #execute the command-line Codeguard.
		if ($? -or $LASTEXITCODE -eq 1)
		{
        $CodeAnalysis =[xml] [IO.File]::ReadAllText("$MyDatabasePath\codeAnalysis.xml")
        $object=convertfrom-xml $CodeAnalysis
        $object|convertTo-json -depth 10 > "$MyDatabasePath\CodeAnalysis.json"

		$feedback += "Written Code analysis for $($param1.Project) $($param1.Version
			) to $MyDatabasePath\codeAnalysis.xml"
		}
		else
		{
			<#report a problem and send back the args for diagnosis 
            (hint, only for script development) #>
			$CLIArgs = '';
			$CLIArgs += $Arguments |
			foreach{ "$($_.Name)=$($_.Value)" }
			$problems += "CodeGuard responded '$result' with error code $LASTEXITCODE when used with parameters $CLIArgs."
		}
		#$Problems += $result | where { $_ -like '*error*' }
	}
}
	catch
	{
		$Param1.Problems.'CheckCodeInDatabase' +=
             "$($PSItem.Exception.Message) at line $($_.InvocationInfo.ScriptLineNumber)"
	}

	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'CheckCodeInDatabase' += $problems;
	}
	else
    {
    if ($AlreadyDone) {$Param1.feedback.'CheckCodeInDatabase'="There is already a database code check for version $($param1.version)"}
    else 
        { $Param1.feedback.'CheckCodeInDatabase' += $Feedback }   
    $Param1.WriteLocations.'CheckCodeInDatabase' = "$MyDatabasePath\codeAnalysis.xml";}
}


<#This scriptblock checks the code in the migration files for any issues,
using SQL Code Guard to do all the work. This runs SQL Codeguard 
and saves the report in a subdirectory the version directory of your 
project artefacts. It also reports back in the $DatabaseDetails
Hashtable. It checks the scripts not the current database.
#>

$CheckCodeInMigrationFiles = {
	Param ($param1) # $CheckCodeInMigrationFiles - (Don't delete this)
	#you must set this value correctly before starting.
	$Problems = @(); #our local problem counter
    $Feedback = @();
    $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';
	$command=$null;
    $command = get-command Codeguard -ErrorAction Ignore 
    if ($command -eq $null) 
	{    if ($CodeGuardAlias -ne $null)
        {Set-Alias CodeGuard   $CodeGuardAlias }
    else
        {$problems += 'You must have provided a path to CodeGuard.exe in the ToolLocations.ps1 file in the resources folder'}
        }
	#check that all the values we need are in the hashtable
	@('ProjectFolder') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
    $escapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'

	#$migrationsPath= if ([string]::IsNullOrEmpty($param1.migrationsPath)) {'\scripts'} else {"\$($param1.migrationsPath)"}
	#if ([string]::IsNullOrEmpty($param1.Reportdirectory)){$migrationsPath=''};# compatibility problem

    dir "$($param1.projectFolder)\V*.sql" | foreach{
		$Thepath = $_.FullName;
		$TheFile = $_.Name
		$Theversion = ($_.Name -replace 'V(?<Version>[.\d]+).+', '${Version}')
        if ([version]$Theversion -le [version]$Param1.version)
            {
		    #now we create the parameters for CodeGuard.
		    $MyVersionReportPath = 
              if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
                {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports"} 
              else {"$($param1.reportLocation)\$($param1.Version)\Reports"} #else the simple version
		    $Arguments = @{
			    source = $ThePath
			    outfile = "$MyVersionReportPath\FilecodeAnalysis.xml" <#
                The file name in which to store the analysis xml report#>
			    #exclude='BP007;DEP004;ST001' 
			    #A semicolon separated list of rule codes to exclude
			    include = 'all' #A semicolon separated list of rule codes to include
		    }
		    # we need to make sure tha path is there
		    if (-not (Test-Path -PathType Container $MyVersionReportPath))
		    {
			    # does the path to the reports directory exist?
			    # not there, so we create the directory 
			    $null = New-Item -ItemType Directory -Force $MyVersionReportPath;
		    }
            $alreadyDone=Test-Path -PathType leaf  "$MyVersionReportPath\FilecodeAnalysis.xml"
	        <# we only do the analysis if it hasn't already been done for this version,
            and we've hit no problems #>
		    if (($problems.Count -eq 0) -and (-not ( $alreadyDone)))
		    {
			    $result = codeguard @Arguments; #execute the command-line Codeguard.
			    if ($? -or $LASTEXITCODE -eq 1)
			    {
                $CodeAnalysis =[xml] [IO.File]::ReadAllText("$MyVersionReportPath\FilecodeAnalysis.xml")
                $object=convertfrom-xml $CodeAnalysis
                $object|convertTo-json -depth 10 > "$MyVersionReportPath\FilecodeAnalysis.json"
				$feedback += "Written file Code analysis for $TheFile for $($param1.Project) project) to $MyVersionReportPath\FilecodeAnalysis.xml and .json"
			    }
			    else
			    {
			        <#report a problem and send back the args for diagnosis 
                    (hint, only for script development) #>
				    $CLIArgs = '';
				    $CLIArgs += $Arguments.GetEnumerator() |
				    foreach{ "$($_.Name)=$($_.Value)" }
				    $problems += "CodeGuard responded '$result' with error code $LASTEXITCODE when used with parameters $CLIArgs."
			    }
			    #$Problems += $result | where { $_ -like '*error*' }
		    }
        }
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'CheckCodeInMigrationFiles' += $problems;
	}
    else
    {
    if ($AlreadyDone) {$Param1.feedback.'CheckCodeInMigrationFiles'="There is already a database code check for version $($param1.version)"}
    else 
        { $Param1.feedback.'CheckCodeInMigrationFiles' += $Feedback }   

    $Param1.WriteLocations.'CheckCodeInMigrationFiles' = "$MyVersionReportPath\FilecodeAnalysis.xml"; }
	$Param1.feedback.'CheckCodeInMigrationFiles' += $Feedback
		
}


<#This scriptblock checks the code in the pending files for any issues,
using SQL Fluff to do all the work. It saves the report in a subdirectory 
of the version directory of your project artefacts. It also reports back 
in the $DatabaseDetails Hashtable. 
#>
$CheckFluffInPendingFiles = {
	Param ($param1) # $CheckFluffInPendingFiles - (Don't delete this)
	#you must set this value correctly before starting.
	$Problems = @(); #our local problem counter
	$Feedback = @();
	$FilesProcessed = 0;
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';
	#these are the post-shredding tidy-up substitutions in Regex form
	$ValueAlterations = @(('(?m:^)\s{1,40}?\|', ''), ("`n", ''), ("`r", ''))
    <#The Regex for slicing up each SQLFluff record. I've commented it to make it
    easier to read #>
	$TheRegex = @'
(?m:^)(?#Get the line number
)L: {1,5}(?<Line>\d{1,4}) {0,10}(?#
Get the position (column]
)\| {1,4}P: {1,10}(?<Position>\d{1,10}) {1,4}(?#
Get the problem number
)\| {1,4}(?<Problem>\w\S{1,5}) {1,4}(?#
Find the description
)\| {1,4}(?<Description>(?s:.){1,200}?(?=\n\w|\z))
'@
	$Warnings = @() #an array to collect up all the warnings
	$ParsingErrors = @() #an array to store all the parsing errors
	$CompleteProblemData = @() #an array to store all the issues
	# to get the dialects, use sqlfluff.exe dialects
	$Dialect = switch -Regex ($param1.RDBMS)
	{
		'sqlserver'     {
			'tsql'
		}
		'postgresql'    {
			'postgres'
		}
		'oracle'    {
			'oracle'
		}
		'sqlite'	    {
			'sqlite'
		}
		'mysql|mariadb'	{
			'mysql'
		}
		default
		{ 'Error' }
	}
	# check to make sure you have installed SQLFluff
	if ($Dialect -eq 'Error')
	{ $problems += "No SQL dialect specified by $($param1.RDBMS)" }
	if ((Get-Command -WarningAction SilentlyContinue 'sqlfluff.exe').name -ne 'sqlfluff.exe')
	{ $problems += "please install SQLFluff.exe using Python" }
    <# now we get from flyway a list of all the migration files from the
    info command and turn it into a PowerShell object #>	
	$Migrations = Flyway info -outputType=json | convertfrom-json
	if ($Migrations.error -ne $null)
	{
		# something wrong within Flyway. Need to deal with it
		$problems += $Migrations.error.message
	}
	if ($Problems.Count -eq 0)
	{
        <# work through the list of files, using just the SQL Flies that are pending.
        We wont do the successfully-applied files because it would upset Flyway 
        if we were to alter them #>
		$migrations.migrations | `
		where { ![string]::IsNullOrEmpty($_.filepath) -and ($_.type -ieq 'SQL') -and ($_.state -ieq 'Pending') } | `
		foreach -Begin { $filesProcessed = 0 } {
			# of the right type of file.
			$filesProcessed++;
			$The_warning = '';
			$TheVersion = $_.version; # the versio attached to the file
			# we'll put each file into the version folder. You might want them in a different plce
			$ReportLocation = "$($param1.reportLocation)\$TheVersion\reports"
			if (-not (Test-Path "$ReportLocation"))
			{ New-Item -ItemType Directory -Path "$ReportLocation" -Force }
            <# you might need to provide other configuration information here
              fix       Fix SQL files.
              lint      Lint SQL files via passing a list of files or using stdin #>
			$report = sqlfluff.exe lint --dialect $dialect "$($_.filepath)"
			#collect any warnings you want listed
			$Warnings += $report | where { $_ -ilike 'warning*' }
			#write out the raw report
			$report > "$ReportLocation\SQLFluff.rpt"
			#Slice up the rather odd formatting and read it into powershell
			#ConvertFrom-Regex is in the resources. It is for text-based data
			$ThisFileAnalysis = ConvertFrom-Regex -source ($report -join "`r`n") `
												  -TheRegex $TheRegex `
												  -ValueAlterations $ValueAlterations
			if ($ThisFileAnalysis -eq $null)
			{ $FluffProblems += "error converting $ReportLocation\SQLFluff.rpt to JSON" }
			#add the current version so we know which file it was in etc.
			$ThisFileAnalysis | foreach{
				$_ | Add-Member -MemberType NoteProperty -Name 'Version' -Value $TheVersion
			}
			#Write out this list of psCustomObjects as a JSON file for later use
			$ThisFileAnalysis | ConvertTo-Json >"$ReportLocation\SQLFluff.json"
			#and build up a complete list for reporting
			$CompleteProblemData += $ThisFileAnalysis
		}
		if ($filesProcessed -eq 0)
		{ $FluffProblems += 'No pending migration files were found that could be run' }
		#Report the complete list of issued
		if ($CompleteProblemData.Count -eq 0) { $FluffProblems += 'empty JSON report ' }
		else
		{
			$CompleteProblemData | convertTo-json > "$($param1.reportLocation)\FluffIssues.json"
		}
		#extract the parsing errors
		$Fluffproblems = $CompleteProblemData | where { $_.Problem -notlike 'L*' }
		$Fluffproblems | convertTo-json > "$($param1.reportLocation)\FluffProblems.json"
		#Display parsing Errors.
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'CheckFluffInPendingFiles' += $problems;
	}
	if ($FluffProblems.Count -gt 0)
	{
		$Param1.warnings.'CheckFluffInPendingFiles' += $FluffProblems;
	}
	$Param1.feedback.'CheckFluffInPendingFiles' += $Feedback
	$Param1.WriteLocations.'CheckFluffInPendingFiles' = "$($param1.reportLocation)\FluffProblems.json";
}


<#This scriptblock gets the current version of a flyway_schema_history data from the 
table in the database. if there is no Flyway Data, then it returns a version of 0.0.0

 #>
$GetCurrentVersion = {
	Param ($param1) # $GetCurrentVersion parameter is a hashtable 
	$problems = @();
	$doit = $true;
	@('rdbms','flywaytable') | foreach{
		if ($param1.$_ -in @($null, ''))
		{
			$Problems += "no value for '$($_)'";
			$DoIt = $False;
		}
	}
	$flywayTable = $Param1.flywayTable
    $flywayTableForOracle = "$($Param1.DefaultSchema).`"$($Param1.flywayTableName)`""
	if ($flywayTable -eq $null)
	{ $flywayTable = 'dbo.flyway_schema_history' }
	$Version = 'unknown'
    $AllVersions=@{}
    $LastAction=@{}

	if ($param1.RDBMS -eq 'sqlserver')
	{
		# Do it the SQL Server way.
		$AllVersions = $GetdataFromSQLCMD.Invoke(
			$param1, "
    SELECT DISTINCT version
      FROM $flywayTable
      WHERE version IS NOT NULL
    FOR JSON AUTO") |
		convertfrom-json
		$LastAction = $GetdataFromSQLCMD.Invoke(
			$param1, "
    SELECT version, type
          FROM $flywayTable
      WHERE
      installed_rank =
        (SELECT Max (installed_rank) FROM $flywayTable
           WHERE success = 1)
    FOR JSON AUTO") |
		convertfrom-json
	} ## we do it the postgresql way
	elseif ($param1.RDBMS -eq 'postgresql')
	{
		# Do it the PostgreSQL way
		$AllVersions = Execute-SQL $param1  "
            SELECT json_agg(e) 
            FROM (SELECT DISTINCT version
      FROM $($param1.flywayTable)
      WHERE version IS NOT NULL
      )e;          
    " | convertfrom-json
		$LastAction = Execute-SQL $param1 "
      SELECT json_agg(e) 
            FROM (SELECT version, type
      FROM $($param1.flywayTable)
      WHERE
      installed_rank =
        (SELECT Max (installed_rank) FROM $flywayTable
           WHERE success = true))e;
    " | convertfrom-json
	} ## OK, lets do it the SQLite way.
	elseif ($param1.RDBMS -eq 'sqlite')
	{
		# Do it the SQLite way
		$AllVersions = $GetdataFromsqlite.Invoke(
			$param1, "SELECT DISTINCT version
      FROM $($param1.flywayTable)
      WHERE version IS NOT NULL
    ") | convertfrom-json
		$LastAction = $GetdataFromSqlite.Invoke(
			$param1, "SELECT version, type
      FROM $($param1.flywayTable)
      WHERE
      installed_rank =
        (SELECT Max (installed_rank) FROM $flywayTable
           WHERE success = 1)
    ") | convertfrom-json
	}
    elseif ($param1.RDBMS -in @('mysql','mariadb'))
	{
		# Do it the MySQL way
		$AllVersions = Execute-SQL $param1  "
        SELECT DISTINCT version
        FROM $($param1.flywayTable)
      WHERE version IS NOT NULL          
    " | convertfrom-json
		$LastAction = Execute-SQL $param1 "
      SELECT version, type
      FROM $($param1.flywayTable)
      WHERE
      installed_rank =
        (SELECT Max(installed_rank) FROM $flywayTable
           WHERE success = true);
    " | convertfrom-json
	}
    elseif ($param1.RDBMS -eq 'oracle')
	{
    $ExecuteResult=$ExecutePLSQLScript.invoke($param1, $null, @(
		@{
			ResultFile = 'AllVersions.json';
			SQL =@"
SELECT DISTINCT `"version`"
        FROM $flywayTableForOracle
      WHERE `"version`" IS NOT NULL;
"@
		},
		@{
			ResultFile = 'LastAction.json';
			SQL =@"
SELECT `"version`", `"type`"
      FROM $flywayTableForOracle
      WHERE
      `"installed_rank`" =
        (SELECT Max(`"installed_rank`") FROM $flywayTableForOracle
           WHERE `"success`" = 1);
"@ }))
        if (![string]::IsNullOrEmpty(($ExecuteResult|ConvertFrom-StringData).'Error Message'))
            { $problems += ($ExecuteResult|ConvertFrom-StringData).'Error Message';
             $problems +=  $param1.uid+' and '+$param1.pwd	
             }
        $response = get-content allversions.json
        if (($Response -join '') -like 'error*')
        { $allversions.error=$Response -join ' ' }
        else
        { $allversions = ($response | convertfrom-json).results.items }
        $response = get-content LastAction.json
        if (($Response -join '') -like 'error*')
        { $LastAction.error= $Response -join ' ' }
        else
        { $LastAction = ($response | convertfrom-json).results.items }
        @('allversions.json', 'LastAction.json') | foreach{
	        If (Test-Path -Path "$_")
	        {
		        Remove-Item "$_"
	        }
        }
	}
	else { $problems += "$($param1.RDBMS) is not supported yet. " }
	if ($AllVersions.error -ne $null) { $problems += $AllVersions.error }
	if ($LastAction.error -ne $null) { $problems += $LastAction.error }
	if ($AllVersions -eq $null) { $problems += 'No response for version list' }
	if ($LastAction -eq $null) { $problems += 'no response for last migration' }
    $version = '0.0.0'; $previous = '0.0.0'
	if ($problems.count -eq 0)
	{
		$OrderedVersions = $AllVersions.version | foreach{
			new-object System.Version ($_)
		} |
		sort | % -Begin { $ii = 1 }{
			[pscustomobject]@{ 'Order' = $ii++; 'version' = $_.ToString() }
		};
		
		$VersionOrder = $OrderedVersions |
		where{ $_.version -eq $Lastaction.version } | Select Order -First 1;
		$previous = ($OrderedVersions |
			where{ $_.Order -eq $VersionOrder.Order - 1 } |
			Select version -First 1).version;
        if ($previous -eq $null) {$previous = '0.0.0'}
		$version = $LastAction.version;
		if ($LastAction.type -like 'UNDO*')
		{
			$param1.Version = $previous;
			$param1.Previous = $Version;
		}
		else
		{
			$param1.Version = $version;
			$param1.Previous = $previous;
		}
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'GetCurrentVersion' += $problems;
	}
	$Param1.feedback.'GetCurrentVersion' = "current version is $version, previous $previous."
}


<#This scriptblock gets the current version of the RDBMS on the server
and is used mainly to check that the migration doesn't use any
functionality that can't be supported on that server version. It updates
the $Param1.ServerVersion 
 #>
$GetCurrentServerVersion = {
	Param ($param1) # $GetCurrentServerVersion parameter is a hashtable 
	$problems = @();
	$doit = $true;
	@('server', 'rdbms') | foreach{
		if ($param1.$_ -in @($null, ''))
		{
			$Problems += "no value for '$($_)'";
			$DoIt = $False;
		}
	}
	switch -Regex ($param1.RDBMS)
	{
		'sqlserver'   {
			$Version = Execute-SQL $param1 @'
SELECT 'SQL20'+ CASE Left(Cast(ServerProperty ('productversion') AS VARCHAR(80)), 
				CharIndex ('.', Cast(ServerProperty ('productversion') AS VARCHAR(80))))
		 WHEN '8.'  THEN '00'
         WHEN '9.'  THEN '05'
         WHEN '10.' THEN '08'
         WHEN '11.' THEN '12'
         WHEN '12.' THEN '14'
         WHEN '13.' THEN '16'
         WHEN '14.' THEN '17'
         WHEN '15.' THEN '19' ELSE 'xx?' END AS Version,
       ServerProperty ('ProductLevel') AS ProductLevel,
       ServerProperty ('Edition') AS Edition,
       ServerProperty ('ProductVersion') AS ProductVersion
FOR JSON PATH
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.version
		}
        'oracle'
        {
        
			# Do it the oracle way
            try
			{$Version = Execute-SQL $param1 'SELECT BANNER FROM v$version;' | Convertfrom-json}
            catch
            {$Problems += "Error. returned '$Version'"}
            
			$Param1.ServerVersion =  $Version[1].banner

        }    


		'postgresql'
		{
			# Do it the PostgreSQL way
			$Version = Execute-SQL $param1 @'
SELECT json_agg(e) FROM (SELECT Version() as "DatabaseVersion")e;
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.DatabaseVersion
		}
		'sqlite'
		{
			## OK, lets do it the SQLite way.
			$Version = Execute-SQL $param1 @'
select sqlite_version() as version;
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.version
		}
		'mysql|mariadb'
		{
			#get MariaDB version
			$Version = Execute-SQL $param1 @'
SHOW VARIABLES LIKE "version"; ;
'@ | Convertfrom-json
			$Param1.ServerVersion = $Version.value
		}
		Default { $problems += "$($param1.RDBMS) is not supported yet. " }
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'$GetCurrentServerVersion' += $problems;
	}
	else
	{
		$Param1.feedback.'$GetCurrentServerVersion' = "current $($param1.RDBMS) version is $($Param1.ServerVersion)."
	}
}



<# This uses SQL Compare to check that a version of a database is correct and hasn't been changed.
It returns comparison equal to true if it was the same or false if there has been drift, 
with a list of objects that have changed. if the comparison returns $null, then it means there 
has been an error #>

$IsDatabaseIdenticalToSource = {
	Param ($param1) # $IsDatabaseIdenticalToSource (Don't delete this)
	$problems = @();
	$warnings = @();
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null, '')) { $problems += "no value for '$($_)'" } }
	if ($param1.Version -eq '0.0.0') { $identical = $null; $warnings += "Cannot compare an empty database" }
	$GoodVersion = try { $null = [Version]$param1.Version; $true }
	catch { $false }
	if (-not ($goodVersion))
	{ $problems += "Bad version number '$($param1.Version)'" }
	#the alias must be set to the path of your installed version of SQL Compare
	$command = $null;
	$command = get-command SQLCompare -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($SQLCompareAlias -ne $null)
		{ Set-Alias SQLCompare $SQLCompareAlias; }
		else
		{ $problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder' }
	}
	$escapedProject = ($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	if ($problems.Count -eq 0)
	{
		
		$SourcePath = if ([string]::IsNullOrEmpty($param1.SourcePath)) { 'Source' }
		else { "$($param1.SourcePath)" }
		#the database scripts path would be up to you to define, of course
		$MyDatabasePath =
		if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
		{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$sourcePath" }
		else { "$($param1.reportLocation)\$($param1.Version)\$sourcePath" } #else the simple version
		$CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
			# can use a hash-table 
			"/Scripts1:$MyDatabasePath",
			"/server2:$($param1.server)",
			"/database2:$($param1.database)",
			"/Assertidentical",
			"/force",
			"/LogLevel:Warning"
		)
		
		if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
		{
			$CLIArgs += @(
				"/username2:$($param1.uid)",
				"/Password2:$($param1.pwd)"
			)
		}
        #we don't want the schema history table
        $schemaAndName=$param1.flywayTable -split '\.';
		if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
		{
			$CLIArgs += @(
				"/filter:$($param1.filterpath)"
			)
		}
		else
		{
			$CLIArgs += @(
                
				"/exclude:table:$(( $schemaAndName[1],$schemaAndName[0] -ne $null)[0])",
				'/exclude:ExtendedProperty') #trivial}
		}
	}
	$MyVersionReportPath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports" }
	else { "$($param1.reportLocation)\$($param1.Version)\Reports" } #else the simple version
	if ($problems.Count -eq 0)
	{
		if (Test-Path -PathType Container $MyDatabasePath) #if it does already exist
		{
			Sqlcompare @CLIArgs >"$MyVersionReportPath\VersionComparison.txt" #simply check that the two are identical
			if ($LASTEXITCODE -eq 0) { $identical = $true; "Database Identical to source" }
			elseif ($LASTEXITCODE -eq 79) { $identical = $False; "Database Different to source" }
			else
			{
				#report a problem and send back the args for diagnosis (hint, only for script development)
				$Arguments = ''; $identical = $null;
				$Arguments += $CLIArgs | foreach{ $_ }
				$problems += "That Went Badly (code $LASTEXITCODE) with paramaters $Arguments."
			}
		}
		else
		{
			$identical = $null;
			$Warnings = "source folder '$MyDatabasePath' did not exist so can't check"
		}
		
	}
	$param1.Checked = $identical
	if ($problems.Count -gt 0)
	{ $Param1.Problems.'IsDatabaseIdenticalToSource' += $problems; }
	else
	{
		$Param1.WriteLocations.'IsDatabaseIdenticalToSource' = "$MyVersionReportPath\VersionComparison.txt";
	}
	if ($warnings.Count -gt 0)
	{ $Param1.Warnings.'IsDatabaseIdenticalToSource' += $Warnings; }
}

<#this routine checks to see if a script folder already exists for this version
of the database and, if not, it will create one and fill it with subdirectories
for each type of object. A tables folder will, for example, have a file for every table
each containing a  build script to create that object.
When this exists, it allows SQL Compare to do comparisons and check that a version has not
drifted.
 #>
$CreateScriptFoldersIfNecessary = {
	Param ($param1) # $CreateScriptFoldersIfNecessary 
	$Problems = @(); #We check that it contains the keys for the values that we need 
	$freedback = @();
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
	@('version', 'server', 'rdbms', 'database', 'project') |
	foreach{ if ($param1.$_ -eq $null) { $problems += "no key for the '$($_)'" } }
	
	#the database scripts path would be up to you to define, of course
	$EscapedProject = ($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$SourcePath = if ([string]::IsNullOrEmpty($param1.SourcePath)) { 'Source' }
	else { "$($param1.SourcePath)" }
	$MyDatabasePath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$sourcePath" }
	else { "$($param1.reportLocation)\$($param1.Version)\$sourcePath" } #else the simple version
	$MyCurrentPath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\current\$sourcePath" }
	else { "$($param1.reportLocation)\current\$sourcePath" } #else the simple version
	if (!(Test-Path -PathType Container $MyDatabasePath))
	{
		switch -Regex ($param1.RDBMS)
		{
			'sqlserver' #using SQL Server
			{
				$command=$null;
                $command = get-command SQLCompare -ErrorAction Ignore 
                if ($command -eq $null) 
				{
					if ($SQLCompareAlias -ne $null)
					{ Set-Alias SQLCompare $SQLCompareAlias }
					else
					{ $problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder' }
				}
				$CLIArgs = @(
					"/server1:$($param1.server)",
					"/database1:$($param1.database)",
					"/Makescripts:$($MyDatabasePath)", #special command to make a scripts directory
					"/force",
					"/LogLevel:Warning"
				)
				
				if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
				{
					$CLIArgs += @(
						"/username1:$($param1.uid)",
						"/Password1:$($param1.pwd)"
					)
				}
                $schemaAndName=$param1.flywayTable -split '\.'
				if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
				{
					$CLIArgs += @(
						"/filter:$($param1.filterpath)"
					)
				}
				else
				{
					$CLIArgs += @(
						"/exclude:table:$(( $schemaAndName[1],$schemaAndName[0] -ne $null)[0])",
						'/exclude:ExtendedProperty') #trivial}
				}
				
				if ($problems.Count -eq 0) #if it doesn't already erxist
				{
					Sqlcompare @CLIArgs #write an object-level  script folder that represents the vesion of the database
					if (!($?))
					{
						#report a problem and send back the args for diagnosis (hint, only for script development)
						$problems += "SQL Compare responded with error code $LASTEXITCODE "
					}
				}
			}
			'mariadb|mysql' #--do it with MySQL or MariaDB
			{
				$command=$null;
                $command = get-command mysqldump -ErrorAction Ignore 
                if ($command -eq $null)
				{
					if ($mysqldumpAlias -ne $null)
					{ Set-Alias mysqldump $mysqldumpAlias  }
					else
					{ $problems += 'You must have provided a path to mysqldump in the $mysqldumpAlias ToolLocations.ps1 file in the resources folder' }
				}

				$ObjectList = Execute-SQL $param1 @'
        SELECT t.Table_Schema AS "Schema", t.TABLE_NAME AS "Name", 
          case when r.Routine_NAME IS NOT NULL 
            then LOWER(r.routine_type) 
            ELSE lower(replace(t.Table_type,'BASE','')) END AS "Type" 
        FROM information_schema.tables t
        LEFT OUTER JOIN information_schema.routines r
        oN r.routine_Schema=t.table_schema
        AND r.routine_Name=t.table_Name
        WHERE table_schema='dbo' AND TABLE_NAME <>'flyway_schema_history'
'@
				$object = $ObjectList | Convertfrom-json
				$object | foreach{
					$WhereToStoreIt = "$MyDatabasePath\$($_.Type.Trim())"
					if ($problems.Count -eq 0)
					{
						if (-not (Test-Path "$WhereToStoreIt" -PathType Container))
						{ $Null = New-Item -ItemType directory -Path "$WhereToStoreIt" -Force }
						mysqldump "--host=$($param1.server)" "--port=$($param1.Port -replace '[^\d]','')" "--password=$($param1.pwd)" "--user=$($param1.uid)" --triggers --skip-set-charset --skip-add-drop-table --skip-set-charset --compact --no-data  "$($_.Schema)" "$($_.Name)" > "$WhereToStoreIt\$($_.Schema).$($_.Name).sql"
						if (!($?))
						{
							#report a problem and send back the args for diagnosis
							$problems += "mysqldump responded with error code $LASTEXITCODE "
						}
					}
				}
			}
			'postgresql'
			{
				$command=$null;
                #Remove-Item Alias:pg_dump
                $command = get-command pg_dump -ErrorAction Ignore 
				if ($command -eq $null)
				{
					if ($PGDumpAlias -ne $null)
					{ Set-Alias pg_dump   $PGDumpAlias; }
					else
					{
						$problems += 'You must have provided a path to pg_dump.exe in the ToolLocations.ps1 file in the resources folder'
					}
				}
				$env:PGPASSWORD = "$($param1.pwd)"
				if (-not (Test-Path "$MyDatabasePath" -PathType Container))
				{ $null = New-Item -ItemType directory -Path "$MyDatabasePath" -Force }
				$Params = @(
					"--dbname=$($param1.database)",
					"--host=$($param1.server)",
					"--username=$($param1.user)",
					"--file=$MyDatabasePath\FullBuild.sql",
					"--port=$($param1.Port -replace '[^\d]', '')",
					'--encoding=UTF8',
					'--schema-only'
				)
				pg_dump @Params
				if (!($?))
				{
					#report a problem and send back the args for diagnosis (hint, only for script development)
					$problems += "pg_dump responded with error code $LASTEXITCODE "
				}
				#Read in the build script 
				$FullBuild = [IO.File]::ReadAllText("$MyDatabasePath\FullBuild.sql")
				#Each statement has a header with useful information. This saves a lot of work, so we parse it
				$Regex = '-- Name: (?<name>.+); Type: (?<type>.+); Schema: (?<schema>.+); Owner: (?<owner>.+)\n(?<contents>(?s:.)+?)(?=(-- Name|-- PostgreSQL))'
				$TheObjects = @() #We hold the list of base objects (Tables, views, procedures)
				$TheChildren = @() #We hold the list of child objects (constraints, indexes etc)
				Select-String $regex -input $FullBuild -AllMatches | Foreach { $_.Matches } | foreach{
					$match = $_
					$name = $match.Groups[2].Value;
					$type = $match.Groups[3].Value;
					$schema = $match.Groups[4].Value;
					$owner = $match.Groups[5].Value;
					$contents = $match.Groups[6].Value;
					if ($contents -match 'ALTER TABLE ONLY (?<tablename>.+)') #then they are child objects
					{
						$TheMatch = $matches;
						$TheChildren += @{ 'Name' = $TheMatch.tablename; 'contents' = $contents }
					}
					elseif ($type -eq 'INDEX')
					{
						if ($contents -imatch 'CREATE INDEX .+ ON (["a-z \._]+) USING')
						{
							$TheChildren += @{ 'Name' = $Matches.1; 'contents' = $contents }
						}
					}
					else #we assume that they are base objects
					{
						$TheObjects += @{ 'Name' = $Name; 'schema' = $schema; 'Type' = $type; 'contents' = $contents }
					}
				}
				$TheObjects | foreach{
					$Object_name = "$($_.schema).$($_.name)";
					$Escaped_Object_name = "$($_.schema).`"$($_.name)`"";
					$_.contents += $TheChildren |
					where { $_.name -in @($Object_name, $Escaped_Object_name) } |
					foreach { $code = '' }{ $code += "$($_.contents)" }{ $code }
				}
				$TheObjects | foreach{
					$SchemaToStoreIt = "$MyDatabasePath\$($_.Type.ToLower())" #store it according to type
					if (-not (Test-Path "$SchemaToStoreIt" -PathType Container)) #make sure exzists
					{ New-Item -ItemType directory -Path "$SchemaToStoreIt" -Force }
					$_.Contents > "$SchemaToStoreIt\$($_.schema.ToLower()).$($_.name).sql"; #pop it into the file
				}
			}
			'sqlite'
			{
				$command=$null;
                $command = get-command sqlite -ErrorAction Ignore 
				if ($command -eq $null)
				{
					if ($sqliteAlias -ne $null)
					{ Set-Alias sqlite $sqliteAlias  }
					else
					{ $problems += 'You must have provided a path to sqlite.exe in the ToolLocations.ps1 file in the resources folder' }
				}
				
				$params = @(
					'.bail on',
					'.mode json',
					'.headers off',
					"SELECT name,type FROM sqlite_schema 
                    WHERE type IN ('table','view') and name NOT LIKE 'sqlite_%' 
                    -- and name not like '$($param1.flywayTable)%' 
                    ORDER BY 1",
					'.quit')
				try {
                $TableList = sqlite "$($param1.database)" @Params
				}
		        catch
		        { $problems += "SQL called to SQLite to get list of objects failed because $($_)" }
                if (!($?)) {$problems +='The SQL Call to  get list of objects  from SQLite failed'}
				$Tables = $TableList | convertFrom-json
				$Scripts = $Tables | foreach {
					$params = @(
						'.bail on',
						".schema $($_.name)",
						'.quit');
					@{ 'name' = $_.name; 'type' = $_.type; 'script' = sqlite "$($param1.database)" @Params; };
				}
				$scripts | foreach{
					$SchemaToStoreIt = "$MyDatabasePath\$($_.Type.ToLower())" #store it according to type
					if (-not (Test-Path "$SchemaToStoreIt" -PathType Container)) #make sure exzists
					{ $null = New-Item -ItemType directory -Path "$SchemaToStoreIt" -Force }
					$_.script > "$SchemaToStoreIt\$($_.name).sql"; #pop it into the file
				}
			}
            'oracle'
            {
            
            $TheListOfSchemas=($param1.schemas.split(',')|foreach  {"'$_'"}) -join ','
            $TheJsonMetadata = Execute-SQL $param1  "
select Object_type, owner||'.'||object_name as TheName, dbms_metadata.get_ddl(object_type, object_name, owner) as Thesource
from
(
    select
        owner,
        --Java object names may need to be converted with DBMS_JAVA.LONGNAME.
        --That code is not included since many database don't have Java installed.
        object_name,
        decode(object_type,
            'DATABASE LINK',      'DB_LINK',
            'JOB',                'PROCOBJ',
            'RULE SET',           'PROCOBJ',
            'RULE',               'PROCOBJ',
            'EVALUATION CONTEXT', 'PROCOBJ',
            'CREDENTIAL',         'PROCOBJ',
            'CHAIN',              'PROCOBJ',
            'PROGRAM',            'PROCOBJ',
            'PACKAGE',            'PACKAGE_SPEC',
            'PACKAGE BODY',       'PACKAGE_BODY',
            'TYPE',               'TYPE_SPEC',
            'TYPE BODY',          'TYPE_BODY',
            'MATERIALIZED VIEW',  'MATERIALIZED_VIEW',
            'QUEUE',              'AQ_QUEUE',
            'JAVA CLASS',         'JAVA_CLASS',
            'JAVA TYPE',          'JAVA_TYPE',
            'JAVA SOURCE',        'JAVA_SOURCE',
            'JAVA RESOURCE',      'JAVA_RESOURCE',
            'XML SCHEMA',         'XMLSCHEMA',
            object_type
        ) object_type
    from dba_objects 
    where owner in ($TheListOfSchemas)
        --These objects are included with other object types.
        and object_type not in ('INDEX PARTITION','INDEX SUBPARTITION','SEQUENCE',
            'LOB','LOB PARTITION','TABLE PARTITION','TABLE SUBPARTITION')
        --Ignore system-generated types that support collection processing.
        and not (object_type = 'TYPE' and object_name like 'SYS_PLSQL_%')
        --Exclude nested tables, their DDL is part of their parent table.
        and (owner, object_name) not in (select owner, table_name from dba_nested_tables)
        --Exclude overflow segments, their DDL is part of their parent table.
        and (owner, object_name) not in (select owner, table_name from dba_tables where iot_type = 'IOT_OVERFLOW')
);
            "
#end
            $scripts = $TheJsonMetadata | convertFrom-json
            $scripts[1] | foreach{
	            $object = $_;
	            $SchemaToStoreIt = "$MyDatabasePath\$($object.object_type.ToLower())" #store it according to type
	            if (-not (Test-Path "$SchemaToStoreIt" -PathType Container)) #make sure exzists
	            { $null = New-Item -ItemType directory -Path "$SchemaToStoreIt" -Force }
	            [System.IO.File]::WriteAllLines("$SchemaToStoreIt\$($object.thename).sql", $object.thesource);
                }
            }

			default
			{ $problems += "Sorry but a script folder isn''t supported from your RDBMS $($param1.RDBMS)" }
		}
		if ($feedback.count -gt 0)
		{ $Param1.feedback.'CreateScriptFoldersIfNecessary' = $feedback }
		if ($problems.count -gt 0)
		{ $Param1.problems.'CreateScriptFoldersIfNecessary' = $problems }
		else
		{
			$Param1.WriteLocations.'CreateScriptFoldersIfNecessary' = "$MyDatabasePath";
            if (-not (Test-Path "$MyCurrentPath" -PathType Container))
                { $null=New-Item -ItemType directory -Path "$MyCurrentPath" -Force}
            Remove-Item "$MyCurrentPath\*" -Recurse 
			copy-item -path  "$MyDatabasePath\*" -recurse -destination "$MyCurrentPath" -Container # copy over the current model
			@{ 'version' = $Param1.version; 'Author' = $Param1.InstalledBy; 'Branch' = $param1.branch } |
			convertTo-json >"$(split-path -path $MyCurrentPath -parent)\Version.json"
		}
	}
	else { $Param1.feedback.'CreateScriptFoldersIfNecessary' = "This version ($($param1.Version)) is already scripted in $MyDatabasePath " }
}

<# 
a script block that produces a build script from a database, using SQL Compare, pg_dump or whatever.

#>

$CreateBuildScriptIfNecessary = {
	Param ($param1) # $CreateBuildScriptIfNecessary (Don't delete this) 
	$problems = @();
    $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
    Trap
        {
	        # Handle the error
	        $err = $_.Exception
	        $problems += $err.Message
	        while ($err.InnerException)
	        {
		        $err = $err.InnerException
		        $problems += $err.Message
	        };
            $Param1.Problems.'CreateBuildScriptIfNecessary' += $problems;
	        # End the script.
	        break
        }
    @('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null, '')) { $Problems += "no value for '$($_)'" } }

	#the database scripts path would be up to you to define, of course
	$EscapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$scriptsPath = if ([string]::IsNullOrEmpty($param1.scriptsPath)) { 'scripts' }
	else { "$($param1.scriptsPath)" }
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	    {$MyDatabasePath = "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$scriptsPath";
         $MyCurrentPath = "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\current\$scriptsPath";}
	else {$MyDatabasePath = "$($param1.reportLocation)\$($param1.Version)\$scriptsPath";
          $MyCurrentPath = "$($param1.reportLocation)\current\$scriptsPath"; } #else the simple version
	
	if (-not (Test-Path -PathType Leaf "$MyDatabasePath\V$($param1.Version)__Build.sql"))
	{
		if (-not (Test-Path -PathType Container $MyDatabasePath))
		{
			# is the path to the scripts directory
			# not there, so we create the directory 
			$null = New-Item -ItemType Directory -pa $MyDatabasePath -Force;
		}
		if (-not (Test-Path -PathType Container $MyCurrentPath))
		{
			# is the path to the current directory
			# not there, so we create the directory 
			$null = New-Item -Path $MyCurrentPath -ItemType "directory" -Force;
		}
		switch -Regex ($param1.RDBMS)
		{
			'sqlserver' #using SQL Server
			{
            	#the alias must be set to the path of your installed version of SQL Compare
			    $command = get-command SQLCompare -ErrorAction Ignore 
                if ($command -eq $null) 
                    {
    	            if ($SQLCompareAlias-ne $null)
                        {Set-Alias SQLCompare $SQLCompareAlias}
                    else
                        {$problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder'}
                }
				$CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
					# can use a hash-table 
					"/server1:$($param1.server)",
					"/database1:$($param1.database)",
					"/empty2",
					"/force", # 
					"/options:NoTransactions,NoErrorHandling", # so that we can use the script with Flyway more easily
					"/LogLevel:Warning",
					"/ScriptFile:$MyDatabasePath\V$($param1.Version)__Build.sql"
				)
				
				if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
				{
					$CLIArgs += @(
						"/username1:$($param1.uid)",
						"/Password1:$($param1.pwd)"
					)
				}
                $schemaAndName=$param1.flywayTable -split '\.'
                if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
		        {
			        $CLIArgs += @(
				        "/filter:$($param1.filterpath)"
			        )
                 }
                else
                    {
                    $CLIArgs += @(
                     "/exclude:table:$(( $schemaAndName[1],$schemaAndName[0] -ne $null)[0])")
		        } 
				
				# if it is done already, then why bother? (delete it if you need a re-run for some reason 	
				Sqlcompare @CLIArgs #run SQL Compare with splatted arguments
				if ($?) { $Param1.feedback.'CreateBuildScriptIfNecessary'="Written build script for $($param1.Project) $($param1.Version) to $MyDatabasePath"
                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql"
                }
				else # if no errors then simple message, otherwise....
				{
					#report a problem and send back the args for diagnosis (hint, only for script development)
					$Arguments = '';
					$Arguments += $CLIArgs | foreach{ $_ }
					$Problems += "SQLCompare Went badly. (code $LASTEXITCODE) with paramaters $Arguments."
				}
				break;
			}
			'postgresql' #using SPostgreSQL
			{
            	#the alias must be set to the path of your installed version of Spg_dump
             $command=$null;
             $command = get-command pg_dump -ErrorAction Ignore;
	         if ($command -eq $null)               
                {if ($PGDumpAlias -ne $null)
                    {Set-Alias pg_dump   $PGDumpAlias;}
                else
                    {$problems += 'You must have provided a path to pg_dump.exe in the ToolLocations.ps1 file in the resources folder'
                    }
                }
                $env:PGPASSWORD="$($param1.pwd)"
                $Params=@(
                    "--dbname=$($param1.database)",
                    "--host=$($param1.server)",
                    "--username=$($param1.user)",
                    "--port=$($param1.Port -replace '[^\d]','')",
                    "--file=$MyDatabasePath\V$($param1.Version)__Build.sql",
                    '--encoding=UTF8',
                    '--schema-only')
                 pg_dump  @Params 
				if ($?) 
                { 
                $Param1.feedback.'CreateBuildScriptIfNecessary'="Written PG build script for $($param1.Project) $($param1.Version) to $MyDatabasePath" 
                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql" -Force
                }
				else # if no errors then simple message, otherwise....
				{
					$Problems += "pg_dump Went badly. (code $LASTEXITCODE)"
				}
				break;
			}
            'mysql|mariadb'
            {
                $command=$null;
                $command = get-command mysqldump -ErrorAction Ignore 
                
	            if ($command -eq $null)
	                {if ($MySQLDumpAlias -ne $null)
                        {Set-Alias mysqldump $MySQLDumpAlias }
                    else
                        {$problems += 'You must have provided a path to mysqldump.exe in $MySQLDumpAlias the ToolLocations.ps1 file in the resources folder'}
                    }            else
                {
                powershell.exe "mysqldump --host=$($param1.server) --port=$($param1.Port -replace '[^\d]','')  --password=$($param1.pwd) --user=$($param1.uid) --no-data --databases $($param1.schemas -replace ',',' ')" > "$MyDatabasePath\V$($param1.Version)__Build.sql"
                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql" -Force
               } 
            }
            'sqlite' #using SQLite
            {
                $command=$null;
                $command = get-command sqlite -ErrorAction Ignore 
	            if ($command -eq $null)
	                {if ($sqliteAlias -ne $null)
                        {Set-Alias sqlite $sqliteAlias }
                    else
                        {$problems += 'You must have provided a path to sqlite.exe in the ToolLocations.ps1 file in the resources folder'}
                    }
         		$params = @(
			        '.bail on',
                    '.schema',
			        '.quit')
		        try
		        {
			        sqlite "$($param1.database)" @Params > "$MyDatabasePath\V$($param1.Version)__Build.sql"
		        }		
                catch
		        { $problems += "SQL called to SQLite  failed because $($_)"
                }           
                if ($?)
                { $Param1.feedback.'CreateBuildScriptIfNecessary'="Written SQLite build script for $($param1.Project)" 

                Copy-Item -Path "$MyDatabasePath\V$($param1.Version)__Build.sql" -Destination "$MyCurrentPath\current__Build.sql" -Force
                }
                else
                {$problems += "SQLite couldn't create the build script for $($param1.Version) $($param1.RDBMS)"}
            }
            default
                {
                $problems += "cannot do a build script for $($param1.Version) $($param1.RDBMS) "
                }
		}
		if ($problems.count -gt 0)
		{ $Param1.Problems.'CreateBuildScriptIfNecessary' += $problems; }
		else
		{
			$Param1.WriteLocations.'CreateBuildScriptIfNecessary' = "$MyDatabasePath\V$($param1.Version)__Build.sql";
		}
	}
	
	else {$Param1.feedback.'CreateBuildScriptIfNecessary'="This version '$($param1.Version)' already has a build script at $MyDatabasePath " }
	
}


<#This scriptblock executes SQL that produces a report in XML or JSON from the database
#>

$ExecuteTableSmellReport = {
	Param ($param1) # $ExecuteTableSmellReport - parameter is a hashtable 
	$problems = @()
	@('server', 'database', 'version', 'project') | foreach{
		if ($param1.$_ -eq $null)
		{ write-error "no value for '$($_)'" }
	}
	<#if ($param1.EscapedProject -eq $null) #check that escapedValues are in place
	{
		$EscapedValues = $param1.GetEnumerator() |
		where { $_.Name -in ('server', 'Database', 'Project') } | foreach{
			@{ "Escaped$($_.Name)" = ($_.Value.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') }
		}
		$EscapedValues | foreach{ $param1 += $_ }
	}#>
    $EscapedProject=($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
	$MyDatabasePath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports"} 
        else {"$($param1.reportLocation)\$($param1.Version)\Reports"} #else the simple version
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $MyDatabasePath;
	}
	$MyOutputReport = "$MyDatabasePath\TableIssues.JSON"
	#the alias must be set to the path of your installed version of SQL Compare
	$command=$null;
    $command = get-command SQLCmd -ErrorAction Ignore 
	if ($command -eq $null)
	{  if ($SQLCmdAlias -ne $null)
        {Set-Alias SQLCmd   $SQLCmdAlias  }
    else
        {$problems += 'You must have provided a path to SQLcmd.exe in the ToolLocations.ps1 file in the resources folder'}
        }
	#is that alias correct?
	$query = @'
SET NOCOUNT ON
/**
summary:   >
 This query finds the following table smells
 1/ is a Wide table (set this to what you consider to be wide)
 2/ is a Heap
 3/ is an undocumented table
 4/ Has no Primary Key
 5/ Has ANSI NULLs set to OFF
 6/ Has no index at all
 7/ No candidate key (unique constraint on column(s))
 8/ Has disabled Index(es)
 9/ has leftover fake index(es)
10/ has a column collation different from the database
11/ Has a surprisingly low Fill-Factor
12/ Has disabled constraint(s)'
13/ Has untrusted constraint(s)'
14/ Has a disabled Foreign Key'
15/ Has untrusted FK'
16/ Has unrelated to any other table'
17/ Has a deprecated LOB datatype
18/ Has unintelligible column names'
19/ Has a foreign key that has no index'
20/ Has a GUID in a clustered Index
21/ Has non-compliant column names'
22/ Has a trigger that has'nt got NOCOUNT ON'
23/ Is not referenced by any procedure, view or function'
24/ Has  a disabled trigger' 
25/ Can't be indexed'
Revisions:
 - Author: Phil Factor
   Version: 1.1
   Modifications:
	-  added tests as suggested by comments to blog
   Date: 30 Mar 2016
 - Author: Phil Factor
   Version: 1.2
   Modifications:
	-  tidying, added five more smells
   Date: 10 July 2020
 - Author: Phil Factor
   Version: 1.3
   Modifications:
	-  re-engineered it for JSON output
   Date: 26 March 2021
 
 returns:   >
 single result of table name, and list of problems        
**/
declare  @TableSmells table (object_id INT, problem VARCHAR(200)) 
INSERT INTO @TableSmells (object_id, problem)
 SELECT object_id, 'wide (more than 15 columns)' AS Problem
          FROM sys.tables /* see whether the table has more than 15 columns */
          WHERE max_column_id_used > 15
        UNION ALL
        SELECT DISTINCT sys.tables.object_id, 'heap'
          FROM sys.indexes /* see whether the table is a heap */
            INNER JOIN sys.tables
              ON sys.tables.object_id = sys.indexes.object_id
          WHERE sys.indexes.type = 0
        UNION ALL
        SELECT s.object_id, 'Undocumented table'
          FROM sys.tables AS s /* it has no extended properties */
            LEFT OUTER JOIN sys.extended_properties AS ep
              ON s.object_id = ep.major_id AND minor_id = 0
          WHERE ep.value IS NULL
        UNION ALL
        SELECT sys.tables.object_id, 'No primary key'
          FROM sys.tables /* see whether the table has a primary key */
          WHERE ObjectProperty(object_id, 'TableHasPrimaryKey') = 0
        UNION ALL
        SELECT sys.tables.object_id, 'has ANSI NULLs set to OFF'
          FROM sys.tables /* see whether the table has ansii NULLs off*/
          WHERE ObjectPropertyEx(object_id, 'IsAnsiNullsOn') = 0
       UNION ALL
        SELECT sys.tables.object_id, 'No index at all'
          FROM sys.tables /* see whether the table has any index */
          WHERE ObjectProperty(object_id, 'TableHasIndex') = 0
        UNION ALL
        SELECT sys.tables.object_id, 'No candidate key'
          FROM sys.tables /* if no unique constraint then it isn't relational */
          WHERE ObjectProperty(object_id, 'TableHasUniqueCnst') = 0
            AND ObjectProperty(object_id, 'TableHasPrimaryKey') = 0
        UNION ALL
        SELECT DISTINCT object_id, 'disabled Index(es)'
          FROM sys.indexes /* don't leave these lying around */
          WHERE is_disabled = 1
        UNION ALL
        SELECT DISTINCT object_id, 'leftover fake index(es)'
          FROM sys.indexes /* don't leave these lying around */
          WHERE is_hypothetical = 1
        UNION ALL
        SELECT c.object_id,
          'has a column ''' + c.name + ''' that has a collation '''
          + collation_name + ''' different from the database'
          FROM sys.columns AS c
          WHERE Coalesce(collation_name, '') 
		  <> DatabasePropertyEx(Db_Id(), 'Collation')
        UNION ALL
        SELECT DISTINCT object_id, 'surprisingly low Fill-Factor'
          FROM sys.indexes /* a fill factor of less than 80 raises eyebrows */
          WHERE fill_factor <> 0
            AND fill_factor < 80
            AND is_disabled = 0
            AND is_hypothetical = 0
        UNION ALL
        SELECT DISTINCT parent_object_id, 'disabled constraint(s)'
          FROM sys.check_constraints /* hmm. i wonder why */
          WHERE is_disabled = 1
        UNION ALL
        SELECT DISTINCT parent_object_id, 'untrusted constraint(s)'
          FROM sys.check_constraints /* ETL gone bad? */
          WHERE is_not_trusted = 1
        UNION ALL
        SELECT DISTINCT parent_object_id, 'disabled FK'
          FROM sys.foreign_keys /* build script gone bad? */
          WHERE is_disabled = 1
        UNION ALL
        SELECT DISTINCT parent_object_id, 'untrusted FK'
          FROM sys.foreign_keys /* Why do you have untrusted FKs?       
      Constraint was enabled without checking existing rows;
      therefore, the constraint may not hold for all rows. */
          WHERE is_not_trusted = 1
        UNION ALL
        SELECT object_id, 'unrelated to any other table'
          FROM sys.tables /* found a simpler way! */
          WHERE ObjectPropertyEx(object_id, 'TableHasForeignKey') = 0
            AND ObjectPropertyEx(object_id, 'TableHasForeignRef') = 0
        UNION ALL
        SELECT object_id, 'deprecated LOB datatype'
          FROM sys.tables /* found a simpler way! */
          WHERE ObjectPropertyEx(object_id, 'TableHasTextImage') = 1 
       UNION ALL
        SELECT DISTINCT object_id, 'unintelligible column names'
          FROM sys.columns /* column names with no letters in them */
          WHERE name COLLATE Latin1_General_CI_AI NOT LIKE '%[A-Z]%' COLLATE Latin1_General_CI_AI
        UNION ALL
        SELECT keys.parent_object_id,
          'foreign key ' + keys.name + ' that has no supporting index'
          FROM sys.foreign_keys AS keys
            INNER JOIN sys.foreign_key_columns AS TheColumns
              ON keys.object_id = constraint_object_id
            LEFT OUTER JOIN sys.index_columns AS ic
              ON ic.object_id = TheColumns.parent_object_id
             AND ic.column_id = TheColumns.parent_column_id
             AND TheColumns.constraint_column_id = ic.key_ordinal
          WHERE ic.object_id IS NULL
        UNION ALL
        SELECT Ic.object_id, Col_Name(Ic.object_id, Ic.column_id)
          + ' is a GUID in a clustered index' /* GUID in a clustered IX */
          FROM sys.index_columns AS Ic
			INNER JOIN sys.tables AS tables
			ON tables.object_id = Ic.object_id
            INNER JOIN sys.columns AS c
              ON c.object_id = Ic.object_id AND c.column_id = Ic.column_id
            INNER JOIN sys.types AS t
              ON t.system_type_id = c.system_type_id
            INNER JOIN sys.indexes AS i
              ON i.object_id = Ic.object_id AND i.index_id = Ic.index_id
          WHERE t.name = 'uniqueidentifier'
            AND i.type_desc = 'CLUSTERED'
        UNION ALL
        SELECT DISTINCT object_id, 'non-compliant column names'
          FROM sys.columns /* column names that need delimiters*/
          WHERE name COLLATE Latin1_General_CI_AI LIKE '%[^_@$#A-Z0-9]%' COLLATE Latin1_General_CI_AI
        UNION ALL /* Triggers lacking `SET NOCOUNT ON`, which can cause unexpected results WHEN USING OUTPUT */
        SELECT ta.object_id,
          'This table''s trigger, ' + Object_Name(tr.object_id)
          + ', has''nt got NOCOUNT ON'
          FROM sys.tables AS ta /* see whether the table has any index */
            INNER JOIN sys.triggers AS tr
              ON tr.parent_id = ta.object_id
            INNER JOIN sys.sql_modules AS mo
              ON tr.object_id = mo.object_id
          WHERE definition NOT LIKE '%set nocount on%'
        UNION ALL /* table not referenced by any routine */
        SELECT sys.tables.object_id,
          'not referenced by procedure, view or function'
          FROM sys.tables /* found a simpler way! */
            LEFT OUTER JOIN sys.sql_expression_dependencies
              ON referenced_id = sys.tables.object_id
          WHERE referenced_id IS NULL
        UNION ALL
        SELECT DISTINCT parent_id, 'has a disabled trigger'
          FROM sys.triggers
          WHERE is_disabled = 1 AND parent_id > 0
        UNION ALL
        SELECT sys.tables.object_id, 'can''t be indexed'
          FROM sys.tables /* see whether the table has a primary key */
          WHERE ObjectProperty(object_id, 'IsIndexable') = 0
DECLARE @json NVARCHAR(MAX)
SELECT @json = (SELECT TableName, problem from
	(SELECT DISTINCT  Object_Schema_Name(Object_ID) + '.' + Object_Name(Object_ID) AS TableName,object_id, Count(*) AS smells
FROM @TableSmells GROUP BY Object_ID)f(TableName,Object_id, Smells)
INNER JOIN @TableSmells AS problems ON f.object_id=problems.object_id
ORDER BY smells desc
FOR JSON AUTO)
SELECT @Json
'@
	
	if (!([string]::IsNullOrEmpty($param1.uid)) -and ([string]::IsNullOrEmpty($param1.pwd)))
	{ $problems += 'No password is specified' }
	If (!(Test-Path -PathType Leaf  $MyOutputReport) -and ($problems.Count -eq 0))
	{
		if (!([string]::IsNullOrEmpty($param1.uid)))
		{
			$MyJSON = sqlcmd -S "$($param1.server)" -d "$($param1.database)" `
							 -Q `"$query`" -U $($param1.uid) -P $($param1.pwd) -o $MyOutputReport -u -y0
			$arguments = "$($param1.server) -d $($param1.database) -U $($param1.uid) -P $($param1.pwd) -o $MyOutputReport"
		}
		else
		{
			$MyJSON = sqlcmd -S "$($param1.server)" -d "$($param1.database)" -Q `"$query`" -E -o $MyOutputReport -u -y0
			$arguments = "$($param1.server) -d $($param1.database) -o $MyOutputReport"
		}
		if (!($?))
		{
			#report a problem and send back the args for diagnosis (hint, only for script development)
			$Problems += "sqlcmd failed with code $LASTEXITCODE, $Myversions, with parameters $arguments"
		}
		$possibleError = Get-Content -Path $MyOutputReport -raw
		if ($PossibleError -like '*Sqlcmd: Error*')
		{
			$Problems += $possibleError;
			Remove-Item $MyOutputReport;
		}
		
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'ExecuteTableSmellReport' += $problems;
	}
	else
	{
		$Param1.WriteLocations.'ExecuteTableSmellReport' = $MyOutputReport;
	}
}

<# This places in a report a json report of the documentation of every table and its
columns. If you add or change tables, this can be subsequently used to update the 
AfterMigrate callback script
for the documentation */#>

$ExecuteTableDocumentationReport = {
	Param ($param1) # $ExecuteTableDocumentationReport  - parameter is a hashtable
	
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
	$problems = @()
	@('server', 'database', 'version', 'project', 'rdbms') | foreach{
		$value = "$($param1.$_.Trim())"
		if ([string]::IsNullOrEmpty($value))
		{ $Problems = "no value for '$($_)'" }
	}
	$escapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$MyDatabasePath =
	if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports" }
	else { "$($param1.reportLocation)\$($param1.Version)\Reports" } #else the simple version
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $MyDatabasePath;
	}
	$MyOutputReport = "$MyDatabasePath\TableDocumentation.JSON"
	#handy stuff for where clauses
	$ListOfSchemas = ($param1.schemas -split ',' | foreach{ "'$_'" }) -join ',';
	$FlywayTableName = ($param1.flywayTable -split '\.')[1]
	
	switch -Regex ($param1.RDBMS)
	{
		'mysql|mariadb'   {
			$ColumnComments = @"
SELECT 
  CONCAT( c.table_schema, '.',c.TABLE_NAME) AS TableObjectName, 
  case when v.Table_Name IS NULL then 'user table' ELSE 'view' END AS "Type", 
  COLUMN_NAME AS "The_Column_Name", COLUMN_COMMENT as "Description" 
FROM information_schema.columns  c
LEFT OUTER JOIN information_schema.views v 
ON c.TABLE_NAME=v.Table_Name
AND v.table_Schema=c.table_Schema
WHERE c.table_schema in ($ListOfSchemas)
	AND c.TABLE_NAME <> '$FlywayTableName'
ORDER BY c.TABLE_NAME, ordinal_Position;
"@;
			#Although there is metadata storage for comments in views, it isnt used.
			$ObjectComments =@"
SELECT CONCAT( table_schema, '.',TABLE_NAME) AS TableObjectName,
'user table' as "Type",
TABLE_COMMENT AS "Description" 
FROM information_schema.tables
WHERE table_schema in ($ListOfSchemas)
	AND TABLE_NAME <> '$FlywayTableName'
	AND TABLE_TYPE = 'BASE TABLE'
		
"@
		}
		
		'postgresql' {
			#fetch all the relations (anything that produces columns)
			$ColumnComments =  @"
            SELECT json_agg(e) 
            FROM (
            SELECT isc.table_schema||'.'||isc.table_name AS TableObjectName,
	CASE WHEN views.table_name IS NULL THEN 'table' else 'view' END AS TYPE,
	COLUMN_NAME AS "The_Column_Name",
   -- obj_description(format('%s.%s',isc.table_schema,isc.table_name)::regclass::oid, 'pg_class') as table_description,
    pg_catalog.col_description(format('%s.%s',isc.table_schema,isc.table_name)::regclass::oid,isc.ordinal_position) as Description
FROM
   information_schema.columns isc
 left outer JOIN (SELECT  table_schema,table_name FROM information_schema."views")as views
            ON isc.table_schema=views.table_schema AND  isc.TABLE_NAME=views.table_name
            WHERE isc.table_catalog=current_database() AND isc.table_schema NOT IN ('pg_catalog','information_schema')
            and isc.TABLE_NAME <> '$FlywayTableName'
            ORDER BY isc.table_schema, isc.TABLE_NAME, isc.ordinal_position        
                ) e;
"@
			$ObjectComments =@"
SELECT json_agg(e) 
  FROM (
    SELECT isc.table_schema||'.'||isc.table_name AS TableObjectName,
	replace(lower(table_type),'base ','') AS TYPE,
	obj_description(format('%s.%s',isc.table_schema,isc.table_name)::regclass::oid, 'pg_class') as Description
     FROM
      information_schema.tables isc
      WHERE isc.table_catalog=current_database() AND isc.table_schema NOT IN ('pg_catalog','information_schema')
      and isc.TABLE_NAME <> '$FlywayTableName'
     ORDER BY isc.table_schema, isc.TABLE_NAME  
    ) e;
"@
		}
		'Sqlserver'   {
			# start of SQL Server comments/description
			$ColumnComments = @"
SELECT Object_Schema_Name (objects.object_id) + '.' + objects.name AS "TableObjectName",
       Lower (Replace (objects.type_desc, '_', ' ')) AS "Type",
       columns.name AS "The_Column_Name",
       Convert (VARCHAR(MAX), epcolumn.value) AS "Description"
  FROM
  sys.columns
    INNER JOIN sys.objects
      ON objects.object_id = COLUMNS.object_id
    LEFT OUTER JOIN sys.extended_properties epcolumn --get any description
      ON epcolumn.major_id = objects.object_id
     AND epcolumn.minor_id = COLUMNS.column_id
     AND epcolumn.class = 1
     AND epcolumn.name = 'MS_Description' --you may choose a different name
  WHERE
  columns.object_id = objects.object_id AND is_ms_shipped = 0
  AND Object_Schema_Name (objects.object_id) IN ($ListOfSchemas) 
  AND objects.name <> '$FlywayTableName'
  order by objects.object_id, columns.column_id
  FOR JSON auto, INCLUDE_NULL_VALUES
"@
			$ObjectComments = @"
SELECT Object_Schema_Name (tables.object_id) + '.' + tables.name AS "TableObjectName",
       Lower (Replace (tables.type_desc, '_', ' ')) AS "Type",
       Convert (VARCHAR(MAX), ep.value) AS "Description"
  FROM
  sys.objects tables
    LEFT OUTER JOIN sys.extended_properties ep
      ON ep.major_id = tables.object_id
     AND ep.minor_id = 0
     AND ep.name = 'MS_Description'
  WHERE
  tables.type IN ('IF', 'FT', 'TF', 'U', 'V')
  AND Object_Schema_Name (tables.object_id) IN ($ListOfSchemas) 
  AND Tables.name <> '$FlywayTableName'
  FOR JSON auto, INCLUDE_NULL_VALUES
"@
		}
 Default{
      $problems+="Sorry but $($param1.RDBMS) is not implemented yet"
      }
	}
	if ($problems.Count -eq 0)
	{
		$Columns = Execute-SQL $param1 $ColumnComments | convertFrom-JSON
		$Objects = Execute-SQL $param1 $ObjectComments | convertFrom-JSON
		#Create the JSON documentation file
		$objects | foreach{
			$What = $_;
			$TableObjectName = $_.TableObjectName;
			$Type = $_.Type;
			$TheColumns = [ordered]@{ }
			$Columns | where { $_.TableObjectName -eq $TableObjectName -and $_.Type -eq $Type } | foreach{
				$TheColumns."$($_.The_Column_Name)" = "$($_.Description)";
			}
			[ordered]@{
				"TableObjectName" = $TableObjectName;
				"Type" = $Type;
				"Description" = "$($_.Description)";
				"TheColumns" = $TheColumns
				
			}
			
		} | convertTo-json > $MyOutputReport
		
	}
	
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'ExecuteTableDocumentationReport' += $problems;
	}
	else
	{
		$Param1.WriteLocations.'ExecuteTableDocumentationReport' = $MyOutputReport;
	}
}


<#
$SaveDatabaseModelIfNecessary

This writes a JSON model of the database to a file that can be used subsequently
to check for database version-drift or to create a narrative of changes for the
flyway project between versions.
'server', The name of the database server
'database', The name of the database
'pwd',The password 
'uid',  The UserID
'version', The version directory that is to be ascribed to the file
'project', The name of the whole project for the output filenames
'RDBMS', the rdbms being used, e.g. sqlserver, mysql, mariadb, postgresql, sqlite
'schemas', the schemas to be used to create the model
'flywayTable' the name and schema of the flyway table
'ReportDirectory' for an ad-hoc model
You can, though use parameters to specify the path to the model and reports for ad-hoc models 
/#>
$SaveDatabaseModelIfNecessary = {
	Param ($param1,
		$MyOutputReport = $null,
		$MyCurrentReport = $null,
		$MyModelPath = $null) # $SaveDatabaseModelIfNecessary - dont delete this
 	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8' #we'll be using out redirection
	$problems = @() #none yet!
   Trap
    {
	    # Handle the error
	    $err = $_.Exception
	    $problems += $err.Message
	    while ($err.InnerException)
	    {
		    $err = $err.InnerException
		    $problems += $err.Message
	    };
        $Param1.Problems.'SaveDatabaseModelIfNecessary' += $problems;
	    # End the script.
	    break
    }
    $feedback = @();
	$AlreadyDone = $false;
	#check that you have the  entries that we need in the parameter table.
	$Essentials = @('server', 'database', 'RDBMS', 'flywayTable')
    if ($param1.RDBMS -ne 'sqlite'){$Essentials +='schemas'}
    if ($param1.RDBMS -eq 'Oracle') 
        {$Essentials= @('RDBMS', 'flywayTable')}
	
	$WeHaveToCalculateADestination = $false; #assume default report locations
	if ($MyOutputReport -eq $null -or $MyCurrentReport -eq $null -or $MyModelPath -eq $null)
	{
		#slightly less required for an ad-hoc model.
		$Essentials += @('project', 'Reportdirectory');
		$WeHaveToCalculateADestination = $true;
	}
	
	$Essentials | foreach{
		if ([string]::IsNullOrEmpty($param1.$_))
		{ $Problems += "no value for '$($_)' parameter in db Details" }
	}
	if ($WeHaveToCalculateADestination)
	{# by default, we need to calculate destinations from the param1
		$escapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
		# if any of the optional parameters aren't given
		if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a classic or NULL value
		{ $where = "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)" }
		else { $where = "$($param1.reportLocation)" }
		
		#if all parameters not provided
		$MyDatabasePath = "$where\$($param1.Version)\Reports";
		$MyCurrentPath = "$where\current";
	}
	if ($MyModelPath -eq $null)
	{ $MyModelPath = "$where\$($param1.Version)\model" };
	# so if not specified in the parameters, generate the correct location.
	if ($MyOutputReport -eq $null)
	{ $MyOutputReport = "$MyDatabasePath\DatabaseModel.JSON" }
	if ($MyCurrentReport -eq $null)
	{ $MyCurrentReport = "$MyCurrentPath\Reports\DatabaseModel.JSON" }
	if ($MyDatabasePath -eq $null)
    {$MyDatabasePath=split-path -Path $MyOutputReport -Parent}
    

    #handy stuff for where clauses in SQL Statements
	$ListOfSchemas = ($param1.schemas -split ',' | foreach{ "'$_'" }) -join ',';
    
	if ($param1.flywayTableName -ne $null)
	{ $FlywayTableName = $param1.flywayTableName}
	else
	{ $FlywayTableName = 'flyway_schema_history' }

	if (!(Test-Path -PathType Leaf  $MyOutputReport)) #only do it once
	{
		try
		{
			@($MyDatabasePath, "$(split-path -path $MyCurrentReport -Parent)") | foreach {
				if (Test-Path -PathType Leaf $_)
				{
					# does the path to the reports directory exist as a file for some reason?
					# there, so we delete it 
					remove-Item $_;
				}
				if (-not (Test-Path -PathType Container $_))
				{
					# does the path to the reports directory exist?
					# not there, so we create the directory 
					$null = New-Item -ItemType Directory -Force $_;
				}
			}
			switch -Regex ($param1.RDBMS)
			{
<# this is the section that creates a SQLite Database Model  #>
				'sqlite' {
					$SQLlines = @() #we build the SQL dynamically - SQLite is a bit awkward for metadata!
					$TablesAndViews = Execute-SQL $param1 "SELECT Name, Type FROM sqlite_schema WHERE TYPE <> 'index';" | convertfrom-json
            <# we process tables and views first but not indexes as they are actually child objects of tables  #>
					$TablesAndViews | foreach{
						$type = $_.type;
						$SQLlines += "SELECT '$($_.type)' as type, '$($_.name)' as object, Name||' '||TYPE||CASE `"notnull`" when 1 THEN ' NOT' ELSE '' END
                 ||' NULL '||CASE WHEN dflt_value IS NOT NULL THEN 'DEFAULT ('||`"dflt_value`"||')' ELSE '' end as col, pk
                  FROM pragma_table_info('$($_.name)')"
					}
            <# we want to scoop up as much metadata as we can in one query so we do this with a UNION ALL. #>
					$query = ($SQLlines -join "`r`nUNION ALL`r`n") + ';'
            <# The query was created dynamically, now we execute it  #>
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
					
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
            SELECT m.name as index_name, m.tbl_Name as table_name, i.seqno, i.cid, i.name as column_name
              FROM sqlite_schema AS m,
              pragma_index_info(m.name) AS i
               WHERE TYPE='index' AND sql IS NOT NULL;
"@ | ConvertFrom-Json
					
            <# we get the list of different base types (obvious in SQLite but it can get tricky with other
            RDBMS  #>
                    $TheSchema=(split-path $param1.database -Leaf).replace('.sqlite3','')
					$THeTypes = $TablesAndViews | Select type -Unique #|foreach{$_.type}
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree =@{$TheSchema=@{}; } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
 					$TheTypes | foreach{
						#$SchemaTree.database | add-member -notePropertyName $_.type -notePropertyValue @{ }
                        $SchemaTree.$TheSchema += @{$_.type=@{ }}
					}
                    
                    $TheRelationMetadata | Select type, object -Unique | foreach{
						$type = $_.type;
						$object = $_.object;
                        $pk = @{ }
						$TheColumnList = $TheRelationMetadata |
						where {$_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						   foreach{ $_.col }
                        $SchemaTree.$TheSchema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
                        $primaryKey = $pk | Where { $_.pk -gt 0 } |
						Sort-Object -Property pk |
						Foreach{ [regex]::matches($_.col, '\A\S{1,80}').value }
						if ($primaryKey.count -gt 0)
						{
							$SchemaTree.$TheSchema.$type.$object += @{ 'PrimaryKey' = $primaryKey }
						}
					} 
					
            <# now inject all the objects into the schema tree. First we get all the relations  
					$TheRelationMetadata | Select type, object -Unique | select -first 3| foreach{
						$type = $_.type;
						$object = $_.object;
						$pk = @{ }
						$TheColumnList = $TheRelationMetadata |
						where { $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						foreach{ $_.col }
						$primaryKey = @();
						$SchemaTree.'database'.$type += @{ $object = @{ 'columns' = $TheColumnList } }
						$primaryKey = $pk | Where { $_.pk -gt 0 } |
						Sort-Object -Property pk |
						Foreach{ [regex]::matches($_.col, '\A\S{1,80}').value }
						if ($primaryKey.count -gt 0)
						{
							$SchemaTree.'database'.$type.$object += @{ 'PrimaryKey' = $primaryKey }
						}
					}#>
            <# now stitch in the indexes with their columns  #>
					$indexes | Select table_name, index_name -Unique | foreach{
						$indexedTable = $_.table_name
						$indexName = $_.index_name
						$columns = $indexes |
						where{ $_.table_name -eq $indexedTable -and $_.index_name -eq $indexName } |
						Sort-Object -Property seqno | Select -ExpandProperty column_name
						$SchemaTree.$TheSchema.table.$indexedTable.indexes += @{ $indexName = $columns }
					}
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
					
				} #end of SQLite version
<# this is the section that creates a PostgreSQL Database Model based where
possible on information schema
 #>				
				'postgresql' {
					#fetch all the relations (anything that produces columns)
					$query = @"
            SELECT json_agg(e) 
            FROM (
            Select columns.table_schema as schema, columns.TABLE_NAME as object, COLUMN_NAME||' '||data_type||
               CASE WHEN data_type LIKE 'char%' THEN ' ('||character_maximum_length||')'ELSE''END||
	            CASE WHEN numeric_precision IS NOT NULL THEN 
		            CASE WHEN data_type LIKE '%int%' THEN ' '
		            ELSE ' ('|| numeric_precision_radix ||','|| numeric_scale || ')' END
	            ELSE '' END||
	            CASE is_identity WHEN 'YES' THEN 'GENERATED '||identity_generation||' AS IDENTITY'
	            ELSE	 
		            CASE is_nullable WHEN 'NO' THEN ' NOT' ELSE '' END ||
	            ' NULL ' ||
		            CASE when column_default is NOT NULL THEN 'DEFAULT ('||
		              column_default || ')' ELSE '' END
	            END AS COLUMN,
	            CASE WHEN views.table_name IS NULL THEN 'table' else 'view' END AS type
            FROM information_schema."columns" 
            left outer JOIN (SELECT  table_schema,table_name FROM information_schema."views")as views
            ON columns.table_schema=views.table_schema AND  columns.TABLE_NAME=views.table_name
            WHERE columns.table_catalog=current_database() AND columns.table_schema IN ($listOfSchemas)
            and columns.TABLE_NAME <> '$FlywayTableName'
            ORDER BY columns.table_schema, columns.TABLE_NAME, ordinal_position
                ) e;
"@
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json 
					#now get the details of the routines
					$query = @"
 SELECT json_agg(e) 
            FROM (SELECT 
		Routine_name AS "name", Routine_Schema AS "schema", 
		CONCAT(Routine_Schema,'.', Routine_name) AS "fullname", 
		LOWER(routine_type) AS "type",
		routine_definition AS definition, 
		MD5(routine_definition) AS "hash" --,
		-- description AS "comment"
   FROM information_schema.routines
	-- INNER JOIN pg_proc ON proname LIKE routine_name
	-- LEFT OUTER JOIN pg_description
	-- ON OID= objoid
	WHERE Routine_Schema IN ($ListOfSchemas) 	
UNION ALL
SELECT 
		Table_name AS "name", 
		Table_Schema AS "schema", 
		CONCAT(table_Schema,'.', table_name) AS "fullname", 
		LOWER(replace(table_type,'BASE ','')) AS "type",
		'' AS definition, 
		MD5('') AS HASH -- ,
		-- description AS COMMENT
	FROM information_schema.tables
	--	INNER JOIN pg_class ON relname LIKE table_name
	-- LEFT OUTER JOIN pg_description
	-- ON OID= objoid
	WHERE Table_Schema IN ($listOfSchemas) 
		AND TABLE_NAME NOT LIKE '$FlywayTableName'
                ) e;
"@
					$Routines = Execute-SQL $param1 $query | ConvertFrom-json
					#now do the constraints 
					$query = @"
            SELECT json_agg(e) 
            FROM (SELECT lower(tc.constraint_type) as type, tc.table_schema as schema, 
               tc.Table_name, kcu.constraint_name, kcu.column_name, 
					ordinal_position,
					rel_tco.table_schema || '.' || rel_tco.table_name AS "referenced_table",
					kcu.column_name AS "referenced_column_name"
            FROM information_schema.table_constraints AS tc 
            JOIN information_schema.key_column_usage AS kcu 
              ON tc.constraint_name = kcu.constraint_name 
              AND tc.table_name=kcu.table_name
              AND tc.table_schema=kcu.table_schema
left outer join information_schema.referential_constraints rco
          on tc.constraint_schema = rco.constraint_schema
          and tc.constraint_name = rco.constraint_name
left outer join information_schema.table_constraints rel_tco
          on rco.unique_constraint_schema = rel_tco.constraint_schema
          and rco.unique_constraint_name = rel_tco.constraint_name
           WHERE tc.table_catalog=current_database() 
              and tc.Table_name <> '$FlywayTableName' 
              AND tc.table_schema NOT IN ('pg_catalog','information_schema')
                ) e;
"@
					$Constraints = Execute-SQL $param1 $query | ConvertFrom-json 
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
            SELECT json_agg(e) 
            FROM(SELECT distinct
	             allindexes.schemaname AS schema,
                t.relname as table_name,
                i.relname as index_name,
                a.attname AS "column_name",
                allindexes.indexdef AS definition
            from
                pg_index ix 
	             INNER join pg_class t on t.oid = ix.indrelid
                INNER JOIN  pg_class i ON i.oid = ix.indexrelid
                inner join pg_attribute a on a.attrelid = t.oid
                LEFT OUTER JOIN pg_indexes allindexes ON t.relname= allindexes.tablename AND i.relname=allindexes.indexname
            where
                a.attnum = ANY(ix.indkey)
                and t.relkind = 'r'
                AND indisprimary=FALSE AND indisunique=FALSE
                AND allindexes.schemaname <>'pg_catalog'
                AND t.relname <> '$FlywayTableName'
            order by
                t.relname,
                i.relname
                ) e;    
"@ | ConvertFrom-Json
					
					#now get all the triggers
					$triggers = Execute-SQL $param1 @'
            SELECT json_agg(e) 
            FROM(
            SELECT TRIGGER_SCHEMA as schema, TRIGGER_NAME, event_object_schema, event_object_table
            FROM information_schema.triggers t
            WHERE t.trigger_catalog=current_database() 
              AND t.trigger_schema NOT IN ('pg_catalog','information_schema')
                ) e; 
'@ | ConvertFrom-Json
					
            <# RDBMS  #>
					$THeTypes = $Routines | Select schema, type -Unique
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					
					
					$TheTypes | Select -ExpandProperty schema -Unique | foreach{
						$TheSchema = $_;
						$ourtypes = @{ }
						$TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
						$SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
						
					}
					
            <# now inject all the objects into the schema tree. First we get all the relations  #>
					$TheRelationMetadata | Select schema, type, object -Unique | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.object;
						$TheColumnList = $TheRelationMetadata |
						where { $_.schema -eq $schema -and $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						foreach{ $_.column }
						$SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
					}
					#display-object $schemaTree
            <# now stitch in the constraints with their columns  #>
					$constraints | Select schema, table_name, Type, constraint_name, referenced_table -Unique | foreach{
						$constraintSchema = $_.schema;
						$constrainedTable = $_.table_name;
						$constraintName = $_.constraint_name;
						$ConstraintType = $_.type;
						$referenced_table = $_.referenced_table;
						# get the original object
						$OriginalConstraint = $constraints |
						where{
							$_.schema -eq $constraintSchema -and
							$_.table_name -eq $constrainedTable -and
							$_.Type -eq $ConstraintType -and
							$_.constraint_name -eq $constraintName
						} | Select -first 1
						$Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
						Select -ExpandProperty column_name
						if ($ConstraintType -eq 'foreign key')
						{
							$Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
							Select -ExpandProperty referenced_column_name
							$SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
								$constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
							}
						}
						else
						{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $columns } }
						
					}
              <# now stitch in the routines and their contents #>
					$routines | Foreach {
						$TheSchema = $_.schema;
						$TheName = $_.name;
						$TheType = $_.type;
						$TheHash = $_.hash;
						$Thecomment = $_.comment;
						$TheDefinition = $_.definition;
						$Contents = @{ }
						if (!([string]::IsNullOrEmpty($TheDefinition))) { $Contents.'definition' = $TheDefinition }
						if ($TheType -ne 'table') { $Contents.'hash' = $Thehash }
						if (!([string]::IsNullOrEmpty($Thecomment))) { $Contents.'comment' = $TheComment }
						if ($SchemaTree.$TheSchema.$TheType.$TheName -eq $null)
						{ $SchemaTree.$TheSchema.$TheType.$TheName = $Contents }
						else
						{ $SchemaTree.$TheSchema.$TheType.$TheName += $Contents }
						
					}
					
<# now stitch in the indexes with their columns  #>
					$indexes | Select schema, table_name, Type, index_name, definition -Unique | foreach{
						$indexSchema = $_.schema;
						$indexedTable = $_.table_name;
						$indexName = $_.index_name;
						$definition = $_.definition;
						$columns = $indexes |
						where{
							$_.schema -eq $indexSchema -and
							$_.table_name -eq $indexedTable -and
							$_.index_name -eq $indexName
						} |
						Select -ExpandProperty column_name
						$SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns; 'def' = "$definition" } }
					}
					
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
				}
<# this is the section that creates a MariaDB or MySQL Database Model based where
possible on information schema 
 #>
				'mysql|mariaDB' {
					#create a delimited list for SQL's IN command
					#fetch all the relations (anything that produces columns)
					$query = @"
        SELECT c.TABLE_SCHEMA as "schema", c.TABLE_NAME as "object", 
            case when v.Table_Name IS NULL then 'Table' ELSE 'View' END AS "Type",
            c.COLUMN_NAME as "column", c.ordinal_position,
            CONCAT (column_type, 
			case when IS_nullable = 'NO' then ' NOT' ELSE '' END ,
			' NULL', 
		  	case when COLUMN_DEFAULT IS NULL then '' ELSE CONCAT (' DEFAULT (',COLUMN_DEFAULT,')') END,
			' ',
			Extra,
			' '
			-- case COLUMN_KEY when 'PRI' then ' PRIMARY KEY' ELSE '' end,
			-- case when column_comment <> '' then CONCAT('-- ',column_comment) ELSE '' end
		 ) AS coltype  
        FROM information_schema.columns c 
        LEFT OUTER JOIN information_schema.views v 
        ON c.TABLE_NAME=v.Table_Name
        AND v.table_Schema=c.table_Schema
        WHERE c.table_schema in ($ListOfSchemas)
        and c.TABLE_NAME <> '$FlywayTableName';
"@
					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
					#now get the details of the routines
					$query = @"
            SELECT 
		Routine_name AS "name", Routine_Schema AS "schema", 
		CONCAT(Routine_Schema,'.', Routine_name) AS "fullname", 
		LOWER(routine_type) AS "type",
		CONCAT(LEFT(routine_definition,800), CASE WHEN LENGTH(routine_definition)>800 THEN '...' ELSE '' END) AS definition, 
		MD5(routine_definition) AS "hash" -- ,
		-- Routine_Comment AS "comment"
	FROM information_schema.routines
	WHERE Routine_Schema IN ($ListOfSchemas) 
UNION ALL
SELECT 
		Table_name AS "name", 
		Table_Schema AS "schema", 
		CONCAT(table_Schema,'.', table_name) AS "fullname", 
		'table' AS "type",
		'' AS definition, 
		MD5('') AS HASH -- ,
		-- TABLE_Comment AS COMMENT
	FROM information_schema.tables
	WHERE Table_type='base table' 
		AND Table_Schema IN ($ListOfSchemas) 
		AND TABLE_NAME NOT LIKE '$FlywayTableName' 
UNION ALL
	SELECT 
		Table_name AS "name", 
		Table_Schema AS "schema", 
		CONCAT(Table_Schema,'.', Table_name) AS "fullname", 
		'view' AS "type",
		View_definition AS definition, 
		MD5(View_definition) AS HASH -- ,
		-- '' AS COMMENT
	FROM information_schema.views
		WHERE Table_Schema IN ($ListOfSchemas) 
		AND TABLE_NAME NOT LIKE '$FlywayTableName'
"@
					$Routines = Execute-SQL $param1 $query | ConvertFrom-json
					#now do the constraints
					$query = @"
             SELECT lower(tc.constraint_type) AS "type", tc.table_schema AS "schema", 
               tc.Table_name, kcu.constraint_name, kcu.column_name,ordinal_position,
			   concat(Referenced_table_Schema,'.',Referenced_table_name) AS "referenced_table",referenced_column_name
            FROM information_schema.table_constraints AS tc 
            JOIN information_schema.key_column_usage AS kcu 
              ON tc.constraint_name = kcu.constraint_name 
              AND tc.table_name=kcu.table_name
              AND tc.table_schema=kcu.table_schema
            WHERE tc.table_schema IN ($ListOfSchemas)
            and tc.TABLE_NAME <> '$FlywayTableName'; 
"@
					$Constraints = Execute-SQL $param1 $query | ConvertFrom-json
            <# Now get the details of all the indexes that aren't primary keys, including the columns,  #>
					$indexes = Execute-SQL $param1 @"
            SELECT Table_Schema as "schema",TABLE_NAME, Index_name, COLUMN_NAME, Seq_in_Index AS "sequence" 
            FROM information_schema.statistics
            WHERE table_Schema IN ($ListOfSchemas) 
              AND index_name NOT IN ('PRIMARY','UNIQUE')
              and TABLE_NAME <> '$FlywayTableName';    
"@ | ConvertFrom-Json
					
					#now get all the triggers
					$triggers = Execute-SQL $param1 @"
            SELECT TRIGGER_SCHEMA, TRIGGER_NAME, event_object_schema, event_object_table
            FROM information_schema.triggers t
            WHERE t.trigger_catalog IN ($ListOfSchemas); 
"@ | ConvertFrom-Json
					
            <# RDBMS  #>
					$THeTypes = $Routines | Select schema, type -Unique
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					
					
					$TheTypes | Select -ExpandProperty schema -Unique | foreach{
						$TheSchema = $_;
						$ourtypes = @{ }
						$TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
						$SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
						
					}
					
            <# now inject all the objects into the schema tree. First we get all the relations  #>
					$TheRelationMetadata | Select schema, type, object -Unique | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.object;
						$TheColumnList = $TheRelationMetadata |
						where { $_.schema -eq $schema -and $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
						Sort-Object -Property ordinal_position |
						foreach{ "$($_.column) $($_.coltype)" }
						$SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
					}
					
            <# now stitch in the constraints with their columns  #>
					$constraints | Select schema, table_name, Type, constraint_name, referenced_table -Unique | foreach{
						$constraintSchema = $_.schema;
						$constrainedTable = $_.table_name;
						$constraintName = $_.constraint_name;
						$ConstraintType = $_.type;
						$referenced_table = $_.referenced_table;
						# get the original object
						$OriginalConstraint = $constraints |
						where{
							$_.schema -eq $constraintSchema -and
							$_.table_name -eq $constrainedTable -and
							$_.Type -eq $ConstraintType -and
							$_.constraint_name -eq $constraintName
						} | Select -first 1
						$Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
						Select -ExpandProperty column_name
						if ($ConstraintType -eq 'foreign key')
						{
							$Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
							Select -ExpandProperty referenced_column_name
							$SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
								$constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
							}
						}
						else
						{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $columns } }
						
					}
					
            <# now stitch in the indexes with their columns  #>
					$indexes | Select schema, table_name, Type, index_name, definition -Unique | foreach{
						$indexSchema = $_.schema;
						$indexedTable = $_.table_name;
						$indexName = $_.index_name;
						$definition = $_.definition;
						$columns = $indexes |
						where{
							$_.schema -eq $indexSchema -and
							$_.table_name -eq $indexedTable -and
							$_.index_name -eq $indexName
						} |
						Select -ExpandProperty column_name
						$SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns; 'def' = "$definition" } }
					}
					$routines | Foreach {
						$TheSchema = $_.schema;
						$TheName = $_.name;
						$TheType = $_.type;
						$TheHash = $_.hash;
						$Thecomment = $_.comment;
						$TheDefinition = $_.definition;
						$Contents = @{ }
						if (!([string]::IsNullOrEmpty($TheDefinition))) { $Contents.'definition' = $TheDefinition }
						if ($TheType -ne 'table') { $Contents.'hash' = $Thehash }
						if (!([string]::IsNullOrEmpty($Thecomment))) { $Contents.'comment' = $TheComment }
						if ($SchemaTree.$TheSchema.$TheType.$TheName -eq $null)
						{ $SchemaTree.$TheSchema.$TheType.$TheName = $Contents }
						else
						{ $SchemaTree.$TheSchema.$TheType.$TheName += $Contents }
						
					}
					
					
					
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
				}
<# start of the oracle section #> 
                'oracle'
                {
                	$ScriptsToExecute = @(
		                @{
			                ResultFile = 'Objects.json';
			                SQL =@"
                /*--- get list of objects ---*/
                select obj.OBJECT_TYPE as "type", obj.OWNER as "schema"
                from sys.all_objects obj
                where obj.owner in ($ListOfSchemas)  
                and object_type not in ('INDEX','SEQUENCE')
                group by obj.object_type,obj.owner;
"@
		                }
		                @{
			                ResultFile = 'columns.json';
			                SQL =@"
                /*--- get all columns and their definitions ---*/
                select col.column_id as "ordinal_position", col.owner as "schema",  type.The_Type as "type",
                       col.table_name as "object", col.column_name as "column", 
                       data_type||
                case
                when col.data_precision is not null and nvl(col.data_scale,0)>0 
                    then '('||col.data_precision||','||col.data_scale||')'
                when col.data_precision is not null and nvl(col.data_scale,0)=0 
                    then '('||col.data_precision||')'
                when col.data_precision is null and col.data_scale is not null 
                    then '(*,'||col.data_scale||')'
                when col.char_length>0 
                    then '('||col.char_length|| 
                        case col.char_used 
                             when 'B' then ' Byte'
                             when 'C' then ' Char'
                             else null 
                        end||')'
                end||decode(nullable, 'N', ' NOT NULL') as "coltype"
                from sys.all_tab_columns col
                inner join 
                    (Select Table_name, 'Table' as The_Type, owner
                    from sys.all_tables
                    union all 
                    Select view_name, 'View',owner
                    from sys.all_views)type
                on col.owner = type.owner 
                and col.table_name = type.table_name
                where type.owner  in ('DBO','PEOPLE','ACCOUNTING') 
                and  type.table_name <> '$FlywayTableName'
                order by "schema", "object", "type", "ordinal_position";
"@
		                }
		                @{
			                ResultFile = 'routines.json';
			                SQL =@"
                /*--- get all routines ---*/
                Select f.schema, f.name, f.type, f.definition, '' as "comment", 0 as "hash",
                 LISTAGG(args.in_out || ' ' || args.data_type, '; ')
                              WITHIN GROUP (ORDER BY position) as arguments
                from (
                select obj.owner as schema,
                       obj.object_id,
                       obj.object_name as name,
                       obj.object_type as type,
                       listagg(text) within group (order by line) as definition
                from sys.all_objects obj
                inner join sys.all_source source 
                  on  source.owner=obj.owner 
                  and source.name=obj.object_name and source.type=object_type
                where obj.object_type in ('PROCEDURE','FUNCTION')
                      and obj.owner in ($ListOfSchemas)  
                      group by obj.owner, obj.object_id, obj.object_type, obj.object_name)f
                left outer join sys.all_arguments args on args.object_id = f.object_id
                left outer join (
                      select object_id,
                             object_name,
                             data_type
                      from sys.all_arguments
                      where position = 1
                ) ret on ret.object_id = f.object_id
                       and ret.object_name = f.name
                group by f.schema, f.name, f.type, f.definition;

"@
		                }
		                @{
			                ResultFile = 'constraints.json';
			                SQL =@"
                /* get all constraints and foreign key constraint targets */
                Select cols.position as "ordinal_position", source.owner as "schema",source.table_name as "table_name", case source.constraint_type  when 'C' then 'check constraint' when 'P' then 'primary key' when 'F' then 'foreign key' when 'U' then 'unique key' 
                        when 'R' then 'foreign_key' when 'V' then 'View Check option'  when 'O' then 'With read only' else source.constraint_type end  as "type", source.constraint_name, 
                source.owner || '.' || source.table_name as Source_Table,cols.column_name as "column_name",
                source.search_condition as "condition", 
                case when source.constraint_type = 'R' then source.R_constraint_name else null end as target_Constraint, 
                case when target.table_name is not null then target.owner || '.' || target.table_name else null end  as "referenced_table",
                FKcols.column_name as "referenced_column_name"
                from all_cons_columns cols
                inner join  all_constraints source
                on cols.TABLE_Name=source.table_name and cols.owner=source.owner  and source.constraint_name=cols.constraint_name
                left outer join all_constraints target
                on source.R_constraint_name =target.constraint_name and source.owner=target.owner 
                left outer join all_cons_columns FKcols
                on FKcols.TABLE_Name=target.table_name and FKcols.owner=target.owner and target.constraint_name=fkcols.constraint_name
                where source.owner in ($ListOfSchemas)  
                and source.table_name not like 'BIN$%'
                and source.table_name not like '$FlywayTableName'
                and source.constraint_name not like 'SYS%';
"@
		                }
		                @{
			                ResultFile = 'triggers.json';
			                SQL =@"
                /*--- get all Triggers ---*/
                select trig.table_owner as "schema", trig.table_name as "Name", trig.owner as "triggerSchema",
                       trig.trigger_name as "triggerName",  trig.trigger_type as "triggerType",
                       trig.base_object_type as "baseObjectType", trig.triggering_event as "event", 
                       trig.status as "status", trig.trigger_body as "script"       
                from sys.all_triggers trig
                inner join sys.all_tables tab on trig.table_owner = tab.owner
                                                and trig.table_name = tab.table_name
                where trig.owner in ($ListOfSchemas);
"@
		                }
		                @{
			                ResultFile = 'Indexes.json';
			                SQL =@"
                /*--- get all Indexes ---*/
                select ind.index_name,
                       ind_col.column_name,
                       ind.index_type,
                       ind.uniqueness as definition,
                       ind.table_owner as schema,
                       ind.table_name as table_name,
                       ind.table_type as object_type,
                       f.column_expression
                from sys.all_indexes ind
                inner join sys.all_ind_columns ind_col on ind.owner = ind_col.index_owner
                                                    and ind.index_name = ind_col.index_name
                LEFT JOIN all_ind_expressions f
                 ON   ind_col.index_owner     = f.index_owner
                 AND  ind_col.index_name      = f.index_name
                 AND  ind_col.table_owner     = f.table_owner
                 AND  ind_col.table_name      = f.table_name
                 AND  ind_col.column_position = f.column_position
                where ind.table_owner like 'DBO'
                and ind.table_name not like '$FlywayTableName'
                order by ind.table_owner,
                         ind.table_name,
                         ind.index_name,
                         ind_col.column_position;
"@
		                }
	                )
	                $ExecutePLSQLScript.invoke($param1, $null, $ScriptsToExecute)
	                $TheTypes = ([IO.File]::ReadAllText("$pwd/objects.json") | convertfrom-json).results.items
	                $TheRelationMetadata = ([IO.File]::ReadAllText("$pwd/columns.json") | convertfrom-json).results.items
	                $constraints = ([IO.File]::ReadAllText("$pwd/constraints.json") | convertfrom-json).results.items
	                $indexes = ([IO.File]::ReadAllText("$pwd/indexes.json") | convertfrom-json).results.items
	                $routines = ([IO.File]::ReadAllText("$pwd/routines.json") | convertfrom-json).results.items
                    <# do the mass extinctiuon of the temporary files #>
                    @("$pwd\objects.json","$pwd\columns.json","$pwd\constraints.json",
                    "$pwd\indexes.json","$pwd\routines.json","$pwd\triggers.json") | foreach{
	                    If (Test-Path -Path "$_")
	                    {
		                    Remove-Item "$_"
	                    }
                    }	
                           <# RDBMS  #>
                            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
	                $SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
                            all the types of relations  #>
	
	
	                $TheTypes | Select -ExpandProperty schema -Unique | foreach{
		                $TheSchema = $_;
		                $ourtypes = @{ }
		                $TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
		                $SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
		
	                }
	
                            <# now inject all the objects into the schema tree. First we get all the relations  #>
	                $TheRelationMetadata | Select schema, type, object -Unique | foreach{
		                $schema = $_.schema;
		                $type = $_.type;
		                $object = $_.object;
		                $TheColumnList = $TheRelationMetadata |
		                where { $_.schema -eq $schema -and $_.type -eq $type -and $_.object -eq $object } -OutVariable pk |
		                Sort-Object -Property ordinal_position |
		                foreach{ "$($_.column) $($_.coltype)" }
		                $SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
	                }
	
                            <# now stitch in the constraints with their columns  #>
	                $constraints | Select schema, table_name, Type, constraint_name, referenced_table -Unique | foreach{
		                $constraintSchema = $_.schema;
		                $constrainedTable = $_.table_name;
		                $constraintName = $_.constraint_name;
		                $ConstraintType = $_.type;
		                $referenced_table = $_.referenced_table;
		                # get the original object
		                $OriginalConstraint = $constraints |
		                where{
			                $_.schema -eq $constraintSchema -and
			                $_.table_name -eq $constrainedTable -and
			                $_.Type -eq $ConstraintType -and
			                $_.constraint_name -eq $constraintName
		                } | Select -first 1
		                $Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
		                Select -ExpandProperty column_name
		                if ($ConstraintType -eq 'foreign key')
		                {
			                $Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
			                Select -ExpandProperty referenced_column_name
			                $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
				                $constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
			                }
		                }
		                else
		                {
			                $SchemaTree.$constraintSchema.table.$constrainedTable."$ConstraintType" += @{ $constraintName = $columns }
		                }
		
	                }
	
                            <# now stitch in the indexes with their columns  #>
	                $indexes | Select schema, table_name, Type, index_name, definition -Unique | foreach{
		                $indexSchema = $_.schema;
		                $indexedTable = $_.table_name;
		                $indexName = $_.index_name;
		                $definition = $_.definition;
		                $columns = $indexes |
		                where{
			                $_.schema -eq $indexSchema -and
			                $_.table_name -eq $indexedTable -and
			                $_.index_name -eq $indexName
		                } |
		                Select -ExpandProperty column_name
		                $SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns; 'def' = "$definition" } }
	                }
	                $routines | Foreach {
		                $TheSchema = $_.schema;
		                $TheName = $_.name;
		                $TheType = $_.type;
		                $TheHash = $_.hash;
		                $Arguments = $_.arguments;
		                $Thecomment = $_.comment;
		                $TheDefinition = $_.definition;
		                $Contents = @{ }
		                if (!([string]::IsNullOrEmpty($TheDefinition))) { $Contents.'definition' = $TheDefinition }
		                if ($TheType -ne 'table') { $Contents.'hash' = $Thehash }
		                if (!([string]::IsNullOrEmpty($Thecomment))) { $Contents.'comment' = $TheComment }
		                if ($SchemaTree.$TheSchema.$TheType.$TheName -eq $null)
		                { $SchemaTree.$TheSchema.$TheType.$TheName = $Contents }
		                else
		                { $SchemaTree.$TheSchema.$TheType.$TheName += $Contents }
		
	                } #$what |convertTo-json
		            $Triggers | Foreach {
                        if ($_ -ne $null)
                            {
                            $Theschema= $_.schema;
                            $Thename= $_.name;
                            $Thetriggerschema= $_.triggerschema;
                            $Thetriggername= $_.triggername;
                            $Thetriggertype= $_.triggertype;
                            $Thebaseobjecttype= $_.baseobjecttype;
                            $Theevent= $_.event; 
                            $Thestatus= $_.status; 
                            $Thescript= $_.script;
		                    $Contents = @{'Name'=$Thetriggername; 'Type'=$Thetriggertype;
                            'event'=$Theevent;'status'=$Thestatus; 'script'=$Thescript};
     	                    $SchemaTree.$TheSchema.$Thebaseobjecttype.$Thename.'trigger' = $Contents
                            }
	                    }	
	
	                $SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
	                $SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
                }
				
<# this is the section that creates a SQL Server Database Model based where
possible on information schema #>
				'sqlserver'  {
#Fetch the preliminaries such as user-defined types, schemas, and RDBMS-specifics.
$Preliminaries = Execute-SQL $param1 @"
    SELECT TheSchema.name AS "schema", 'userType' AS "Type",
       UserTypes.name AS name,
       'CREATE TYPE ' + TheSchema.name + '.' + UserTypes.name + ' FROM '
       + sysTypes.name + ' '
       + CASE WHEN UserTypes.precision <> 0 AND UserTypes.scale <> 0 THEN
                '(' + Convert (VARCHAR(5), UserTypes.precision) + ','
                + Convert (VARCHAR(5), UserTypes.scale) + ')'
           WHEN UserTypes.max_length > 0 THEN
             '(' + Convert (VARCHAR(5), UserTypes.max_length) + ')'
           WHEN UserTypes.max_length = -1 THEN '(MAX)' ELSE '' END
       + CASE WHEN UserTypes.is_nullable = 0 THEN ' NOT' ELSE '' END
       + ' NULL' AS "definition"
  FROM
  sys.types UserTypes
    INNER JOIN sys.types sysTypes
      ON UserTypes.system_type_id = sysTypes.system_type_id
     AND sysTypes.is_user_defined = 0
     AND UserTypes.is_user_defined = 1
     AND sysTypes.system_type_id = sysTypes.user_type_id
    INNER JOIN sys.schemas AS TheSchema
      ON TheSchema.schema_id = UserTypes.schema_id
    for json path
"@ | ConvertFrom-Json
if ($Preliminaries.Error -ne $null)
                        { $Problems += $Preliminaries.Error
                        write-error "$($Preliminaries.Error)"}
                         
					#fetch all the relations (anything that produces columns)
	$query = @"
SELECT
	Object_Schema_Name(ParentObjects.object_id) AS "Schema",
	REPLACE(LOWER(REPLACE(REPLACE(ParentObjects.type_Desc, 'user_', ''), 'sql_', '')), '_', ' ') AS "type",
	ParentObjects.Name,
	colsandparams.name + ' ' +
	CASE
		WHEN colsandparams.definition IS NOT NULL THEN ' AS ' + colsandparams.definition
		ELSE
			CASE
				WHEN t.is_user_defined = 1 THEN ts.name + '.'
				ELSE ''
			END + t.[name] +
			CASE
				WHEN t.[name] IN ('char', 'varchar', 'nchar', 'nvarchar') THEN
					'(' +
					CASE
						WHEN colsandparams.ValueTypemaxlength = -1 THEN 'MAX'
						ELSE CONVERT(VARCHAR(4),
							CASE
								WHEN t.[name] IN ('nchar', 'nvarchar') THEN colsandparams.ValueTypemaxlength / 2
								ELSE colsandparams.ValueTypemaxlength
							END
						)
					END + ')'
				WHEN t.[name] IN ('decimal', 'numeric') THEN
					'(' + CONVERT(VARCHAR(4), colsandparams.ValueTypePrecision) +
					',' + CONVERT(VARCHAR(4), colsandparams.ValueTypeScale) + ')'
				ELSE ''
			END +
			CASE
				WHEN colsandparams.is_identity = 1 THEN
					' IDENTITY(' + CONVERT(VARCHAR(5), colsandparams.seed_value) + ',' +
					CONVERT(VARCHAR(5), colsandparams.increment_value) + ')'
				ELSE ''
			END +
			CASE
				WHEN colsandparams.XMLcollectionID <> 0 THEN
					'(' +
					CASE
						WHEN colsandparams.isXMLDocument = 1 THEN 'DOCUMENT '
						ELSE 'CONTENT '
					END +
					COALESCE(
						QUOTENAME(Schemae.name) + '.' + QUOTENAME(SchemaCollection.name),
						'NULL'
					) + ')' +
					CASE
						WHEN colsandparams.collation_name IS NOT NULL THEN
							' COLLATE ' + colsandparams.collation_name + ' '
						ELSE ''
					END +
					CASE
						WHEN colsandparams.is_nullable = 0 THEN ' NOT'
						ELSE ''
					END + ' NULL '
				ELSE ''
			END +
			COALESCE(' -- ' + colsandparams.Description, '')
	END AS "Column",
	colsandparams.TheOrder AS "TheOrder"
FROM
	(SELECT
		cols.object_id,
		cols.name,
		'Columns' AS "Type",
		CONVERT(NVARCHAR(2000), EP.value) AS "Description",
		cols.column_id AS TheOrder,
		cols.xml_collection_id,
		cols.max_length AS ValueTypemaxlength,
		cols.precision AS ValueTypePrecision,
		cols.scale AS ValueTypeScale,
		cols.is_nullable,
		cols.is_identity,
		cc.definition,
		IC.seed_value,
		IC.increment_value,
		cols.xml_collection_id AS XMLcollectionID,
		cols.is_xml_document AS isXMLDocument,
		cols.user_type_id,
		cols.collation_name
	FROM
		sys.objects AS object
		INNER JOIN sys.columns AS cols ON cols.object_id = object.object_id
		LEFT OUTER JOIN sys.extended_properties AS EP ON cols.object_id = EP.major_id
			AND EP.class = 1
			AND EP.minor_id = cols.column_id
			AND EP.name = 'MS_Description'
		LEFT OUTER JOIN sys.identity_columns IC ON ic.column_id = cols.column_id
			AND ic.object_id = object.object_id
		LEFT OUTER JOIN sys.computed_columns cc ON cols.object_id = cc.object_id
			AND cols.column_id = cc.column_id
	WHERE
		object.is_ms_shipped = 0
	UNION ALL
	SELECT
		params.object_id,
		params.name AS "Name",
		CASE WHEN params.parameter_id = 0 THEN 'Return' ELSE 'Parameters' END AS "Type",
		CONVERT(NVARCHAR(2000), EP.value) AS "Description",
		params.parameter_id AS TheOrder,
		params.xml_collection_id,
		params.max_length AS ValueTypemaxlength,
		params.precision AS ValueTypePrecision,
		params.scale AS ValueTypeScale,
		params.is_nullable,
		0 AS is_Identity,
		NULL AS "definition",
		NULL AS seed_value,
		NULL AS increment_value,
		params.xml_collection_id AS XMLcollectionID,
		params.is_xml_document AS isXMLDocument,
		params.user_type_id,
		NULL AS collation_name
	FROM
		sys.objects AS object
		INNER JOIN sys.parameters AS params ON params.object_id = object.object_id
		LEFT OUTER JOIN sys.extended_properties AS EP ON params.object_id = EP.major_id
			AND EP.class = 2
			AND EP.minor_id = params.parameter_id
			AND EP.name = 'MS_Description'
	WHERE
		object.is_ms_shipped = 0
	) AS colsandparams
	INNER JOIN sys.types AS t ON colsandparams.user_type_id = t.user_type_id
	INNER JOIN sys.schemas ts ON t.schema_id = ts.schema_id
	LEFT OUTER JOIN sys.xml_schema_collections AS SchemaCollection ON SchemaCollection.xml_collection_id = colsandparams.xml_collection_id
	LEFT OUTER JOIN sys.schemas AS Schemae ON SchemaCollection.schema_id = Schemae.schema_id
	INNER JOIN sys.objects ParentObjects ON ParentObjects.object_id = colsandparams.object_id
WHERE
	Object_Name(ParentObjects.object_id) <> '$FlywayTableName'
	AND Object_Schema_Name(ParentObjects.object_id) IN ($ListOfSchemas)
ORDER BY
	"Schema", "type", Name, colsandparams.TheOrder
FOR JSON PATH
"@

					$TheRelationMetadata = Execute-SQL $param1 $query | ConvertFrom-json
                    if ($TheRelationMetadata.Error -ne $null)
                        { $Problems += $TheRelationMetadata.Error
                        write-error "$($TheRelationMetadata.Error)"}
    #now we need to get the base 'parent' types
                    $query = @"
--all the base types and their documentation (and any other common information)
SELECT sch.name AS "schema", 
  Replace (Lower (Replace(Replace(o.type_desc,'user_',''),'sql_','')), '_', ' ') as "type",
   o.name AS "Name",
  Coalesce(ep.value,'') AS "documentation"
FROM sys.objects o
INNER JOIN sys.schemas sch
ON sch.schema_id = o.schema_id AND o.parent_object_id = 0
LEFT OUTER JOIN sys.extended_properties ep
ON o.object_id = ep.major_id  AND  ep.minor_id=0 
AND ep.name LIKE 'MS_Description'
WHERE sch.name IN ($ListOfSchemas)
AND o.name <> '$FlywayTableName'
FOR JSON path
"@
					$TheBaseTypes = Execute-SQL $param1 $query | ConvertFrom-json
                    if ($TheBaseTypes.Error -ne $null)
                        {write-error "$($TheBaseTypes.Error)";
                        $Problems += $TheBaseTypes.Error;
                        }
                        #now get the details of the routines
					$query = @'
SELECT Replace (Lower (Replace(Replace(so.type_desc,'user_',''),'sql_','')), '_', ' ') as type,
        so.name, Object_Schema_Name(so.object_id) AS "schema", 
        definition, 
        checksum(definition) AS hash, ep.value AS documentation
        FROM sys.sql_modules ssm
        INNER JOIN sys.objects so
        ON so.OBJECT_ID=ssm.object_id
		LEFT OUTER JOIN sys.extended_properties ep
        ON so.object_id = ep.major_id
		and minor_id=0 AND ep.name LIKE 'MS_Description'
        FOR JSON path                   
'@
					$Routines = Execute-SQL $param1 $query | ConvertFrom-json
                    if ($Routines.Error -ne $null)
                        {write-error "$($Routines.Error)";
                        $Problems += $Routines.Error;
                        }					
					#now do the constraints
					$query = @"
SELECT *
  FROM
    (SELECT Schema_Name (tab.schema_id) AS "schema",
            tab.name AS [table_name], 'Foreign Key' AS "type",
            fk.name AS "constraint_name", '' AS "definition",
            Schema_Name (pk_tab.schema_id) + '.' + pk_tab.name AS referenced_table,
            col.name AS column_name,
            fk_cols.constraint_column_id AS ordinal_position,
            pk_col.name AS referenced_column,
            fk_cols.constraint_column_id AS referenced_ordinal_position
       FROM
       sys.tables tab
         INNER JOIN sys.columns col
           ON col.object_id = tab.object_id
         INNER JOIN sys.foreign_key_columns fk_cols
           ON fk_cols.parent_object_id = tab.object_id
          AND fk_cols.parent_column_id = col.column_id
         INNER JOIN sys.foreign_keys fk
           ON fk.object_id = fk_cols.constraint_object_id
         INNER JOIN sys.tables pk_tab
           ON pk_tab.object_id = fk_cols.referenced_object_id
         INNER JOIN sys.columns pk_col
           ON pk_col.column_id = fk_cols.referenced_column_id
          AND pk_col.object_id = fk_cols.referenced_object_id
     UNION ALL
     SELECT Object_Schema_Name (tab.object_id) AS "Schema", tab.name,
            CASE WHEN pk.is_primary_key = 1 THEN 'primary key'
              WHEN pk.is_unique_constraint = 1 THEN 'unique key' ELSE 'index' END AS "Type",
            pk.name, '' AS "Definition", NULL AS referenced_table,
            col.name AS fk_column_name, col.column_id AS fk_ordinal_position,
            NULL AS referenced_column, NULL AS referenced_ordinal_position
       FROM
       sys.tables tab
         LEFT OUTER JOIN sys.indexes pk
           ON tab.object_id = pk.object_id
         INNER JOIN sys.index_columns ic
           ON ic.object_id = tab.object_id AND ic.index_id = pk.index_id
         INNER JOIN sys.columns col
           ON ic.object_id = col.object_id AND ic.column_id = col.column_id
     UNION ALL
     SELECT Schema_Name (t.schema_id) AS "Schema", t.[name],
            'Check constraint', con.[name] AS constraint_name,
            con.[definition], NULL, NULL, NULL, NULL, NULL
       FROM
       sys.check_constraints con
         LEFT OUTER JOIN sys.objects t
           ON con.parent_object_id = t.object_id
         LEFT OUTER JOIN sys.all_columns col
           ON con.parent_column_id = col.column_id
          AND con.parent_object_id = col.object_id
     UNION ALL
     SELECT Schema_Name (t.schema_id) AS "Schema", t.[name],
            'Check constraint', con.[name] AS constraint_name,
            con.[definition], NULL, NULL, NULL, NULL, NULL
       FROM
       sys.check_constraints con
         LEFT OUTER JOIN sys.objects t
           ON con.parent_object_id = t.object_id
         LEFT OUTER JOIN sys.all_columns col
           ON con.parent_column_id = col.column_id
          AND con.parent_object_id = col.object_id
     UNION ALL
     SELECT Schema_Name (t.schema_id) AS "Schema", t.[name],
            'Default constraint', con.[name],
            col.[name] + ' = ' + con.[definition], NULL, NULL, NULL, NULL,
            NULL
       FROM
       sys.default_constraints con
         LEFT OUTER JOIN sys.objects t
           ON con.parent_object_id = t.object_id
         LEFT OUTER JOIN sys.all_columns col
           ON con.parent_column_id = col.column_id
          AND 
           con.parent_object_id = col.object_id) f
 where f.table_name <> '$FlywayTableName'
FOR JSON AUTO
"@
					$Constraints = Execute-SQL $param1 $query | ConvertFrom-json
					if (!($constraints.Error -eq $null)) { $Problems += $constraints.Error }
<#            
           # Now get the details of all the indexes that aren't primary keys, including the columns,  
					$indexes = Execute-SQL $param1 @"
    SELECT Schema_Name (t.schema_id) AS "schema", t.name AS table_name,
       Replace (
         Lower (Replace (Replace (t.type_desc, 'user_', ''), 'sql_', '')),
         '_',
         ' ') AS type, i.type_desc AS indexType, i.[name] AS Index_name,
       col.name, ic.key_ordinal
      FROM
      sys.indexes i
        INNER JOIN sys.objects t
          ON t.object_id = i.object_id
        INNER JOIN sys.index_columns ic
          ON ic.object_id = t.object_id AND ic.index_id = i.index_id
        INNER JOIN sys.columns col
          ON ic.object_id = col.object_id AND ic.column_id = col.column_id
      WHERE
      t.is_ms_shipped = 0
    AND i.is_primary_key = 0
    AND i.is_unique = 0
    AND i.type IN (1, 2)
       for json path
"@ | ConvertFrom-Json
#>
					
					#now get all the triggers
					$triggers = Execute-SQL $param1 @'
 select schema_name(tab.schema_id) AS "schema",
 tab.name as [table],
    trig.name as trigger_name,
    case when is_instead_of_trigger = 1 then 'Instead of'
        else 'After' end as [activation],
    (case when objectproperty(trig.object_id, 'ExecIsUpdateTrigger') = 1 
            then 'Update ' else '' end
    + case when objectproperty(trig.object_id, 'ExecIsDeleteTrigger') = 1 
            then 'Delete ' else '' end
    + case when objectproperty(trig.object_id, 'ExecIsInsertTrigger') = 1 
            then 'Insert ' else '' end
    ) as [event],
    case when trig.[type] = 'TA' then 'Assembly (CLR) trigger'
        when trig.[type] = 'TR' then 'SQL trigger' 
        else '' end as [type],
    case when is_disabled = 1 then 'Disabled'
        else 'Active' end as [status],
		left(object_definition(trig.object_id),70)+CASE when LEN(object_definition(trig.object_id))>70 THEN '...' ELSE '' END AS definition, 
        checksum(object_definition(trig.object_id)) AS hash
from sys.triggers trig
    inner join sys.objects tab
        on trig.parent_id = tab.object_id
order by schema_name(tab.schema_id) + '.' + tab.name, trig.name
FOR JSON auto
'@ | ConvertFrom-Json
					
            <# RDBMS  #>
					#$ErrorActionPreference='Stop'
                    #$SchemaTree|convertto-json -depth 10 
                    #$SchemaTree=New-Object PSObject @{}
                    #$TheBaseTypes= schema, type, Name, documentation
                   	$THeTypes = ($TheBaseTypes | Select schema, type)+($preliminaries | Select schema, type)+($routines | Select schema, type)|Select schema,type -unique
            <# OK. we now have to assemble all this into a model that is as human-friendly as possible  #>
					$SchemaTree = @{ } <# This will become our model of the schema. Fist we put in
            all the types of relations  #>
					$TheTypes | Select -ExpandProperty schema -Unique | foreach{
						$TheSchema = $_;
						$ourtypes = @{ }
						$TheTypes | where { $_.schema -eq $TheSchema } | Select -ExpandProperty type | foreach{ $OurTypes += @{ $_ = @{ } } }
						$SchemaTree | add-member -NotePropertyName $TheSchema -NotePropertyValue $OurTypes
					}          
            #Get the tables and columns in place
                      #Schema, type, Name , Column, TheOrder
                     $TheRelationMetadata | Select schema, type, name -Unique | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.name;
						$TheColumnList = $TheRelationMetadata |
						where { $_.schema -eq $schema -and $_.type -eq $type -and $_.name -eq $object } |
						foreach{ $_.column }
						$SchemaTree.$schema.$type += @{ $object = @{ 'columns' = $TheColumnList } }
					} 
            #Add in the documentation for the tables          
                     $TheBaseTypes | Select schema, type, name, documentation | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.name;
                        $Documentation = $_.documentation;
						$SchemaTree.$schema.$type.$object += @{ 'documentation'=$Documentation } 
					}           
 
            <# now we get the difinitions for all the routines  #>
                    #Routines=schema, type, name. definition
					($Routines | Select schema, type, name, definition)+
                    ($preliminaries | Select schema, type, name, definition) | foreach{
						$schema = $_.schema;
						$type = $_.type;
						$object = $_.name;
						$Definition = $_.definition
						$SchemaTree.$schema.$type.$Object += @{ 'Definition'=$Definition } 
					}
					#$SchemaTree.$schema.$type.$Object.Definition
                    #$SchemaTree|convertto-json -depth 10
					#$SchemaTree.$schema.$type.$Object|convertto-json -depth 10
            <# now stitch in the constraints and indexes with their columns  #>
                    #Constraints=schema ,table_name,type,constraint_name ,definition ,referenced_table,column_name,ordinal_position,
                    #referenced_column,referenced_ordinal_position
					$constraints | Select schema, table_name, Type, constraint_name, referenced_table, definition -Unique | foreach{
						$constraintSchema = $_.schema;
						$constrainedTable = $_.table_name;
						$constraintName = $_.constraint_name;
						$ConstraintType = $_.type;
						$referenced_table = $_.referenced_table;
						$definition = $_.definition;
 						# get the original object
						if ($ConstraintType -notin @('Unique key','index', 'Primary key', 'foreign key'))
						{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $definition } }
						else
						{
							#we have to deal with columns
							$OriginalConstraint = $constraints |
							where{
								$_.schema -eq $constraintSchema -and
								$_.table_name -eq $constrainedTable -and
								$_.Type -eq $ConstraintType -and
								$_.constraint_name -eq $constraintName
							} #| Select -first 1 --why this?
							$Columns = $OriginalConstraint | Sort-Object -Property ordinal_position |
							Select -ExpandProperty column_name
							if ($ConstraintType -eq 'foreign key')
							{
								$Referencing = $OriginalConstraint | Sort-Object -Property ordinal_position |
								Select -ExpandProperty referenced_column
								$SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{
									$constraintName = @{ 'Cols' = $columns; 'Foreign Table' = $referenced_table; 'Referencing' = "$Referencing" }
								}
							}
							elseif ($ConstraintType -in @('Unique key', 'index', 'Primary key'))
							{ $SchemaTree.$constraintSchema.table.$constrainedTable.$ConstraintType += @{ $constraintName = $columns } }
							
						}
						
					}
				  <# these are already included #>
                  <# now stitch in the indexes with their columns  
					$indexes | Select schema, table_name, Type, IndexType, index_name -Unique | foreach{
						$indexSchema = $_.schema;
						$indexedTable = $_.table_name;
						$indexName = $_.index_name;
						$indexType = $_.indexType;
                        $columns = $indexes |
						where{
							$_.schema -eq $indexSchema -and
							$_.table_name -eq $indexedTable -and
							$_.index_name -eq $indexName
						} | Sort-Object -Property key_ordinal | Select -ExpandProperty name
                        Write-Warning "$indexSchema $indexedTable $indexName "
						$SchemaTree.$indexSchema.table.$indexedTable.index += @{ $indexName = @{ 'Indexing' = $columns;'IndexType'=$indexType } }
					} #>
					$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8'
					$SchemaTree | convertTo-json -depth 10 > "$MyOutputReport"
					$SchemaTree | convertTo-json -depth 10 > "$MycurrentReport"
					
					
				} #end SQL Server
				default
				{
					$Param1.Problems.'SavedDatabaseModelIfNecessary' += "The $_ database isn't supported yet. Sorry about that."
				}
			}
			#Final things to do: Break up the model into individual objects, in different folders depending on type
            #while we are about it, we'll do the table manifest
			if (Test-Path "$MyOutputReport" -PathType leaf)
			{
			$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8' #we'll need it
			$Model = [IO.File]::ReadAllText("$MyOutputReport") | ConvertFrom-JSON
			$model.psobject.Properties |
			foreach{ $schema = $_.Name; $_.Value.psobject.Properties } |
			Foreach{ $Type = $_.Name.ToLower(); $_.Value.psobject.Properties } |
			foreach{
				$objectName = $_.Name;
				$WhereToStoreIt = "$MyModelPath\$type"
				if (-not (Test-Path "$WhereToStoreIt" -PathType Container))
				{ $null = New-Item -ItemType directory -Path "$WhereToStoreIt" -Force }
				$_.Value | convertto-json > "$WhereToStoreIt\$schema.$objectName.json"
				Copy-Item -Path "$MyModelPath" -Destination "$MyCurrentPath" -Recurse -Force
			}
			$feedback += "written object-level model to $MyModelPath"
			
            #now do the table manifest
            #calculate the path to save the manifest to
            $MyManifestPath="$(split-path -Path $MyOutputReport -Parent)\TableManifest.txt";
            #get all the foreign key references
            $TableReferences = Display-Object $Model | where { $_.path -like '$*.*.table.*.Foreign key.*' } | foreach{
	            $Splitpath = ($_.Path -split '\.')
	            $References = $_.value.'Foreign Table'; $Table = "$($Splitpath[1]).$($Splitpath[3])";
	            [pscustomObject]@{ 'referencing' = $Table; 'references' = $references }
            }
            $ObjectsToBuild = Display-Object $Model -reportNodes $true | where {
                 ($_.path -split '\.').count -eq 4 
                 } | select Path # get all the objects (for later full manifests)
            $Tables = $ObjectsToBuild | where { $_.path -like '$*.*.table.*' } | foreach {
	            $Splitpath = ($_.Path -split '\.'); "$($Splitpath[1]).$($Splitpath[3])"
            }
            # Now we work out the dependency order. 
	        # We put them in sorted order within their dependency group, starting with the tables that
	        # don't reference anything. 
	        $TablesInDependencyOrder = @()
	        $ii = 20; # just to stop mutual dependencies hanging the script
	        Do
	        {
		        $NotYetPicked = $Tables | where { $_ -notin $TablesInDependencyOrder }
		        $TablesInDependencyOrder += $notyetpicked | foreach{
			        $Name = $_; $Referencing = ($TableReferences | where { $_.referencing -eq $Name }).references;
			        $AlreadyThere = $Referencing | where { $_ -in $TablesInDependencyOrder }
			        if ($Referencing.Count -eq $AlreadyThere.Count) { $Name }
		        } | sort-object
		        $ii--;
	        }
	        while (($TablesInDependencyOrder.count -lt $Tables.count) -and ($ii -gt 0))
	        if ($TablesInDependencyOrder.count -ne $Tables.count)
	        { Throw 'could not get tables in dependency order' }
            $TablesInDependencyOrder > $MyManifestPath #and save the manifest
            $feedback += "written table manifest  to $MyManifestPath"
            }
		}
		catch { $problems += "$($PSItem.Exception.Message)" }
	}
	else
	{
		$AlreadyDone = $true;
		$feedback += "Nothing to do. The model is already there in '$MyOutputReport'"
	}
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'SaveDatabaseModelIfNecessary' += $problems;
	}
	else
	{
		if ($AlreadyDone)
		{
			$Param1.Feedback.'SaveDatabaseModelIfNecessary' = "$MyOutputReport already exists"
		}
		else
		{
			if ($feedback.count -gt 0)
			{ $Param1.Feedback.'SaveDatabaseModelIfNecessary' = $feedback }
			$Param1.WriteLocations.'SaveDatabaseModelIfNecessary' = $MyOutputReport;
		}
	}
	
}



<# By default, this will take the DACPAC for the current version of the database, and will 
   compare it with the previous version. it should, logically, work either way to do forward 
   or UNDO migrations. Sadly it doesn't.  However, you  can use it to compare a source and target
   version of your own choosing, simply by providing version numbers for the two parameters 
   $TheSourceVersion and $TheTargetVersion.
   The file is processed to remove  any CREATE SCHEMA statements,SQLCMD statements
   and any SQLCMD parameter definitions #>
$CreatePossibleMigrationScriptFromDacpac = { 
	Param ($param1, # the dbDetails array where we pass all the info
		$OutputScriptType = 'Versioned',
        [bool]$OverWrite = $true
	) # $CreatePossibleMigrationScriptFromDacpac  (Don't delete this) 
	$problems = @(); # well, not yet
	$feedback = @()
	@('version', 'previous') |
		foreach{
			if ($param1.$_ -in @($null, ''))
			{
				Process-FlywayTasks $param1 $GetCurrentVersion;
			}
		}
		
	@('version', 'previous', 'project', 'reportLocation') | foreach{
		if ($param1.$_ -in @($null, '')) { $WeCanDoIt = $false; $Problems += "no value for '$($_)'" }
	}
	$EscapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$ReportDirectory = "$($param1.reportLocation)\Versions";
	#default to undo
	$SourceVersion = $param1.previous
	$TargetVersion = $param1.version
	$prefix = 'U';
    if ($OutputScriptType -eq 'versioned')
	{
		$SourceVersion = $param1.version
		$TargetVersion = $param1.previous
        $prefix = 'V'
	}
	if ([version]$SourceVersion -gt [version]$TargetVersion) {$prefix = 'U'};
	$SourceFile = "$ReportDirectory\$SourceVersion\$EscapedProject$SourceVersion.DACPAC";
	$TargetFile = "$ReportDirectory\$TargetVersion\$EscapedProject$TargetVersion.DACPAC";
	if ($prefix -eq 'U')
	{ $OutputFile = "$ReportDirectory\$TargetVersion\$($prefix)_$($TargetVersion)__UndoMigration_to_$($SourceVersion).sql" }
	else
	{ $OutputFile = "$ReportDirectory\$SourceVersion\$($prefix)_$($SourceVersion)__ForwardMigration_from_$($TargetVersion).sql" };
	@($SourceFile, $TargetFile) | foreach {
		if (-not (Test-Path "$_")) { $WeCanDoIt = $false; $Feedback += "cannot find the dacpac file $_ " }
	}
	if ($OverWrite -eq $false -and (Test-Path "$OutputFile"))
         { $WeCanDoIt = $false; $Feedback += "Migration file $OutputFile already exists" }
	if ($WeCanDoIt) #if it has passed the tests
	{
    $Result=Export-ChangeScriptFromDACPACS `
         -SourceDacPac $sourceFile -TargetDacPac $TargetFile -OutputFilePath $OutputFile -OverWrite $overwrite -DatabaseName $param1.project
    $Result[0]#problems
    if ($Result[0].count -gt 0)
	{ $Param1.problems.'CreatePossibleMigrationScriptFromDacpac' = $Result[1] }
    if ($Result[1].count -gt 0)
	{ $Param1.feedback.'CreatePossibleMigrationScriptFromDacpac' = $Result[1] }
	if ($Result[2].count -gt 0)
	{$Param1.WriteLocations.'CreatePossibleMigrationScriptFromDacpac' = $Result[2];
		}
	}
	
}

$ExtractFromSQLServerIfNecessary = { <# 
     this connects to the SQL Server database and will then, For the current version of the database 
     extract either a 
       'DacPac'     (output a .dacpac single file). 
       'Flat'  (all files in a single folder),
       'SchemaObjectType' (files in folders for each schema and object type), 
       'Schema' (files in folders for each schema),
       'ObjectType'  (files in folders for each object type), 'Flat' (all files in the same folder)
       'File' (1 single file). #>
	Param ($param1,
		$OutputType = 'DacPac', <# {DacPac|File|Flat|ObjectType|Schema|SchemaObjectType} #>
		$RedoIt = $false, #by default you just do it the once
		$doDiagnostics = $false # do you want the log saved to disk to see what went wrong?
	) # $ExtractFromSQLServerIfNecessary (Don't delete this) 
	$problems = @(); # well, not yet (all problems are returned in this array)
	$feedback = @(); # well, nothing yet (all feedback is  returned in this array)
	if ($doDiagnostics -eq $null) { $doDiagnostics = $False }
	if ($OutputType -notin ('DacPac', 'File', 'Flat', 'ObjectType', 'Schema', 'SchemaObjectType'))
	{# just assume that if the output type is wrong, the user wanted a DACPAC.
		$OutputType = 'DACPAC'; # 'DacPac'
    }
	#check that we have values for the necessary details
	@('version', 'server', 'reportLocation', 'database', 'pwd', 'uid', 'project', 'projectDescription') |
	foreach{ if ([string]::IsNullOrEmpty($param1.$_)) { $Problems += "no value for '$($_)'" } }
	<# has the user installed SQLPackage? - or specified the path as a variable#>
    $command = get-command sqlpackage -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($SQLPackageAlias -ne $null)
		{ Set-Alias sqlpackage   $SQLPackageAlias }
		else
		{ $problems += 'You must have provided a path to $SQLPackage.exe in the ToolLocations.ps1 file in the resources folder' }
	} #the database scripts path would be up to you to define, of course
	$VersionsPath = if ([string]::IsNullOrEmpty($param1.VersionsPath)) { 'Versions' }
	else { "$($param1.VersionsPath)" }
	$EscapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
    <# The SqlPackage Extract action creates a schema of a connected database in a
     DACPAC file (.dacpac). By default, data is not included in the .dacpac file.
     To include data, utilize the Export action or use the Extract properties
     ExtractAllTableData/TableData.
     /p:ExtractTarget
     Specifies alternative output formats of the database schema, default is 'DacPac'
     to output a .dacpac single file. Additional options output one or more .sql files
     organized by either 'SchemaObjectType' (files in folders for each schema and
     object type), 'Schema' (files in folders for each schema), 'ObjectType'
     (files in folders for each object type), 'Flat' (all files in the same folder),
     or 'File' (1 single file).
      #>
	# objectType SchemaObjectType, schema and flat create a directory. File creates a file
	$ReportDirectory = "$($param1.reportLocation)\$($param1.Version)\";
	$OutputFile = "$ReportDirectory$($EscapedProject)$($param1.Version)-$OutputType.dacpac"
	$ExtractArguments = @("/Action:Extract", <#
         Specifies a source file to be used as the source of action instead of a
         database. For the Publish and Script actions, SourceFile may be a .dacpac
         file or a schema compare .scmp file. If this parameter is used, no other
         source parameter is valid. #>
		"/TargetFile:$Outputfile",
    <#  Specifies a target file to be used for the dacpac #>
        "/p:CommandTimeout=10", #Ten secondes timeout
        "/p:LongRunningCommandTimeout=1000",
		"/p:ExtractAllTableData=true",
		"/p:ExtractTarget=$OutputType",
		"/p:VerifyExtraction=true",
		"/p:DacApplicationDescription=$($param1.projectDescription)",
		"/p:DacApplicationName=$($param1.database)-[$($param1.version)]",
		"/SourcePassword:$($param1.pwd)",
    <#   For SQL Server Auth scenarios, defines the password to use to access the
         Source database. (short form /sp) #>
		"/SourceServerName:$($param1.Server)", <#
         Defines the name of the server hosting the source database. (short form
         /ssn) #>
		"/SourceDatabaseName:$($param1.database)", <#
         Specifies an override for the name of the database that is the source of
         SqlPackage.exe Action. (short form /sdn) #>
		"/SourceUser:$($param1.uid)"<#
         For SQL Server Auth scenarios, defines the SQL Server user to use to
         access the source database. (short form /su) #>
		"/SourceTrustServerCertificate:true"
	)
	if ($DoDiagnostics)
	{
		$ExtractArguments += `
		"/DiagnosticsFile:$ReportDirectory$($EscapedProject)$($param1.Version)$OutputType.log "
	}
    <# a bug in SQLPackage means that it always has a DACPAC filetype even when that isn't so.
    We calculate what the file name should be once we execute SQL Package and will change it!  #>
    $NewOutputType = switch ($OutputType)
	{
		'File' { 'SQL' }
		default { $OutputType }
	}
	$ChangedOutput = $outputFile.Replace("-$OutputType.dacpac", ".$NewOutputType")
    if ($ChangedOutput -eq $OutputFile){$Feedback+="$ChangedOutput is unchanged from $OutputFile"}
	$AlreadyDone = (Test-Path $ChangedOutput)
	if ($RedoIt -eq $false -and $AlreadyDone -eq $true)
	{
		$Feedback += "The $OutputType has already been created for $($EscapedProject) $($param1.Version)"
	}
	else
	{ <# deal with feedback #>
		
		if ($problems.Count -eq 0)
		{
			if (-not (Test-Path "$ReportDirectory" -PathType Container))
			{ New-Item -ItemType directory -Path "$ReportDirectory" -Force }
			else
			{
				if (Test-Path $OutputFile)
				{ Remove-item $OutputFile }
			}
			#Now we cactually execute SQL Package.
			$console = sqlpackage $ExtractArguments | foreach{ "$_ `n" }
			$Feedback += "$console"
			if ($?) # if no errors then simple message, otherwise...
			{
				if (Test-Path $ChangedOutput)
				{ Remove-item $ChangedOutput }
				
				rename-item $outputFile $ChangedOutput
				$Feedback += "Written $OutputType for $EscapedProject $($param1.Version) to $ChangedOutput"
			}
			else
			{
				#report a problem and send back the args for diagnosis (hint, only for script development)
				$Problems += "SQLpackage Went badly. (code $LASTEXITCODE) with paramaters $ExtractArguments"
			}
		}
	}
	if ($problems.count -gt 0)
	{ $Param1.Problems.'ExtractFromSQLServerIfNecessary' += $problems; }
	if ($feedback.count -gt 0)
	{ $Param1.feedback.'ExtractFromSQLServerIfNecessary' = $feedback }
	
}

<# By default, this will take the DACPAC for the current version of the database, and will 
   compare it with the previous version. it should, logically, work either way to do forward 
   or UNDO migrations. Sadly it doesn't. The file is processed to remove  any CREATE SCHEMA
   statements,SQLCMD statements and any SQLCMD parameter definitions #>
$CreatePossibleMigrationScriptFromDacpac = { 
	Param ($param1, # the dbDetails array where we pass all the info
		$OutputScriptType = 'Versioned',
        [bool]$OverWrite = $true
	) # $CreatePossibleMigrationScriptFromDacpac  (Don't delete this) 
	$problems = @(); # well, not yet
	$feedback = @()
	@('version', 'previous') |
		foreach{
			if ($param1.$_ -in @($null, ''))
			{
				Process-FlywayTasks $param1 $GetCurrentVersion;
			}
		}
		
	@('version', 'previous', 'project', 'reportLocation') | foreach{
		if ($param1.$_ -in @($null, '')) { $WeCanDoIt = $false; $Problems += "no value for '$($_)'" }
	}
	$EscapedProject = ($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	$ReportDirectory = "$($param1.reportLocation)\Versions";
	#default to undo
	$SourceVersion = $param1.previous
	$TargetVersion = $param1.version
	$prefix = 'U';
    if ($OutputScriptType -eq 'versioned')
	{
		$SourceVersion = $param1.version
		$TargetVersion = $param1.previous
        $prefix = 'V'
	}
	if ([version]$SourceVersion -gt [version]$TargetVersion) {$prefix = 'U'};
	$SourceFile = "$ReportDirectory\$SourceVersion\$EscapedProject$SourceVersion.DACPAC";
	$TargetFile = "$ReportDirectory\$TargetVersion\$EscapedProject$TargetVersion.DACPAC";
	if ($prefix -eq 'U')
	{ $OutputFile = "$ReportDirectory\$TargetVersion\$($prefix)_$($TargetVersion)__UndoMigration_to_$($SourceVersion).sql" }
	else
	{ $OutputFile = "$ReportDirectory\$SourceVersion\$($prefix)_$($SourceVersion)__ForwardMigration_from_$($TargetVersion).sql" };
	@($SourceFile, $TargetFile) | foreach {
		if (-not (Test-Path "$_")) { $WeCanDoIt = $false; $Feedback += "cannot find the dacpac file $_ " }
	}
	if ($OverWrite -eq $false -and (Test-Path "$OutputFile"))
         { $WeCanDoIt = $false; $Feedback += "Migration file $OutputFile already exists" }
	if ($WeCanDoIt) #if it has passed the tests
	{
    $Result=Export-ChangeScriptFromDACPACS `
         -SourceDacPac $sourceFile -TargetDacPac $TargetFile -OutputFilePath $OutputFile -OverWrite $overwrite -DatabaseName $param1.project
    if ($Result[0].count -gt 0)
	{ $Param1.problems.'CreatePossibleMigrationScriptFromDacpac' = $Result[1] }
    if ($Result[1].count -gt 0)
	{ $Param1.feedback.'CreatePossibleMigrationScriptFromDacpac' = $Result[1] }
	if ($Result[2].count -gt 0)
	{$Param1.WriteLocations.'CreatePossibleMigrationScriptFromDacpac' = $Result[2];
		}
	}
}

<# this creates a first-cut UNDO script for the metadata (not the data) which can
be adjusted and modified quickly to produce an UNDO Script. It does this by using
SQL Compare to generate a  idepotentic script comparing the database with the 
contents of the previous version.#>
$CreateUndoScriptIfNecessary = {
	Param ($param1) # $CreateUndoScriptIfNecessary (Don't delete this) 
	$problems = @(); # well, not yet
    $feedback= @(); # well, nothing yet
    $WeCanDoIt=$true; #assume that we can generate a script ....so far!
    #check that we have values for the necessary details
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
    $command=$null;
    $command = get-command SQLCompare -ErrorAction Ignore
    if ($command -eq $null) 
        {
    	if ($SQLCompareAlias-ne $null)
            {Set-Alias SQLCompare $SQLCompareAlias}
        else
            {$problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder'}
    }	#the database scripts path would be up to you to define, of course
    $scriptsPath= if ([string]::IsNullOrEmpty($param1.scriptsPath)) {'scripts'} else {"$($param1.scriptsPath)"}
    $sourcePath= if ([string]::IsNullOrEmpty($param1.sourcePath)) {'Source'} else {"$($param1.SourcePath)"}
    $EscapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
   #What was the previous version?
    if ($param1.Previous -in ($null,'0.0.0')) {
        $feedback += "no previous version to undo to"; 
        $null=New-Item -ItemType Directory -Force "$env:Temp\DummySource"
        $PreviousDatabasePath="$env:Temp\DummySource";
        }
    else
        {
        $PreviousDatabasePath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Previous)\$sourcePath"} 
        else {"$($param1.reportLocation)\$($param1.Previous)\$sourcePath"} #else the simple version

        If (!(Test-Path -path $PreviousDatabasePath -PathType Container)) 
            {$WeCanDoIt=$False} #Because no previous source
        } 
    $CurrentUndoPath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\$scriptsPath"} 
        else {"$($param1.reportLocation)\$($param1.Version)\$scriptsPath"} #else the simple version
    if (Test-Path -Path "$CurrentUndoPath\U$($Param1.Version)__Undo.sql" -PathType Leaf )
        {$WeCanDoIt=$False} #Because it has already been done             
    If ($WeCanDoIt)
	    {
        $CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
		    # can use a hash-table 
            "/Scripts1:$PreviousDatabasePath"
		    "/server2:$($param1.server)",
            '/include:identical',#a migration may just be data, no metadata.
		    "/database2:$($param1.database)",
		    "/force", # 
		    "/options:NoErrorHandling,IgnoreQuotedIdentifiersAndAnsiNullSettings,NoTransactions,DoNotOutputCommentHeader,ThrowOnFileParseFailed,ForceColumnOrder,IgnoreNoCheckAndWithNoCheck,IgnoreSquareBrackets,IgnoreWhiteSpace,ObjectExistenceChecks,IgnoreSystemNamedConstraintNames,IgnoreTSQLT,NoDeploymentLogging", 
# so that we can use the script with Flyway more easily
		    "/LogLevel:Warning",
		    "/ScriptFile:$CurrentUndoPath\U$($Param1.Version)__Undo.sql"
	    )
	
	    if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
	    {
		    $CLIArgs += @(
			    "/username2:$($param1.uid)",
			    "/Password2:$($param1.pwd)"
		    )
	    }
        $schemaAndName=$param1.flywayTable -split '\.'
         if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
		{
			$CLIArgs += @(
				"/filter:$($param1.filterpath)"
			)
         }
        else
            {
            $CLIArgs += @(
               "/exclude:table:$(( $schemaAndName[1],$schemaAndName[0] -ne $null)[0])")
		} 
	    if (-not (Test-Path -PathType Container $CurrentUndoPath))
	    {
		    # is the path to the scripts directory
		    # not there, so we create the directory 
		    $null = New-Item -ItemType Directory -Force $CurrentUndoPath;
	    }
		# if it is done already, then why bother? (delete it if you need a re-run for some reason 	
		Sqlcompare @CLIArgs #run SQL Compare with splatted arguments
		if ($LASTEXITCODE-eq 63) { "no changes to the metadata between ($param1.Previous) and this version $($param1.Version) of $($param1.Project)" }
		if ($?) { "Written build script for $($param1.Project) $($param1.Version) to $MyDatabasePath" }
		else # if no errors then simple message, otherwise....
		{
			#report a problem and send back the args for diagnosis (hint, only for script development)
			$Arguments = '';
			$Arguments += $CLIArgs | foreach{ $_ }
			$Problems += "SQLCompare Went badly. (code $LASTEXITCODE) with paramaters $Arguments"
		}
		if ($problems.count -gt 0)
		{ $Param1.Problems.'CreateUNDOScriptIfNecessary' += $problems; }
	    else
	    {
	    $Param1.WriteLocations.'CreateUNDOScriptIfNecessary' = "$CurrentUndoPath\U$($Param1.Version)__Undo.sql";
	    }

	}
	else {
          $feedback+= "This version '$($param1.Version)' already has a undo script to get to $($param1.Previous) at $CurrentUndoPath\U$($Param1.Version)__Undo.sql "
         }
	if ($feedback.count -gt 0)
		{$Param1.feedback.'CreateUNDOScriptIfNecessary' = $feedback}
	
}

<# this creates a first-cut migration script for the metadata (not the data) which can
be adjusted, documented  and modified quickly to produce an migration Script. It does this by using
SQL Compare to generate a  idepotentic script comparing contents of the current version of the database with the 
 live version. Do not do this within a Flyway Session (such as an 'afterEach')#>
$CreatePossibleMigrationScript = {
	Param ($param1) # $CreatePossibleMigrationScript (Don't delete this) 
	$problems = @(); # well, not yet
     #check that we have values for the necessary details
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null,'')) { $Problems += "no value for '$($_)'" } }
	# the alias must be set to the path of your installed version of SQL Compare
    $command=$null;
    $command = get-command SQLCompare -ErrorAction Ignore 
    if ($command -eq $null) {
    	if ($SQLCompareAlias-ne $null)
            {Set-Alias SQLCompare $SQLCompareAlias -Scope Script}
        else
            {$problems += 'You must have provided a path to SQL Compare in the ToolLocations.ps1 file in the resources folder'}
    }	#the database scripts path would be up to you to define, of course
    $scriptsPath= if ([string]::IsNullOrEmpty($param1.scriptsPath)) {'scripts'} else {"$($param1.scriptsPath)"}
    $sourcePath= if ([string]::IsNullOrEmpty($param1.sourcePath)) {'Source'} else {"$($param1.SourcePath)"}
    $EscapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
    $CurrentVersionPath = 
        if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
          {"$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)"} 
        else {"$($param1.reportLocation)\$($param1.Version)"} #else the simple version
 	
    $CLIArgs = @(# we create an array in order to splat the parameters. With many command-line apps you
		# can use a hash-table 
        "/Scripts2:$CurrentVersionPath\$SourcePath"
		"/server1:$($param1.server)",
        '/include:identical',#a migration may just be data, no metadata.
		"/database1:$($param1.database)",
        "/report:$CurrentVersionPath\Drift.xml",
        "/reportType:XML"
		"/force", # 
		"/options:NoErrorHandling,IgnoreExtendedProperties,IgnoreQuotedIdentifiersAndAnsiNullSettings,NoTransactions,DoNotOutputCommentHeader,ThrowOnFileParseFailed,ForceColumnOrder,IgnoreNoCheckAndWithNoCheck,IgnoreSquareBrackets,IgnoreWhiteSpace,ObjectExistenceChecks,IgnoreSystemNamedConstraintNames,IgnoreTSQLT,NoDeploymentLogging", 
# so that we can use the script with Flyway more easily
		"/LogLevel:Warning",
		"/ScriptFile:$CurrentVersionPath\$scriptsPath\MigrationFrom$($param1.Version)ToNextVersion.sql"
	)
	
	if ($param1.uid -ne $NULL) #add the arguments for credentials where necessary
	{
		$CLIArgs += @(
			"/username1:$($param1.uid)",
			"/Password1:$($param1.pwd)"
		)
	}
    $schemaAndName=$param1.flywayTable -split '\.';
    if ($param1.'filterpath' -ne $NULL) #add the arguments for compare filters
	{
		$CLIArgs += @(
			"/filter:$($param1.filterpath)"
		)
        }
    else
        {
        $CLIArgs += @(
             "/exclude:table:$(( $schemaAndName[1],$schemaAndName[0] -ne $null)[0])")
	} 
	if (-not (Test-Path -PathType Container $CurrentVersionPath))
	{
		# is the path to the scripts directory
		# not there, so we create the directory 
		$null = New-Item -ItemType Directory -Force $CurrentVersionPath;
	}
	# if it is done already, then why bother? (delete it if you need a re-run for some reason 	
	Sqlcompare @CLIArgs #run SQL Compare with splatted arguments
	if ($LASTEXITCODE-eq 63) {$param1.feedback.'CreatePossibleMigrationScript'= "There have been no changes to version $($param1.Version) of $($param1.Project)" }
	if ($?) { "Written build script for $($param1.Project) $($param1.Version) to $MyDatabasePath" }
	else # if no errors then simple message, otherwise....
	{
		#report a problem and send back the args for diagnosis (hint, only for script development)
		$Arguments = '';
		$Arguments += $CLIArgs | foreach{ $_ }
		$Problems += "SQLCompare Went badly. (code $LASTEXITCODE) with paramaters $Arguments"
	}
	if ($problems.count -gt 0)
	{ $Param1.Problems.'CreatePossibleMigrationScript' += $problems; }
	else
	{
	$Param1.WriteLocations.'CreatePossibleMigrationScript' = "$CurrentVersionPath\$scriptsPath\MigrationFrom$($param1.Version)ToNextVersion.sql";
    $Param1.feedback.'CreatePossibleMigrationScript'="A migration file has been created from the present version ($($param1.Version))" 
	}
	
}


<#
This script performs a bulk copy operation to get data into a database. It
can only do this if the data is in a suitable directory. At the moment it assumes
that you are using a DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path 
Unlike many other tasks, you are unlikely to want to do this more than once for any
database.If you did, you'd need to clear out the existing data first! It is intended
for static scripts AKA baseline migrations.
#>
$BulkCopyIn = {
	Param ($param1) # $BulkCopyIn (Don't delete this) 
	$problems = @(); # well, not yet
	$WeCanDoIt = $true; #assume that we can BCP data in.so far!
	#check that we have values for the necessary details
	@('server', 'database', 'project', 'version') |
	foreach{ if ($param1.$_ -in @($null, '')) { $Problems += "no value for '$($_)'" } }
    if ([string]::IsNullOrEmpty($param1.dataPath)) #has he specified a datapath
            {$FilePath = '..\Data'}
	    else
            {$FilePath = "..\$($param1.datapath)"};
	
	#Now finished getting credentials. Is the data directory there
	if (!(Test-Path -path $Filepath -PathType Container))
	{
		$Problems += 'No appropriate directory with bulk files yet';
		$weCanDoIt = $false;
	}
	if ($weCanDoIt)
	{
		#now we know the version we get a list of the tables.
		$Tables = $GetdataFromSQLCMD.Invoke($Param1, @"
SELECT Object_Schema_Name (object_id) AS [Schema], name
     FROM sys.tables
     WHERE
     is_ms_shipped = 0 AND name NOT LIKE 'Flyway%'
  FOR JSON AUTO
"@) | ConvertFrom-Json
		if ($Tables.Error -ne $null)
		{
			$internalLog += $Tables.Error;
			$weCanDoIt = $false;
		}
	}
	
	$Result = $GetdataFromSQLCMD.Invoke($Param1, @'
    EXEC sp_MSforeachtable "ALTER TABLE ? NOCHECK CONSTRAINT all"
'@);
	if ($Result.Error -ne $null)
	{
		$internalLog += $Tables.Error;
		$weCanDoIt = $false;
	}
	
	if ($weCanDoIt)
	{
		$directory = "$Filepath\$($version.Split([IO.Path]::GetInvalidFileNameChars()) -join '_')";
		$Tables |
		foreach {
			# calculate where it gotten from #
			$filename = "$($_.Schema)_$($_.Name)".Split([IO.Path]::GetInvalidFileNameChars()) -join '_';
			$progress = '';
			Write-Verbose "Reading in $filename from $($directory)\$filename.bcp"
			if ($User -ne '') #using standard credentials 
			{
				$Progress = BCP "$($_.Schema).$($_.Name)" in "$directory\$filename.bcp" -q -n -E `
								"-U$($user)"  "-P$password" "-d$($Database)" "-S$server"
			}
			else #using windows authentication
			{
				#-E Specifies that identity value or values in the imported data are to be used
				$Progress = BCP "$($_.Schema).$($_.Name)" in "$directory\$filename.bcp" -q -n -E `
								"-d$($Database)" "-S$server"
			}
			if (-not ($?) -or $Progress -like '*Error*') # if there was an error
			{
				$Problems += "Error with data import  of $($directory)\$($_.Schema)_$($_.Name).bcp -  $Progress ";
			}
		}
	}
	$Result = $GetdataFromSQLCMD.Invoke($Param1, @'
ALTER TABLE ? WITH CHECK CHECK CONSTRAINT all
'@)
	if ($problems.count -gt 0)
	{ $Param1.Problems.'BulkCopyIn' += $problems; }
	
}

<#
This script performs a bulk copy operation to get data out of a database, and
into a suitable directory. At the moment it assumes that you wish to use a 
DATA directory at the same level as the scripts directory. 
BCP must have been previously installed in the path.
#>
$BulkCopyOut = {
	Param ($param1) # $BulkCopyOut (Don't delete this) 
	$problems = @(); # well, not yet
	$WeCanDoIt = $true; #assume that we can BCP data in.so far!
	#check that we have values for the necessary details
	@('version', 'server', 'database', 'project') |
	foreach{ if ($param1.$_ -in @($null, '')) { $Problems += "no value for '$($_)'" } }
	if ([string]::IsNullOrEmpty($param1.dataPath)) #has he specified a datapath
        {$FilePath = '..\Data'}
	else
        {$FilePath = "..\$($param1.datapath)"}
	if (!(Test-Path -path $Filepath -PathType Container))
	{
		$Null = New-Item -ItemType Directory -Path $FilePath -Force
	}
	
	#now we know the version we get a list of the tables.
	$Tables = $GetdataFromSQLCMD.Invoke($param1, @"
SELECT Object_Schema_Name (object_id) AS [Schema], name
     FROM sys.tables
     WHERE
     is_ms_shipped = 0 AND name NOT LIKE 'Flyway%'
  FOR JSON AUTO
"@) | ConvertFrom-Json
	if ($Tables.Error -ne $null)
	{
		$internalLog += $Tables.Error;
		$WeCanDoIt = $false;
	}
	if ($WeCanDoIt)
	{
		$directory = "$Filepath\$($version.Split([IO.Path]::GetInvalidFileNameChars()) -join '_')";
        if (-not (Test-Path "$directory" -PathType Container))
        { New-Item -ItemType directory -Path "$directory" -Force}        
		$Tables |
		foreach {
			# calculate where it gotten from #
			$filename = "$($_.Schema)_$($_.Name)".Split([IO.Path]::GetInvalidFileNameChars()) -join '_';
			$progress = '';
			Write-Verbose "writing out $filename to  $($directory)\$filename.bcp"
			if ($User -ne '') #using standard credentials 
			{
				$Progress = BCP "$($_.Schema).$($_.Name)"  out  "$directory\$filename.bcp"  `
								-n "-d$($Database)"  "-S$($server)"  `
								"-U$user" "-P$password"
			}
			else #using windows authentication
			{
				#-E Specifies that identity value or values in the imported data are to be used
				
				$Progress = BCP "$($_.Schema).$($_.Name)" in "$directory\$filename.bcp" -q -n -E `
								"-d$($Database)" "-S$server"
				
			}
			if (-not ($?) -or $Progress -like '*Error*') # if there was an error
			{
				$Problems += "Error with data export  of $($directory)\$($_.Schema)_$($_.Name).bcp -  $Progress ";
			}
		}
	}
	if ($problems.count -gt 0)
	{ $Param1.Problems.'BulkCopyOut' += $problems; }


}


<#
This script creates a PUML file for a Gantt chart at the current version of the database. This can be
read into any editor that takes PlantUML files to give a Gantt chart
#>
$GeneratePUMLforGanttChart = {
	Param ($param1) # $GeneratePUMLforGanttChart (Don't delete this) 
	$problems = @(); #no problems so far
	#check that you have the  entries that we need in the parameter table.
	@('server', 'database', 'project', 'uid') | foreach{
		if ([string]::IsNullOrEmpty($Param1.$_))
		{ $Problems += "no value for '$($_)'" }
	}
    $EscapedProject=($Param1.project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.','-'
	$MyDatabasePath =
	if  ($param1.directoryStructure -in ('classic',$null)) #If the $ReportDirectory has a value
	{ "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Version)\Reports" }
	else { "$($param1.reportLocation)\$($param1.Version)\Reports" } #else the simple version
	$flywayTable = $Param1.flywayTable
	if ($flywayTable -eq $null)
	{ $flywayTable = 'dbo.flyway_schema_history' }
	if (-not (Test-Path -PathType Container $MyDatabasePath))
	{
		# does the path to the reports directory exist?
		# not there, so we create the directory 
		$null = New-Item -ItemType Container -Force $MyDatabasePath;
	}
    $Puml=''
	if ($problems.Count -eq 0)
	{
		$puml = $GetdataFromSQLCMD.Invoke($param1, @"
/* we read the Flyway Schema History into a table variable so we can then do a line--by-line select with 
a guarantee of doing it in the order of the primary key */
set nocount on
DECLARE @FlywaySchemaTable TABLE
   ([installed_rank] [INT] NOT NULL PRIMARY KEY,
   [version] [NVARCHAR](50) NULL,
   [description] [NVARCHAR](200) NULL,
   [installed_by] [NVARCHAR](100) NOT NULL,
   [installed_on] [DATETIME] NOT NULL)
/* now read in the table */
INSERT INTO @FlywaySchemaTable
  (Installed_rank, version,  
   installed_by, installed_on, description )
   --I've added the placeholders in case you want to execute this in a callback
SELECT fsh.installed_rank, version, installed_by, installed_on, description
  FROM $flywayTable FSH 
    INNER JOIN
      (SELECT  Max (installed_rank) AS installed_rank
         FROM $flywayTable 
         WHERE
         success = 1 AND type = 'SQL' AND version IS NOT NULL
         GROUP BY version) f
      ON f.installed_rank = fSH.installed_rank
  ORDER BY fSH.installed_rank;

/* now we calculate the version. This is slightly complicated by the
possibility that you've done an UNDO. I've added the placeholders
in case you want to execute this in a callback */
DECLARE @Version [NVARCHAR](50) =
    (SELECT TOP 1 [version] --we need to find the greatest successful version.
        FROM $flywayTable -- 
        WHERE
        installed_rank =
        (SELECT Max (installed_rank)
            FROM $flywayTable 
            WHERE success = 1));

DECLARE @PlantUMLCode NVARCHAR(MAX)='@startgantt
skinparam LegendBorderRoundCorner 2
skinparam LegendBorderThickness 1
skinparam LegendBorderColor silver
skinparam LegendBackgroundColor white
skinparam LegendFontSize 11
printscale weekly
saturday are closed
sunday are closed
title Gantt Chart for version '+@Version+'
legend top left
  Database: '+Db_Name()+'
  Server: '+@@ServerName+'
  RDBMS: sqlserver
  Flyway Version: '+@Version+'
endlegend
printscale weekly
saturday are closed
sunday are closed
'
DECLARE @PreviousDescription NVARCHAR(100) 
--used to temporarily hold the previous description

SELECT @PlantUMLCode=@PlantUMLCode + 
  CASE WHEN @PreviousDescription IS NULL THEN 'Project starts '+Convert(NCHAR(11),Convert(DATETIME2,Installed_on,112)) +'
' ELSE '' END+
'['+version+' - '+description+'] on {'+[installed_by]+'} starts '+ Convert(NCHAR(11),Convert(DATETIME2,Installed_on,112))+'
' 
+ CASE WHEN @PreviousDescription IS NOT NULL THEN '['+@Previousdescription+'] ends '+Convert(NCHAR(11),Convert(DATETIME2,Installed_on,112))+'
' ELSE '' END,
      @PreviousDescription = version+' - '+description
FROM @FlywaySchemaTable WHERE version IS NOT null
SELECT @PlantUMLCode=@PlantUMLCode+'@endgantt'
SELECT @PlantUMLCode

"@, $null, $true)
		
		[IO.File]::WriteAllLines("$MyDatabasePath\GanttChart.puml", $puml) # It must be UTF8!!!
	}
if ($problems.count -gt 0)
	{ $Param1.Problems.'GeneratePUMLforGanttChart' += $problems; 
	}
	else
	{
	$Param1.WriteLocations.'GeneratePUMLforGanttChart' = "$MyDatabasePath\GanttChart.puml";
	}

}
<# create a markdown report of what happened in the last migration  
$param1=$param1
#>

$CreateVersionNarrativeIfNecessary = {
	Param ($param1) # $CreateVersionNarrativeIfNecessary - dont delete this
	$problems = @() #none yet!
    $warnings =@()
    $feedback =@()
    $unnecessary=$false;
    $AlreadyDone=$false;
    $GoodVersion=$true;
    #check that you have the  entries that we need in the parameter table.
    try
        {
	    @( 'version', 'previous', 'project') | foreach{
		    if ([string]::IsNullOrEmpty($param1.$_))
		    { $Problems += "no value for '$($_)'" }
	    }
	#calculate the report paths
	    $escapedProject = ($Param1.Project.Split([IO.Path]::GetInvalidFileNameChars()) -join '_') -ireplace '\.', '-'
	
	    if ($param1.directoryStructure -in ('classic', $null)) #If the $ReportDirectory has a value
	    {
		    $PreviousVersionReportPath =
		    "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.Previous)\Reports"
		    $currentVersionReportPath =
		    "$($env:USERPROFILE)\$($param1.Reportdirectory)$($escapedProject)\$($param1.version)\Reports"
	    }
	    else
	    {
		    $PreviousVersionReportPath = "$($param1.reportLocation)\$($param1.previous)\Reports"
		    $currentVersionReportPath = "$($param1.reportLocation)\$($param1.Version)\Reports"
		
	    }
        if (-not (Test-Path "$PreviousVersionReportPath" -PathType Container))
            { New-Item -ItemType directory -Path "$PreviousVersionReportPath" -Force}
        if (-not (Test-Path "$currentVersionReportPath" -PathType Container))
            { New-Item -ItemType directory -Path "$currentVersionReportPath" -Force}
	}
	catch
	{
		$problems += "$($PSItem.Exception.Message)"
	}

	if ($param1.Version -eq '0.0.0') { $warnings += "Cannot compare an empty database with anything" }
	$GoodVersion = try { $null = [Version]$param1.Version; $true }
	catch { $false }
	if (-not ($goodVersion))
	{ $problems += "Bad version number '$($param1.Version)'" }
	
	if ($goodVersion)
	{
		if ($param1.previous -eq '0.0.0') {
          $feedback += "Cannot compare with an empty database";
          $unnecessary=$true
           }
		$GoodVersion = try { $null = [Version]$param1.previous; $true }
		catch { $false }
		if (-not ($goodVersion))
		{ $problems += "Bad previous version number '$($param1.Version)'" }
	}
	
	if ($goodVersion)
	{
		If (Test-Path -Path "$currentVersionReportPath\VersionNarrative.MD")
		{
			$AlreadyDone = $true
		}
		If (!(Test-Path -Path "$PreviousVersionReportPath\DatabaseModel.JSON"))
		{
			$feedback += "No database model exists for '$($param1.Previous)'"
            $unnecessary=$true;
		}
		If (!(Test-Path -Path "$currentVersionReportPath\DatabaseModel.JSON"))
		{
			$warnings += "No database model exists for '$($param1.version)'"
		}
		if ($problems.Count -eq 0 -and $warnings.count  -eq 0 -and !$unnecessary -and !$AlreadyDone)
		{
			try
			{
				#see what is changed by comparing the models before and after the migration
				$Comparison = Diff-Objects -Parent $Details.Database  -depth 10 <#get the previous model from file#>`
				([IO.File]::ReadAllText("$PreviousVersionReportPath\DatabaseModel.JSON") |
					ConvertFrom-JSON)<#and get the current model from file#> `
				([IO.File]::ReadAllText("$currentVersionReportPath\DatabaseModel.JSON") |
					ConvertFrom-JSON) |
				where { $_.match -ne '==' }
				$Comparison | Export-CSV -Path "$currentVersionReportPath\MetadataChanges.report"
                $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';
				($Comparison | ConvertTo-JSON) > "$currentVersionReportPath\MetadataChanges.json"
				#we can do all sorts of more intutive reports from the output
                $ObjectName='nomatch';
                $narrative=@()
                $Narrative+="### Alterations to project $($param1.projectName), database $($param1.database), in version $($param1.version)`n"
				$narrative+=$Comparison | foreach{
					$current = $_;
                    $objectList=$current.Ref.split( '.')
                    switch ($current.Match)
					{
						'->' {
							  if ($current.Target -ne '(PSCustomObject)')
                               {
                                   if ($current.ref -like "$($ObjectName)*")
								    { "  - Added $($current.Target) to $($current.Ref -replace $ObjectName,'' )`n " }
                                    else
                                    {"- Added '$($current.Target -replace $ObjectName,'' )' to '$($current.Ref)'`n ";
                                      $ObjectName='nomatch';
                                    }
                                }
							else {
                                     "- Added  '$($current.Ref)'`n";
                                     $objectName=$current.Ref;
						         }
                            }
						'<>' {
							"- Changed '$($current.Source)' to '$($current.Target
							)' at '$($current.Ref)`n"
						}
						'<-' {
                             if ($current.Source -eq '(PSCustomObject)') 
							{"- Deleted  '$($current.Ref
							) `n"}
                            
						}
						default { "No metadata change `n" }
					}
                }
            $PSDefaultParameterValues['Out-File:Encoding'] = 'utf8';    					
			$Narrative > "$currentVersionReportPath\VersionNarrative.MD"

			}
			catch
			{
				$problems += "$($PSItem.Exception.Message)"
			}
		}
		if ($problems.Count -gt 0)
		{
			$Param1.Problems.'CreateVersionNarrativeIfNecessary' += $problems;
		}
		elseif ($warnings.Count -gt 0)
		{
			$Param1.warnings.'CreateVersionNarrativeIfNecessary' += $warnings;
		}
		else
		{
			if ($AlreadyDone)
			{
				$Param1.Feedback.'CreateVersionNarrativeIfNecessary' = "$Version narrative for $($param1.version) already exists"
			}
			if ($unnecessary)
			{
				$Param1.Feedback.'CreateVersionNarrativeIfNecessary' += "$Version narrative for $($param1.version) isn't necessary"
			}
			else
			{
				$Param1.WriteLocations.'CreateVersionNarrativeIfNecessary' = "$currentVersionReportPath\VersionNarrative.MD";
			}
		}
	}
}

<#
This creates a simple entity diagram for the current version. You only need two files to do this and
you don't need to contact the database. The ER diagram has all objects that are either added, removed or
changed colour-coded so you can see immediately what has changed. The idea of this is to be able to paste
the resulting SVG file or other image file of the diagram, produced by PlantUMLc.exe.
#>

$WriteOutERDiagramCode = {
	Param ($param1,
		$version = $Null,
		#the flyway version of the database. Leave null if using framework
		$Title = $null,
        #The shared directory where all the files are read or written to
        $FileLocations = $null,
		#the flyway project. Leave null if using framework
		$MetadatachangeFile = $null,
		#Specify if not using the default location
		$modelFile = $null,
		#Specify if not using the default location
		$MyPUMLFile = $null) # $WriteOutERDiagramCode - dont delete this
	
	
	$PSDefaultParameterValues['Out-File:Encoding'] = 'utf8' #we'll be using out redirection
	$problems = @() #none yet!
	$feedback = @();
	if ($version -eq $null)
	{ $version = $param1.version };
	#the flyway version of the database. Leave null if using framework
	if ($Title -eq $null)
	{ $Title = "$($Param1.project) database: $($Param1.Database) $version $($Param1.branch) branch" };
	#the flyway project. Leave null if using framework
    if ($FileLocations -eq $null) # just in case you wish to specify a shard location
	{ $FileLocations = "$($Param1.reportLocation)\$version\Reports" };
	if ($MetadatachangeFile -eq $null)
	{ $MetadatachangeFile = "$FileLocations\MetadataChanges.json" }
	if ($modelFile -eq $null)
	{ $modelFile = "$FileLocations\DatabaseModel.JSON" }
	if ($MyPUMLFile -eq $null)
	{ $MyPUMLFile = "$FileLocations\ERDiagram.PUML" }
	
	# variables to colour changed, added (or deleted database objects)
	$deleted = '#pink ##[bold]red'
	$added = '#lightgreen ##[bold]green'
	$altered = '#gold ##[bold]saddlebrown'
	
	# check that the files exist
	@($MetadatachangeFile, $modelFile) | foreach{
		If (!(Test-Path -PathType Leaf -Path $_))
		{
			$problems += "$_ does not exist. You'll need this for this diagram"
		}
	}
	
	if ($problems.Count -eq 0)
	{
<# First, get the change file and convert it to a PoSh object#>
		$changes = ConvertFrom-json ([IO.File]::ReadAllText($MetadatachangeFile))
		if ($changes -ne $null)
		{
			$ChangedEntities = $changes | foreach{ @{ 'object' = $_.Ref.Split('.'); 'match' = $_.match } }
			$ChangedObjects = @{ }
			$ChangedEntities | foreach {
				"$($_.object[1]).$($_.object[3])" } | sort -Unique | foreach{ $ChangedObjects."$($_)" = '--' }
			$ChangedEntities | foreach {
				@{ "$($_.object[1]).$($_.object[3])" = $_.match } } |
			foreach{
				$which = $_;
				$change = [string]$which.Values[0]
				$objectName = [string]$which.Keys[0]
				$ChangedObjects."$objectName" =
				switch ($ChangedObjects."$objectName")
				{
					'--' { "$change" }
					'<-' { '<' + $change[1] }
					'->' { $change[0] + '>' }
					default { '<>' }
				}
			}
		}
		
<# Get the model and convert it into a PlantUML script #>
		$Model = ConvertFrom-json ([IO.File]::ReadAllText($modelFile))
		$EntityCode = $model | Display-object -depth 3 |
		foreach{
			#split into the individual objects
			$bits = $_.Path.split('.');
			$objectName = "$($Bits[1]).$($bits[3])"
			$ObjecType = "$($Bits[2].replace(' ', '_'))"
			[pscustomobject]@{
				'ObjectName' = $objectName; 'ObjectType' = $ObjecType;
				'change' = if ($changes -ne $null)
				{
					$TheObjectName = "$($Bits[1]).$($bits[3])"
					if ($Changedobjects.$TheObjectName -ne $null)
					{
						switch ($Changedobjects.$TheObjectName)
						{
							'->' { $added } '<-' { $deleted }  '<>' { $altered }
							default { '?' }
						}
					}
				}
				else { '' }
			}
		} | foreach {
			#finally write them all out
			" $($_.ObjectType)($($_.ObjectName))  $($_.change)"
		}
		
		#Now add in any completely deleted objects that wouldn't be in the current model
		if ($changes.count -gt 0)
		{
			$ObjectPaths = $model | Display-object -depth 3 | foreach{ "$($_.path)" }
			$changes |
			foreach{ $Bits = $_.ref.split('.'); "`$.$($bits[1] + '.' + $bits[2] + '.' + $bits[3])" } |
			select -unique | where { $_ -notin $ObjectPaths } | foreach {
				#finally write them all out
				$EntityCode += " $($bits[2])($($bits[1] + $bits[3]))  $deleted"
			}
		}
		
		#now print out the 'hard entities'.
		$EntityRelations = $model | Display-object -depth 10 |
		where{ $_.path -like '$.*.*.*.foreign key.*.Foreign Table' } |
		foreach{
			$bits = $_.Path.split('.');
			[pscustomobject]@{
				'TableSchema' = "$($Bits[1])";
				'TableName' = "$($bits[3])";
				'Key' = "$($bits[5])";
				'ReferenceSchema' = "$($_.Value)".Split('.')[0];
				'ReferenceTable' = "$($_.Value)".Split('.')[1]
			}
		} | foreach {
			"$($_.TableSchema).$($_.TableName)  }|..|| $($_.ReferenceSchema).$($_.ReferenceTable)   "
			
		}
<# now print out the plantuml header and the code. #>
		$pumlCode = @"
@startuml
skinparam class {
BackgroundColor WhiteSmoke
ArrowColor black
BorderColor gray
}
skinparam wrapWidth 150
skinparam handwritten true
skinparam monochrome false
skinparam packageStyle rect
skinparam defaultFontName Buxton Sketch
skinparam shadowing true
skinparam MessageAlign left
skinparam header{
  FontColor black
  FontSize 14
}
skinparam footer{
  FontColor black
  FontSize 10
}
left to right direction

Title $title
header Date: $(((get-date).Date).ToString().replace(' 00:00:00', '')
		)
footer <back:pink>Pink background</back> means deleted <back:lightgreen> green Background</back>means added and <back:gold>Gold background</back> means altered.

!define table(x) class x << (T,mistyrose) >>
!define user_table(x) class x << (T,mistyrose) >>
!define view(x) class x << (V,lightblue) >>
!define dml_trigger(x) class x << (R,red) >>
!define table_valued_function(x) class x << (F,darkorange) >>
!define aggregate_function(x) class x << (F,white) >>
!define scalar_function(x) class x << (F,plum) >>
!define clr_scalar_function(x) class x << (F,tan) >>
!define clr_table_valued_function(x) class x << (F,wheat) >>
!define inline_table_valued_function(x) class x << (F,gaisboro) >>
!define stored_procedure(x) class x << (P,indianred) >>
!define clr_stored_procedure(x) class x << (P,lemonshiffon) >>
!define extended_stored_procedure(x) class x << (P,linen) >>


$($EntityCode -join "`r`n") 
$($EntityRelations -join "`r`n  ")

@enduml
"@
		$pumlCode > "$MyPUMLFile"
	} #if parameters were OK
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'WriteOutERDiagramCode' += $problems;
	}
	else
	{
		$Param1.WriteLocations.'WriteOutERDiagramCode' = $MyPUMLFile;
	}
}



$SaveFlywaySchemaHistoryIfNecessary = {
	Param ($param1) # $SaveFlywaySchemaHistoryIfNecessary - dont delete this
	$problems = @() #none yet!
	$warnings = @()
	$feedback = @()
	$WriteLocations = @()
	$unnecessary = $false;
	
	$info = Flyway info -outputType=json # collect Flyway's info as a JSON file
	if ($info.error -ne $null) { $Problems += $info.error.message }
	else
	{
		$InfoObject = $info | convertFrom-json #convert it to something Powershell can read
		$SaveAsFile = "$($param1.Reportlocation)\current\Migrationinfo.json"
		#where we store the reports. We get this from the framework from $param1
		$Report = $InfoObject.psobject.Properties | where { $_.name -ne 'migrations' } |
		foreach{ @{ $_.name = $_.value } } #Get all the base info 
		#add to this, the extra info that we collect from the framework
		$Report += @('Branch', 'Variant', 'Server', 'Database', 'InstalledBy', 'previous') | foreach{
			@{ $_ = "$($param1.$_)" }
		}
		#save it to the report for the current migration
		$Report | convertTo-json > $SaveAsFile
		# now we get the migration collection that came from RFlkyway Info
		$infoObject.migrations | where {$_.state -eq 'success'}| foreach{
			if ($_.version -eq '') # it is the initial state before a migration is applied
			{
				# Write this to the base of the report location just once
				$SaveAsFile = "$($param1.Reportlocation)\Creationinfo.json"
				if (!(Test-Path $SaveAsFile -PathType leaf))
				{
					convertTo-json $_ > $SaveAsFile;
					$WriteLocations += $SaveAsFile;
				}
				else
				{ $feedback += "$SaveAsFile with the database creation info already existed" }
			}
			else #it must be a valid migration version
			{
				#write it to the current version report folder just once
                if (!(Test-Path "$($param1.Reportlocation)\$($_.Version)" -PathType Container))
                    {New-Item -ItemType directory -Path "$($param1.Reportlocation)\$($_.Version)"}
                if (!(Test-Path "$($param1.Reportlocation)\$($_.Version)\reports" -PathType Container))
                    {New-Item -ItemType directory -Path "$($param1.Reportlocation)\$($_.Version)\reports"}
				$SaveAsFile = "$($param1.Reportlocation)\$($_.Version)\reports\ApplyInfo.json"
				if (!(Test-Path $SaveAsFile -PathType leaf))
				{
					convertTo-json $_ > $SaveAsFile;
					$WriteLocations += $SaveAsFile;
				}
				else
				{ $feedback += "$SaveAsFile already existed" }
			}
		}
	}
	if ($WriteLocations.Count -gt 0)
	{
		$Param1.WriteLocations.'SaveFlywaySchemaHistoryIfNecessary' = $writeLocations
	};
	if ($feedback.Count -gt 0)
	{
		$Param1.Feedback.'SaveFlywaySchemaHistoryIfNecessary' = $feedback
	};
	if ($problems.Count -gt 0)
	{
		$Param1.Problems.'SaveFlywaySchemaHistoryIfNecessary' += $problems;
	}
	if ($warnings.Count -gt 0)
	{
		$Param1.warnings.'SaveFlywaySchemaHistoryIfNecessary' += $warnings;
	}
}


Function GetorSetPassword{
[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[string]$uid,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[string]$Server,
		[Parameter(Mandatory = $false,
				   Position = 3)]
		[string]$RDBMS =$null) #change to your  database system if you have two on the one server!



    $PwdDetails= @{
        'RDBMS'=$RDBMS; #jdbc name. Only necessary for systems with several RDBMS on the same server
	    'Server'=$server;
        'pwd' = ''; #Always leave blank
	    'uid' = $uid; #leave blank unless you use credentials
	    'problems' = @{ }; # for reporting any big problems
         }
 
    $FetchAnyRequiredPasswords.Invoke($PwdDetails);
    if ($PwdDetails.Problems.FetchAnyRequiredPasswords.Count -gt 0)
         {Write-error "$($PwdDetails.Problems.FetchAnyRequiredPasswords)" }
    $PwdDetails.pwd
}


<#
	.SYNOPSIS
		executes a list of scriptblock tasks
	
	.PARAMETER DatabaseDetails
		a hashtable with the Flyway database details
	
	.PARAMETER Invocations
		the list of scriptblocks to execute
	
	.PARAMETER AddedParameters
		Any Additional parameters to the scriptblock(s) other than the hashtable
	

#>
function Process-FlywayTasks
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[array]$Invocations,
		[Parameter(Position = 3)]
		[array]$AddedParameters = $null
	)
	$parameters = @()
	$parameters += $DatabaseDetails;
	if ($AddedParameters -ne $null) { $parameters += $AddedParameters }
	$Invocations | foreach{
		if ($_ -eq $null)
		{
			Write-error "the scriptblock wasn't recognised"
		}
		elseif ($DatabaseDetails.Problems.Count -eq 0)
		{
			#try to get the name of the task
			if ($_.Ast.Extent.Text -imatch '(?<=# ?\$)([\w]{5,80})')
			{ $TaskName = $matches[0] }
			else { $TaskName = 'unknown' }
			try # try executing the script block
			{ $returnedArray = $_.Invoke($parameters) }
			catch #if it hit an exception
			{
				# handle the exception
				$where = $PSItem.InvocationInfo.PositionMessage
				$ErrorMessage = $_.Exception.Message
				$FailedItem = $_.Exception.ItemName
				$DatabaseDetails.Problems.exceptions += "$Taskname failed with $FailedItem : $ErrorMessage at `n $where."
			}
			"Executed $TaskName"
		}
		
	}
	#print out any errors and warnings. 
	if ($DatabaseDetails.Problems.Count -gt 0) #list out every problem and which task failed
	{
		$DatabaseDetails.Problems.GetEnumerator() |
		Foreach{ Write-warning "Problem! $($_.Key)---------"; $_.Value } |
		foreach { write-warning "`t$_" }
		$DatabaseDetails.Problems = @{ }
		
	}
	if ($DatabaseDetails.Warnings.Count -gt 0) #list out exery warning and which task failed
	{
		$DatabaseDetails.Warnings.GetEnumerator() |
		Foreach{ Write-warning "$($_.Key)---------"; $_.Value } |
		foreach { write-warning  "`t$_" }
		$DatabaseDetails.Warnings = @{ }
	}
	
	$DatabaseDetails.WriteLocations.GetEnumerator() |
	Foreach{
		Write-Output "For the $($_.Key), we saved the report in $($_.Value)"
		$DatabaseDetails.WriteLocations = @{ }
	}
	
	$DatabaseDetails.feedback.GetEnumerator() |
	Foreach{
		Write-Output "in $($_.Key), $($_.Value)"
		$DatabaseDetails.feedback = @{ }
	}
	
}

<#
	.SYNOPSIS
		Executes SQL queries that return results according to the RDBMS you have
	
	.DESCRIPTION
		This is a simple front-end for all the scritblocks that can be used to execute SQL for its corresponding RDBMS. This isn't the quickest way of doing it because a connection isn't maintained but, hell, we are scripting.
	
	.PARAMETER DatabaseDetails
		the hashtable with the project parmeters .
	
	.PARAMETER query
		the sql query
	
	.PARAMETER fileBasedQuery
		a query. If a file, put the path in the $fileBasedQuery parameter
	
	.NOTES
		Additional information about the function.
#>
function Execute-SQL
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[String]$query,
		[Parameter(Position = 3)]
		[String]$fileBasedQuery = $null
	)
	
	$Scriptblock = switch ($DatabaseDetails.RDBMS)
	{
		'postgresql'   { $GetdataFromPsql }
		'oracle'   { $GetdataFromOracle }
		'sqlserver'  { $GetdataFromSQLCMD }
		'sqlite' { $GetdataFromSqlite }
        'mariadb' { $GetdataFromMySQL }
        'Mysql' { $GetdataFromMySQL }
        	}
	$ErrorsSoFar = $Error.count
	$Scriptblock.invoke($DatabaseDetails,$query,$fileBasedQuery) ;
	if ($Error.Count -gt $ErrorsSoFar)
	{ 0 .. ($Error.Count - $ErrorsSoFar-1) | foreach{ Write-warning "$($error[$_])" } }
}

<#
	.SYNOPSIS
		Executes SQL queries that return results according to the RDBMS you have
	
	.DESCRIPTION
		This is a simple front-end for all the scritblocks that can be used to execute SQL for its corresponding RDBMS. This isn't the quickest way of doing it because a connection isn't maintained but, hell, we are scripting.
	
	.PARAMETER DatabaseDetails
		the hashtable with the project parmeters .
	
	.PARAMETER query
		the sql query
	
	.PARAMETER fileBasedQuery
		a query. If a file, put the path in the $fileBasedQuery parameter
	
#>
function Execute-SQLStatement
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails, #this is the same ubiquitous hashtable 
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[String]$Statement, #a query. If a file, put the path in the $fileBasedQuery parameter
		[String]$fileBasedQuery = $null, #if from a file, Specify it here
		[boolean]$simpleText = $true, #just return simple text, not JSON
		[boolean]$timing = $false, #return timing information from the RDBMS
		#do you return timing information
		[boolean]$muted = $false #do you return the data       
	)
	
	$Scriptblock = switch ($DatabaseDetails.RDBMS)
	{
		'postgresql'   { $GetdataFromPsql }
		'oracle'   { $GetdataFromOracle }
		'sqlserver'  { $GetdataFromSQLCMD }
		'sqlite' { $GetdataFromSqlite }
        'mariadb' { $GetdataFromMySQL }
        'Mysql' { $GetdataFromMySQL }
        	}
	$ErrorsSoFar = $Error.count
	$Scriptblock.invoke($DatabaseDetails,$Statement,$fileBasedQuery,$simpleText,$timing,$muted) ;
	if ($Error.Count -gt $ErrorsSoFar)
	{ 0 .. ($Error.Count - $ErrorsSoFar-1) | foreach{ Write-warning "$($error[$_])" } }
}
function Execute-SQLTimedStatement
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true,
				   Position = 1)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true,
				   Position = 2)]
		[String]$Statement,

       [Parameter(Mandatory = $true,
				   Position = 3)]
		[bool]$muted
	)
	
	$Scriptblock = switch ($DatabaseDetails.RDBMS)
	{
		'postgresql'   { $GetdataFromPsql }
		'oracle'   { $GetdataFromOracle }
		'sqlserver'  { $GetdataFromSQLCMD }
		'sqlite' { $GetdataFromSqlite }
        'mariadb' { $GetdataFromMySQL }
        'Mysql' { $GetdataFromMySQL }
        default {throw "Sorry, we don't support $_ yet!"}
        	}
	$ErrorsSoFar = $Error.count
	$Scriptblock.invoke($DatabaseDetails,$Statement,$null,$true,$true,$muted) ;
	if ($Error.Count -gt $ErrorsSoFar)
	{ 0 .. ($Error.Count - $ErrorsSoFar-1) | foreach{ Write-warning "$($error[$_])" } }
}

<#
	.SYNOPSIS
		Parses a SQL test script into individual statements or queries and assigns then to one of 
        four categories, 'Arrange', 'Act', 'Assert', 'Teardown'
        
	
	.DESCRIPTION
		This is a way of providing a way to convert  a single SQL script, annotated as required
        in block comments, into an object that can be executed within the framework as many times 
        as you need. The individual statements  can be run a specified number of times. and you
        can specify that the routine should pause a specific  number of seconds before it is executed. 
        This information is kept in the object and is therefore easily repeated.

	
	.PARAMETER Content
		A description of the Content parameter.
	
	.PARAMETER Terminator
		The statement terminator you use for SQL
	
	.EXAMPLE
		PS C:\> Create-TestObject

#>
function Create-TestObject
{
	[CmdletBinding()]
	[OutputType([array])]
	param
	(
		[Parameter(Position = 1,Mandatory = $true)]
		[string]$Content,
		[Parameter(Position = 2)]
		$Terminator = ';' # The statement terminator you use for SQL
	)
	
	switch ($Terminator)
	{
		'GO' { $TerminatorLength = 2 } #Meaning you delete it
		default { $TerminatorLength = 0 } #Meaning you leave it there
	}
	$Tasks = @()
	if ($Terminator -eq 'GO')
        { $TerminatorLength = 2}
    else
        {$TerminatorLength=0}  #Meaning you leave it there
	$ExecuteAllQuantity = 1;
	$ExecuteNextQuantity = $ExecuteAllQuantity;
	$PauseAllQuantity = 0;
    $PauseAllQuantity = $PauseNextQuantity;
	
	$RegexForInterpretingExecution = [regex]@'
(?i)(?m)^(?<ExecuteOrPause>EXECUTE|PAUSE){1,10}\s(?#
what you do )(?<AllOrNext>ALL|NEXT)\s{1,10}(?#
how many times)(?<Quantity>\d{1,10})\s{0,10}(?#
ignore this)(?<TimesOrSecs>TIMES|SECS)?\s{0,5}(?#
do we do it random order?)(?<RandomOrSerial>randomly|serially)?
'@
    $RegexForGettingComparisonFilename=[regex]@'
Compare with\s{1,80}?(?<ComparisonFile>[\w\s]{1,80})
'@
	Tokenize-SQLString $Content | where {
		$_.value -eq $Terminator -or $_.name -eq 'BlockComment'
	} | foreach -begin { $State = 'Act'; $StartIndex = -1 } {
		if ($_.value -eq $Terminator)
		{
			$EndIndex = $_.Index;
			$Tasks += @{
				'State' = $State;
				'Times' = $ExecuteNextQuantity;
				'PauseBefore' = $PauseNextQuantity;
                'compare' = $WhatWeCompareWith;
				'Expression' = "$($Content.Substring($StartIndex+1 , $EndIndex - $StartIndex - $TerminatorLength))"
			};
			
			$StartIndex = $EndIndex + $TerminatorLength;
			$ExecuteNextQuantity = $ExecuteAllQuantity;
			$PauseNextQuantity = $PauseAllQuantity;
            $WhatWeCompareWith='';
		}
		else
		{
			$CommentText = $_.Value -ireplace '(?s)\A\s*/\*\s*(?<TheComment>.*)\s*\*/\s*\z', '${TheComment}'
			if ($CommentText.Trim() -in ('Arrange', 'Act', 'Assert', 'Teardown'))
			{
				$State = $CommentText.Trim()
			}
			else
			{
				if (($CommentText.Trim() -like 'Execute*') -or ($CommentText.Trim() -like 'Pause*'))
				{
					if ($CommentText.Trim() -imatch $RegexForInterpretingExecution)
					{
						$ExecuteOrPause = $matches.ExecuteOrPause;
						$Action = $matches.AllOrNext;
						$Quantity = $matches.Quantity;
						$How = $matches.RandomOrSerial;
						if ($ExecuteOrPause -eq 'pause')
						{
							if ($Action -eq 'Next') { $PauseNextQuantity = $Quantity }
							else { $PauseAllQuantity = $Quantity }
						}
						if ($ExecuteOrPause -eq 'Execute')
						{
							if ($Action -eq 'Next') { $ExecuteNextQuantity = $Quantity }
							else { $ExecuteAllQuantity = $Quantity; $RandomOrSerially = $How; }
						}
					}
				}
                elseif($CommentText.Trim() -like 'Compare With*')
                {
                if ($CommentText.Trim() -imatch $RegexForGettingComparisonFilename) {
	                $WhatWeCompareWith = $matches.ComparisonFile
                    } else {
	                    $WhatWeCompareWith = ''
                    }
                }
			}
		}
	}
	@{
		'TheOrder' = $SectionAction;
		'Times' = $ExecuteAllQuantity;
		'Pause' = $PauseAllQuantity
		'Tasks' = $Tasks
	} #End foreach token
}

<#
	.SYNOPSIS
		Runs a SQL Script that has been annotated for AAAT usage
	
	.DESCRIPTION
		This takes a script, turns it into a test object. It then runs the 
		four sections of an AAAT script through from construction to 
		teardown, running each  each SQL Expression or statement individually.
		it will execute the ARRANEGE section first, then do all the scripts
		in the ACT section as many times as you need, pausing between them if 
		you specify it, and providing timings.
	
	.PARAMETER TheFilename
		The path to the script wiht the annotated script
	
	.PARAMETER DBDetails
		A description of the database.
	
	.EXAMPLE
	Run-AnnotatedTestScript -TheFilename '<PathToTheFile' -DBDetails $DBDetails
	
#>
function Run-AnnotatedTestScript
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true)]
		[string]$TheFilename,
		[Parameter(Mandatory = $true)]
		[hashtable]$TheDBDetails,
		[Parameter(Mandatory = $false)]
		[string]$Delimiter=';'
	)
    $Content = Get-Content -Raw $TheFileName
	try
	{ #is it already JSON?
		$JSONObject = $Content | convertfrom-json
	}
	catch #No, we have to convert it
	{
		$JSONObject = Create-TestObject $Content $Delimiter
	}
	$DefaultPause = $JSONObject.Pause
	$DefaultTimes = $JSONObject.Times
	$DefaultOrder = $JSONObject.TheOrder
	
	$JSONObject.Tasks | where { $_.State -like 'Arrange' } | foreach{
		Execute-SQLStatement $TheDbDetails $_.Expression -simpleText $true -timing $false -muted $true
	}
	if ($DefaultOrder -eq 'serially')
	{ $Tasks = $JSONObject.Tasks | where { $_.State -like 'Act' } }
	else
	{ $Tasks = $JSONObject.Tasks | where { $_.State -like 'Act' } | Sort-Object { Get-Random } }
	if ($Tasks.count -gt 0)
    {
        $SectionIterationsLeft = $DefaultTimes;
	    while ($SectionIterationsLeft -gt 0)
	    {
		    $Tasks | foreach{
			    if (!([string]::IsNullOrEmpty($_.PauseBefore)))
			    {
				    if ($_.PauseBefore -gt 0)
				    { Start-Sleep -seconds $_.PauseBefore; }
			    }
			    $Iterations = $_.times
			    if ($Iterations -eq $null) { $Iterations = $DefaultTimes }
			    while ($iterations -gt 0)
			    {
				    Execute-SQLStatement $TheDbDetails $_.Expression -simpleText $true -timing $true -muted $true;
				    $iterations--;
			    }
			    $SectionIterationsLeft--
		    }
        }
	}
	$JSONObject.Tasks | where { $_.State -like 'Assert' } | foreach{
        $TheComparisonName=$_.compare;
		if ($TheDbDetails.rdbms -eq 'sqlserver')
		# fix for a problem with getting JSON from SQL Server
		{ $TheExpression = $_.expression -ireplace ';\s*?\z', ' for json path' }
		else
		{ $TheExpression = $_.expression };
		# If there is no data to match with
		if ([string]::IsNullOrEmpty($TheComparisonName))
		{ Write output "No comparison specified for expression" }
		else
		{
			$TheCompareFile = "$($TheDbDetails.TestsLocations.split(',')[0])\$TheComparisonName.json"
			if (!(Test-path $TheCompareFile -PathType Leaf))
			{
				#if no test file yet exists, create it so in future it works. Replace this with the verified one
				(Execute-SQLStatement $TheDbDetails $TheExpression `
									 -simpleText $false)-ireplace '\AWarning:.+', '' >$TheCompareFile
				$CorrectResult = get-content -raw $TheCompareFile  | ConvertFrom-Json
				if ([string]::IsNullOrwhitespace($CorrectResult.Error))
				{
					Write output "comparison missing so written out for $TheComparisonName "
				}
				Else
				{
					Write output "Error in comparison - details in  $TheComparisonName "
				}
			} # we hadn't a comparison file
			else
			{
				#we have a correct file we can test against.
				$CorrectResult = get-content -raw $TheCompareFile | ConvertFrom-Json
				$TestResult = Convertfrom-JSON (
                    (Execute-SQLStatement $TheDbDetails $TheExpression -simpleText $false) -ireplace '\AWarning:.+', ''
                    )
				Compare-Resultsets -TestResult $TestResult -CorrectResult $CorrectResult|foreach{write-output "For $TheComparisonName, $($_)."}
			} # we had the correct file
		} # We had a comparison specified
	} # We had some assertions
	$JSONObject.Tasks | where { $_.State -like 'Teardown' } | foreach{
		Execute-SQLStatement $TheDbDetails $_.Expression -simpleText $true -timing $false -muted $true
	}
}

<#
	.SYNOPSIS
		Creates an array of SQL Expressions/queries from a larger script
	
	.DESCRIPTION
		This allows us to get timings for each one of a large number of 
        SQL Expressions
	
	.PARAMETER TheFileName
		A description of the TheFileName parameter.
	
	.PARAMETER TheDBDetails
		The file holding the strings
	
	.PARAMETER Content
		The SQL file that can contain a number of SQL expressions
	
	.PARAMETER Terminator
        Only necessary if you are using an unusual terminator;
        We need DBDetails anyway, so we use that to determine
		The SQL Terminator to use, usually ; but Transact SQL uses GO

	.EXAMPLE
		Run-EachSQLExpression -TheFileName '<pathToTheFile>' -TheDBDetails $DBDetails 
#>
function Run-EachSQLExpression
{
	[CmdletBinding()]
	[OutputType([array])]
	param
	(
		[Parameter(Position = 1)]
		[string]$TheFileName,
		[Parameter(Position = 2)]
		$TheDBDetails,
		[Parameter(Position = 3)]
		$Terminators =[array]$null
	)
	$Content = Get-Content -Raw $TheFileName;
   if ($Terminators -eq $null)
    {
        switch ($TheDBDetails.rdbms)
 	    {
		    'sqlserver' { $Terminators = 'GO' } #Meaning you delete it
		    default { $Terminators = ';' } #Meaning you leave it there
	    }
    }
	$Tasks = @()
	Tokenize-SQLString $Content | where {
		$_.value -in $Terminators
	} | foreach -begin { $Sections = @(); $StartIndex = 0 } {
		if ($_.value -eq 'GO') {$TerminatorLength=2} else {$TerminatorLength=0}
		$EndIndex = $_.Index;
		$TheExpression = "$($Content.Substring($StartIndex, $EndIndex - $StartIndex - $TerminatorLength))"
        $StartIndex = $EndIndex + $TerminatorLength;
		write-verbose "---Executing `"$TheExpression`""
		Execute-SQLStatement $TheDbDetails $TheExpression -simpleText $true -timing $true -muted $true;
	}
}

<#
	.SYNOPSIS
		Run the tests that are available See:
        https://www.red-gate.com/hub/product-learning/flyway/running-unit-and-integration-tests-during-flyway-migrations
        https://www.red-gate.com/hub/product-learning/flyway/performance-testing-databases-with-flyway-and-powershell
	
	.DESCRIPTION
		Runs all the available and suitable tests in the directory
	
	.PARAMETER TheDetails
		The DBDetails hashtable
	
	.PARAMETER ThePath
		The path to the test directory you want
	
	.PARAMETER Thetype
		The type of test T= a Timed Test,  P=a performance test (each one timed)   A=an AAAT test

	.PARAMETER Script
		The filetype to the test directory you want

	.EXAMPLE
				PS C:\> Run-TestsForMigration -TheDetails $value1 -ThePath 'Value2'
	
#>
function Run-TestsForMigration
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true)]
		[System.Collections.Hashtable]$DatabaseDetails,
		[Parameter(Mandatory = $true)]
		[string]$ThePath,
		[Parameter(Mandatory = $false)]
		[string]$Type = 'T', 
        #an integration test (T)'P', a  performance test (P), an AAAT test (A)
		[Parameter(Mandatory = $false)]
		[string]$Script = '*'
	)
    $Problems=@()
    if ([string]::IsNullOrEmpty($DBDetails.version))
    {Process-FlywayTasks $DBDetails $GetCurrentVersion}
    #check the report directory to make sure it exists
	$OurReportDirectory = "$($DatabaseDetails.reportLocation)\$($DatabaseDetails.version)\reports\tests"
	if (-not (Test-Path $OurReportDirectory -PathType Container))
	{ New-Item -ItemType Directory -Path $OurReportDirectory -Force
      # if not then create it 
    }
	if (-not (Test-Path $ThePath -PathType Container))
    {
    Throw 'no valid path to the tests specified'
    }
    #for each file in the test location
		Dir "$ThePath\$($Type)*.$($Script)" -name |
	foreach{
		if ($_ -imatch "\A(?m:^)$type(?<StartVersion>.*)-(?<EndVersion>.*)__(?<Description>.*)\.$Script\z")
		{#get the versions for which the test file is appropriate
			@{
				#turn blank strings into nulls so we can process underfined starts and ends properly
				'StartVersion' = if ([string]::IsNullOrWhiteSpace($matches.StartVersion)){[version]'0.0.0.0'}
                                 else {$matches.StartVersion};
				'EndVersion' = if ([string]::IsNullOrWhiteSpace($matches.EndVersion)){[version]'999.0.0.0'}
                                 else {$matches.EndVersion};
				'Description' = $matches.Description.Replace('_', ' ');
				'Filename' = $matches.0;
			}
		}
		else
		{ @{
				#turn blank strings into nulls so we can process underfined starts and ends properly
				'StartVersion' = '0.0.0.0';
				'EndVersion' = '9.9.9.9';
				'Description' = 'unversioned file';
				'Filename' = $matches.0;
			}
       }
	}|    
	where {
		[version]$DatabaseDetails.version -ge [version]$_.StartVersion -and
		[version]$DatabaseDetails.version -lt [version]$_.EndVersion -and
        $_.Description -ne 'unversioned file'
	} | foreach {
		"executing $($_.Filename) ($($_.Description))"
		# now we execute it
		if ($Script -eq 'ps1') { $TestOutput = . "$ThePath\$($_.Filename)" }
		elseif ($Script -eq 'sql')
		{
			if ($Type -eq 'T') 
            # we run these together with a with timing and with the results 'muted'
            {$TestOutput = Execute-SQLStatement $DatabaseDetails '-' -fileBasedQuery "$ThePath\$($_.Filename)" -simpleText $true -timing $true -muted $true }
			elseif ($Type -eq 'P') 
            # we run these with eavh expression individually with timings and with the results 'muted'
            {$TestOutput = Run-EachSQLExpression -TheFileName "$ThePath\$($_.Filename)"  -TheDBDetails $DatabaseDetails }
			elseif ($Type -eq 'A') 
            <# we run a SQL Script that has been annotated for AAAT usage. 
            It runs the four sections of an AAAT script through from construction to teardown #>
            {$TestOutput = Run-AnnotatedTestScript "$ThePath\$($_.Filename)"  $DatabaseDetails }
            else
            {$TestOutput = Execute-SQLStatement $DatabaseDetails '-' -fileBasedQuery "$ThePath\$($_.Filename)" }
 		}
		$testOutput > "$OurReportDirectory\Report_$($_.Description).txt"
		write-output $TestOutput
	}
}


<#
	.SYNOPSIS
		Compare two DACPACs and create a change script from them using SQLPackage that
		Creates a Transact-SQL incremental update script that updates the schema of a 
        target DACPAC to match the schema of a source DACPAC.
	
	.DESCRIPTION
		This compares two dacpacs and creates a schange script from them It is a thin shell over SQLPackage.
	
	.PARAMETER SourceDacPac
		The path to the source Dacpac
	
	.PARAMETER TargetDacPac
		The path to the The target DACPAC
	
	.PARAMETER OutputFilePath
		The path to the change script
	
	.PARAMETER OverWrite
		Do we overwrite the generated script if it already exists?
	
	.EXAMPLE
		PS C:\> Export-ChangeScriptFromDACPACS -SourceDacPac $path1 -TargetDacPac $path2 -OutputFilePath $path3
	
	.NOTES
		This is designed to support various specific script-generation tasks
#>
function Export-ChangeScriptFromDACPACS
{
	[CmdletBinding()]
	param
	(
		[Parameter(Mandatory = $true)]
		$SourceDacPac,
		[Parameter(Mandatory = $true)]
		$TargetDacPac,
		[Parameter(Mandatory = $true)]
		$OutputFilePath,
		[bool]$OverWrite = $false,
        $DatabaseName ='Database'
	)
	
<# The file is processed to remove  any CREATE SCHEMA statements, and any SQLCMD parameter
 definitions #>
	$problems = @(); # well, not yet
	$feedback = @()
	$WeCanDoIt = $true # until proven otherwise
	# the alias must be set to the path of your installed version of SQLpackage
	$command = get-command sqlpackage -ErrorAction Ignore
	if ($command -eq $null)
	{
		if ($SQLPackageAlias -ne $null)
		{ Set-Alias sqlpackage   $SQLPackageAlias }
		else
		{ $problems += 'You must have provided a path to $SQLPackage.exe in the ToolLocations.ps1 file in the resources folder' }
	}
	
	@($SourceDacPac, $TargetDacPac) | foreach {
		if (-not (Test-Path "$_")) { $WeCanDoIt = $false; $Feedback += "cannot find the dacpac file $_ " }
	}
	if ($OverWrite -eq $false -and (Test-Path "$OutputFilePath")) { $WeCanDoIt = $false; $Feedback += "Migration file $OutputFilePath already exists" }
	if ($WeCanDoIt) #if it has passed the tests
	{
		$ScriptArguments = @(
    <# Creates a Transact-SQL incremental update script that updates the schema of a 
       target to match the schema of a source. #>
			"/Action:Script", <#
         Specifies a source file to be used as the source of action instead of a
         database. For the Publish and Script actions, SourceFile may be a .dacpac
         file or a schema compare .scmp file. If this parameter is used, no other
         source parameter is valid. #>
			"/SourceFile:$SourceDACPAC",
         <#Specifies a source file to be used as the source of action instead of a
         database. For the Publish and Script actions, SourceFile may be a .dacpac
         file or a schema compare .scmp file. If this parameter is used, no other
         source parameter is valid. #>
			"/OutputPath:$OutputFilePath", <#
         Specifies the file path where the output files are generated. (short form
         /op)#>
			"/OverwriteFiles:$OverWrite", <#
         Specifies if SqlPackage.exe should overwrite existing files. Specifying
         false causes SqlPackage.exe to abort action if an existing file is
         encountered. Default value is True. (short form /of)"/TargetUser:<string>",<#
         For SQL Server Auth scenarios, defines the SQL Server user to use to
         access the target database. (short form /tu) #>
			"/TargetDatabaseName:$DatabaseName",
			"/TargetFile:$TargetDacPac",    <# Specifies a target file (that is, a 
			.dacpac file) to be used as the target of action instead of a database.
			If this parameter is used, no other target parameter shall be valid. 
			This parameter shall be invalid for actions that only support database
			targets.#>
			'/p:CommentOutSetVarDeclarations=true'
			"/p:CreateNewDatabase=False"
		)
		$console = sqlpackage $ScriptArguments
		$Feedback += "$console"
		if ($?)
		{
            $TargetVersion=split-path -Path $TargetDacPac -Leaf
			$SourceVersion=split-path -Path $SourceDacPac -Leaf
            $feedback += "Written $prefix migration for $($param1.Project) from $TargetVersion to $SourceVersion" 
		}
		else # if no errors then simple message, otherwise....
		{
			#report a problem and send back the args for diagnosis (hint, only for script development)
			$Arguments = '';
			$Arguments += $ScriptArguments | foreach{ $_ }
			$Problems += "Script generation Went badly. (code $LASTEXITCODE) with paramaters $Arguments"
		}
		if ($problems.count -eq 0)
		{
			# now convert all the SQLcmd output and other things not allowed
			$script = [IO.File]::ReadAllText($OutputFilePath)
			
			@(
				@(@'
:setvar __IsSqlCmdEnabled "True"
GO
IF N'$(__IsSqlCmdEnabled)' NOT LIKE N'True'
    BEGIN
        PRINT N'SQLCMD mode must be enabled to successfully execute this script.';
        SET NOEXEC ON;
    END
'@, ''),
				@(@'
GO
USE [$(DatabaseName)];
'@, '')
			) | foreach {
				$Script = $script.replace($_[0], $_[1])
			}
			# remove code to create schemas This has to be done by a regex
			$Script = $Script -creplace ':setvar.*|:on error exit', ''
			$Script = $Script -creplace '(\n|\r)+\s(\n|\r)+', "`n"
			$Script = $Script -creplace '(?s)PRINT N''Creating Schema \[.{1,256}?\]...'';\s+?GO\s+?CREATE SCHEMA \[.{1,256}?\].+?GO', ''
			# and write the script back
			[System.IO.File]::WriteAllLines($OutputFilePath, $script);
			$WriteLocations = "$OutputFilePath";
		}
	}	
    @($problems, $Feedback, $WriteLocations)
}


'FlywayTeamwork framework  loaded. V1.2.646'


